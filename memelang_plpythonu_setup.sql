CREATE EXTENSION IF NOT EXISTS plpython3u;
DROP SCHEMA IF EXISTS meme CASCADE;
CREATE SCHEMA meme;
CREATE TABLE meme.meme (aid varchar(255), rid varchar(255), bid varchar(255), qnt DECIMAL(20,6));
CREATE FUNCTION meme.query(memelang_in TEXT) RETURNS TABLE (LIKE meme.meme) AS $$ #!/usr/bin/env python
import contextlib as __stickytape_contextlib

@__stickytape_contextlib.contextmanager
def __stickytape_temporary_dir():
    import tempfile
    import shutil
    dir_path = tempfile.mkdtemp()
    try:
        yield dir_path
    finally:
        shutil.rmtree(dir_path)

with __stickytape_temporary_dir() as __stickytape_working_dir:
    def __stickytape_write_module(path, contents):
        import os, os.path

        def make_package(path):
            parts = path.split("/")
            partial_path = __stickytape_working_dir
            for part in parts:
                partial_path = os.path.join(partial_path, part)
                if not os.path.exists(partial_path):
                    os.mkdir(partial_path)
                    with open(os.path.join(partial_path, "__init__.py"), "wb") as f:
                        f.write(b"\n")

        make_package(os.path.dirname(path))

        full_path = os.path.join(__stickytape_working_dir, path)
        with open(full_path, "wb") as module_file:
            module_file.write(contents)

    import sys as __stickytape_sys
    __stickytape_sys.path.insert(0, __stickytape_working_dir)

    __stickytape_write_module('typing.py', b'"""\nThe typing module: Support for gradual typing as defined by PEP 484.\n\nAt large scale, the structure of the module is following:\n* Imports and exports, all public names should be explicitly added to __all__.\n* Internal helper functions: these should never be used in code outside this module.\n* _SpecialForm and its instances (special forms): Any, NoReturn, ClassVar, Union, Optional\n* Two classes whose instances can be type arguments in addition to types: ForwardRef and TypeVar\n* The core of internal generics API: _GenericAlias and _VariadicGenericAlias, the latter is\n  currently only used by Tuple and Callable. All subscripted types like X[int], Union[int, str],\n  etc., are instances of either of these classes.\n* The public counterpart of the generics API consists of two classes: Generic and Protocol.\n* Public helper functions: get_type_hints, overload, cast, no_type_check,\n  no_type_check_decorator.\n* Generic aliases for collections.abc ABCs and few additional protocols.\n* Special types: NewType, NamedTuple, TypedDict.\n* Wrapper submodules for re and io related types.\n"""\n\nfrom abc import abstractmethod, ABCMeta\nimport collections\nimport collections.abc\nimport contextlib\nimport functools\nimport operator\nimport re as stdlib_re  # Avoid confusion with the re we export.\nimport sys\nimport types\nfrom types import WrapperDescriptorType, MethodWrapperType, MethodDescriptorType\n\n# Please keep __all__ alphabetized within each category.\n__all__ = [\n    # Super-special typing primitives.\n    \'Any\',\n    \'Callable\',\n    \'ClassVar\',\n    \'Final\',\n    \'ForwardRef\',\n    \'Generic\',\n    \'Literal\',\n    \'Optional\',\n    \'Protocol\',\n    \'Tuple\',\n    \'Type\',\n    \'TypeVar\',\n    \'Union\',\n\n    # ABCs (from collections.abc).\n    \'AbstractSet\',  # collections.abc.Set.\n    \'ByteString\',\n    \'Container\',\n    \'ContextManager\',\n    \'Hashable\',\n    \'ItemsView\',\n    \'Iterable\',\n    \'Iterator\',\n    \'KeysView\',\n    \'Mapping\',\n    \'MappingView\',\n    \'MutableMapping\',\n    \'MutableSequence\',\n    \'MutableSet\',\n    \'Sequence\',\n    \'Sized\',\n    \'ValuesView\',\n    \'Awaitable\',\n    \'AsyncIterator\',\n    \'AsyncIterable\',\n    \'Coroutine\',\n    \'Collection\',\n    \'AsyncGenerator\',\n    \'AsyncContextManager\',\n\n    # Structural checks, a.k.a. protocols.\n    \'Reversible\',\n    \'SupportsAbs\',\n    \'SupportsBytes\',\n    \'SupportsComplex\',\n    \'SupportsFloat\',\n    \'SupportsIndex\',\n    \'SupportsInt\',\n    \'SupportsRound\',\n\n    # Concrete collection types.\n    \'ChainMap\',\n    \'Counter\',\n    \'Deque\',\n    \'Dict\',\n    \'DefaultDict\',\n    \'List\',\n    \'OrderedDict\',\n    \'Set\',\n    \'FrozenSet\',\n    \'NamedTuple\',  # Not really a type.\n    \'TypedDict\',  # Not really a type.\n    \'Generator\',\n\n    # One-off things.\n    \'AnyStr\',\n    \'cast\',\n    \'final\',\n    \'get_args\',\n    \'get_origin\',\n    \'get_type_hints\',\n    \'NewType\',\n    \'no_type_check\',\n    \'no_type_check_decorator\',\n    \'NoReturn\',\n    \'overload\',\n    \'runtime_checkable\',\n    \'Text\',\n    \'TYPE_CHECKING\',\n]\n\n# The pseudo-submodules \'re\' and \'io\' are part of the public\n# namespace, but excluded from __all__ because they might stomp on\n# legitimate imports of those modules.\n\n\ndef _type_check(arg, msg, is_argument=True):\n    """Check that the argument is a type, and return it (internal helper).\n\n    As a special case, accept None and return type(None) instead. Also wrap strings\n    into ForwardRef instances. Consider several corner cases, for example plain\n    special forms like Union are not valid, while Union[int, str] is OK, etc.\n    The msg argument is a human-readable error message, e.g::\n\n        "Union[arg, ...]: arg should be a type."\n\n    We append the repr() of the actual value (truncated to 100 chars).\n    """\n    invalid_generic_forms = (Generic, Protocol)\n    if is_argument:\n        invalid_generic_forms = invalid_generic_forms + (ClassVar, Final)\n\n    if arg is None:\n        return type(None)\n    if isinstance(arg, str):\n        return ForwardRef(arg)\n    if (isinstance(arg, _GenericAlias) and\n            arg.__origin__ in invalid_generic_forms):\n        raise TypeError(f"{arg} is not valid as type argument")\n    if (isinstance(arg, _SpecialForm) and arg not in (Any, NoReturn) or\n            arg in (Generic, Protocol)):\n        raise TypeError(f"Plain {arg} is not valid as type argument")\n    if isinstance(arg, (type, TypeVar, ForwardRef)):\n        return arg\n    if not callable(arg):\n        raise TypeError(f"{msg} Got {arg!r:.100}.")\n    return arg\n\n\ndef _type_repr(obj):\n    """Return the repr() of an object, special-casing types (internal helper).\n\n    If obj is a type, we return a shorter version than the default\n    type.__repr__, based on the module and qualified name, which is\n    typically enough to uniquely identify a type.  For everything\n    else, we fall back on repr(obj).\n    """\n    if isinstance(obj, type):\n        if obj.__module__ == \'builtins\':\n            return obj.__qualname__\n        return f\'{obj.__module__}.{obj.__qualname__}\'\n    if obj is ...:\n        return(\'...\')\n    if isinstance(obj, types.FunctionType):\n        return obj.__name__\n    return repr(obj)\n\n\ndef _collect_type_vars(types):\n    """Collect all type variable contained in types in order of\n    first appearance (lexicographic order). For example::\n\n        _collect_type_vars((T, List[S, T])) == (T, S)\n    """\n    tvars = []\n    for t in types:\n        if isinstance(t, TypeVar) and t not in tvars:\n            tvars.append(t)\n        if isinstance(t, _GenericAlias) and not t._special:\n            tvars.extend([t for t in t.__parameters__ if t not in tvars])\n    return tuple(tvars)\n\n\ndef _subs_tvars(tp, tvars, subs):\n    """Substitute type variables \'tvars\' with substitutions \'subs\'.\n    These two must have the same length.\n    """\n    if not isinstance(tp, _GenericAlias):\n        return tp\n    new_args = list(tp.__args__)\n    for a, arg in enumerate(tp.__args__):\n        if isinstance(arg, TypeVar):\n            for i, tvar in enumerate(tvars):\n                if arg == tvar:\n                    new_args[a] = subs[i]\n        else:\n            new_args[a] = _subs_tvars(arg, tvars, subs)\n    if tp.__origin__ is Union:\n        return Union[tuple(new_args)]\n    return tp.copy_with(tuple(new_args))\n\n\ndef _check_generic(cls, parameters):\n    """Check correct count for parameters of a generic cls (internal helper).\n    This gives a nice error message in case of count mismatch.\n    """\n    if not cls.__parameters__:\n        raise TypeError(f"{cls} is not a generic class")\n    alen = len(parameters)\n    elen = len(cls.__parameters__)\n    if alen != elen:\n        raise TypeError(f"Too {\'many\' if alen > elen else \'few\'} parameters for {cls};"\n                        f" actual {alen}, expected {elen}")\n\n\ndef _remove_dups_flatten(parameters):\n    """An internal helper for Union creation and substitution: flatten Unions\n    among parameters, then remove duplicates.\n    """\n    # Flatten out Union[Union[...], ...].\n    params = []\n    for p in parameters:\n        if isinstance(p, _GenericAlias) and p.__origin__ is Union:\n            params.extend(p.__args__)\n        elif isinstance(p, tuple) and len(p) > 0 and p[0] is Union:\n            params.extend(p[1:])\n        else:\n            params.append(p)\n    # Weed out strict duplicates, preserving the first of each occurrence.\n    all_params = set(params)\n    if len(all_params) < len(params):\n        new_params = []\n        for t in params:\n            if t in all_params:\n                new_params.append(t)\n                all_params.remove(t)\n        params = new_params\n        assert not all_params, all_params\n    return tuple(params)\n\n\n_cleanups = []\n\n\ndef _tp_cache(func):\n    """Internal wrapper caching __getitem__ of generic types with a fallback to\n    original function for non-hashable arguments.\n    """\n    cached = functools.lru_cache()(func)\n    _cleanups.append(cached.cache_clear)\n\n    @functools.wraps(func)\n    def inner(*args, **kwds):\n        try:\n            return cached(*args, **kwds)\n        except TypeError:\n            pass  # All real errors (not unhashable args) are raised below.\n        return func(*args, **kwds)\n    return inner\n\n\ndef _eval_type(t, globalns, localns):\n    """Evaluate all forward references in the given type t.\n    For use of globalns and localns see the docstring for get_type_hints().\n    """\n    if isinstance(t, ForwardRef):\n        return t._evaluate(globalns, localns)\n    if isinstance(t, _GenericAlias):\n        ev_args = tuple(_eval_type(a, globalns, localns) for a in t.__args__)\n        if ev_args == t.__args__:\n            return t\n        res = t.copy_with(ev_args)\n        res._special = t._special\n        return res\n    return t\n\n\nclass _Final:\n    """Mixin to prohibit subclassing"""\n\n    __slots__ = (\'__weakref__\',)\n\n    def __init_subclass__(self, /, *args, **kwds):\n        if \'_root\' not in kwds:\n            raise TypeError("Cannot subclass special typing classes")\n\nclass _Immutable:\n    """Mixin to indicate that object should not be copied."""\n\n    def __copy__(self):\n        return self\n\n    def __deepcopy__(self, memo):\n        return self\n\n\nclass _SpecialForm(_Final, _Immutable, _root=True):\n    """Internal indicator of special typing constructs.\n    See _doc instance attribute for specific docs.\n    """\n\n    __slots__ = (\'_name\', \'_doc\')\n\n    def __new__(cls, *args, **kwds):\n        """Constructor.\n\n        This only exists to give a better error message in case\n        someone tries to subclass a special typing object (not a good idea).\n        """\n        if (len(args) == 3 and\n                isinstance(args[0], str) and\n                isinstance(args[1], tuple)):\n            # Close enough.\n            raise TypeError(f"Cannot subclass {cls!r}")\n        return super().__new__(cls)\n\n    def __init__(self, name, doc):\n        self._name = name\n        self._doc = doc\n\n    def __eq__(self, other):\n        if not isinstance(other, _SpecialForm):\n            return NotImplemented\n        return self._name == other._name\n\n    def __hash__(self):\n        return hash((self._name,))\n\n    def __repr__(self):\n        return \'typing.\' + self._name\n\n    def __reduce__(self):\n        return self._name\n\n    def __call__(self, *args, **kwds):\n        raise TypeError(f"Cannot instantiate {self!r}")\n\n    def __instancecheck__(self, obj):\n        raise TypeError(f"{self} cannot be used with isinstance()")\n\n    def __subclasscheck__(self, cls):\n        raise TypeError(f"{self} cannot be used with issubclass()")\n\n    @_tp_cache\n    def __getitem__(self, parameters):\n        if self._name in (\'ClassVar\', \'Final\'):\n            item = _type_check(parameters, f\'{self._name} accepts only single type.\')\n            return _GenericAlias(self, (item,))\n        if self._name == \'Union\':\n            if parameters == ():\n                raise TypeError("Cannot take a Union of no types.")\n            if not isinstance(parameters, tuple):\n                parameters = (parameters,)\n            msg = "Union[arg, ...]: each arg must be a type."\n            parameters = tuple(_type_check(p, msg) for p in parameters)\n            parameters = _remove_dups_flatten(parameters)\n            if len(parameters) == 1:\n                return parameters[0]\n            return _GenericAlias(self, parameters)\n        if self._name == \'Optional\':\n            arg = _type_check(parameters, "Optional[t] requires a single type.")\n            return Union[arg, type(None)]\n        if self._name == \'Literal\':\n            # There is no \'_type_check\' call because arguments to Literal[...] are\n            # values, not types.\n            return _GenericAlias(self, parameters)\n        raise TypeError(f"{self} is not subscriptable")\n\n\nAny = _SpecialForm(\'Any\', doc=\n    """Special type indicating an unconstrained type.\n\n    - Any is compatible with every type.\n    - Any assumed to have all methods.\n    - All values assumed to be instances of Any.\n\n    Note that all the above statements are true from the point of view of\n    static type checkers. At runtime, Any should not be used with instance\n    or class checks.\n    """)\n\nNoReturn = _SpecialForm(\'NoReturn\', doc=\n    """Special type indicating functions that never return.\n    Example::\n\n      from typing import NoReturn\n\n      def stop() -> NoReturn:\n          raise Exception(\'no way\')\n\n    This type is invalid in other positions, e.g., ``List[NoReturn]``\n    will fail in static type checkers.\n    """)\n\nClassVar = _SpecialForm(\'ClassVar\', doc=\n    """Special type construct to mark class variables.\n\n    An annotation wrapped in ClassVar indicates that a given\n    attribute is intended to be used as a class variable and\n    should not be set on instances of that class. Usage::\n\n      class Starship:\n          stats: ClassVar[Dict[str, int]] = {} # class variable\n          damage: int = 10                     # instance variable\n\n    ClassVar accepts only types and cannot be further subscribed.\n\n    Note that ClassVar is not a class itself, and should not\n    be used with isinstance() or issubclass().\n    """)\n\nFinal = _SpecialForm(\'Final\', doc=\n    """Special typing construct to indicate final names to type checkers.\n\n    A final name cannot be re-assigned or overridden in a subclass.\n    For example:\n\n      MAX_SIZE: Final = 9000\n      MAX_SIZE += 1  # Error reported by type checker\n\n      class Connection:\n          TIMEOUT: Final[int] = 10\n\n      class FastConnector(Connection):\n          TIMEOUT = 1  # Error reported by type checker\n\n    There is no runtime checking of these properties.\n    """)\n\nUnion = _SpecialForm(\'Union\', doc=\n    """Union type; Union[X, Y] means either X or Y.\n\n    To define a union, use e.g. Union[int, str].  Details:\n    - The arguments must be types and there must be at least one.\n    - None as an argument is a special case and is replaced by\n      type(None).\n    - Unions of unions are flattened, e.g.::\n\n        Union[Union[int, str], float] == Union[int, str, float]\n\n    - Unions of a single argument vanish, e.g.::\n\n        Union[int] == int  # The constructor actually returns int\n\n    - Redundant arguments are skipped, e.g.::\n\n        Union[int, str, int] == Union[int, str]\n\n    - When comparing unions, the argument order is ignored, e.g.::\n\n        Union[int, str] == Union[str, int]\n\n    - You cannot subclass or instantiate a union.\n    - You can use Optional[X] as a shorthand for Union[X, None].\n    """)\n\nOptional = _SpecialForm(\'Optional\', doc=\n    """Optional type.\n\n    Optional[X] is equivalent to Union[X, None].\n    """)\n\nLiteral = _SpecialForm(\'Literal\', doc=\n    """Special typing form to define literal types (a.k.a. value types).\n\n    This form can be used to indicate to type checkers that the corresponding\n    variable or function parameter has a value equivalent to the provided\n    literal (or one of several literals):\n\n      def validate_simple(data: Any) -> Literal[True]:  # always returns True\n          ...\n\n      MODE = Literal[\'r\', \'rb\', \'w\', \'wb\']\n      def open_helper(file: str, mode: MODE) -> str:\n          ...\n\n      open_helper(\'/some/path\', \'r\')  # Passes type check\n      open_helper(\'/other/path\', \'typo\')  # Error in type checker\n\n   Literal[...] cannot be subclassed. At runtime, an arbitrary value\n   is allowed as type argument to Literal[...], but type checkers may\n   impose restrictions.\n    """)\n\n\nclass ForwardRef(_Final, _root=True):\n    """Internal wrapper to hold a forward reference."""\n\n    __slots__ = (\'__forward_arg__\', \'__forward_code__\',\n                 \'__forward_evaluated__\', \'__forward_value__\',\n                 \'__forward_is_argument__\')\n\n    def __init__(self, arg, is_argument=True):\n        if not isinstance(arg, str):\n            raise TypeError(f"Forward reference must be a string -- got {arg!r}")\n        try:\n            code = compile(arg, \'<string>\', \'eval\')\n        except SyntaxError:\n            raise SyntaxError(f"Forward reference must be an expression -- got {arg!r}")\n        self.__forward_arg__ = arg\n        self.__forward_code__ = code\n        self.__forward_evaluated__ = False\n        self.__forward_value__ = None\n        self.__forward_is_argument__ = is_argument\n\n    def _evaluate(self, globalns, localns):\n        if not self.__forward_evaluated__ or localns is not globalns:\n            if globalns is None and localns is None:\n                globalns = localns = {}\n            elif globalns is None:\n                globalns = localns\n            elif localns is None:\n                localns = globalns\n            self.__forward_value__ = _type_check(\n                eval(self.__forward_code__, globalns, localns),\n                "Forward references must evaluate to types.",\n                is_argument=self.__forward_is_argument__)\n            self.__forward_evaluated__ = True\n        return self.__forward_value__\n\n    def __eq__(self, other):\n        if not isinstance(other, ForwardRef):\n            return NotImplemented\n        if self.__forward_evaluated__ and other.__forward_evaluated__:\n            return (self.__forward_arg__ == other.__forward_arg__ and\n                    self.__forward_value__ == other.__forward_value__)\n        return self.__forward_arg__ == other.__forward_arg__\n\n    def __hash__(self):\n        return hash(self.__forward_arg__)\n\n    def __repr__(self):\n        return f\'ForwardRef({self.__forward_arg__!r})\'\n\n\nclass TypeVar(_Final, _Immutable, _root=True):\n    """Type variable.\n\n    Usage::\n\n      T = TypeVar(\'T\')  # Can be anything\n      A = TypeVar(\'A\', str, bytes)  # Must be str or bytes\n\n    Type variables exist primarily for the benefit of static type\n    checkers.  They serve as the parameters for generic types as well\n    as for generic function definitions.  See class Generic for more\n    information on generic types.  Generic functions work as follows:\n\n      def repeat(x: T, n: int) -> List[T]:\n          \'\'\'Return a list containing n references to x.\'\'\'\n          return [x]*n\n\n      def longest(x: A, y: A) -> A:\n          \'\'\'Return the longest of two strings.\'\'\'\n          return x if len(x) >= len(y) else y\n\n    The latter example\'s signature is essentially the overloading\n    of (str, str) -> str and (bytes, bytes) -> bytes.  Also note\n    that if the arguments are instances of some subclass of str,\n    the return type is still plain str.\n\n    At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.\n\n    Type variables defined with covariant=True or contravariant=True\n    can be used to declare covariant or contravariant generic types.\n    See PEP 484 for more details. By default generic types are invariant\n    in all type variables.\n\n    Type variables can be introspected. e.g.:\n\n      T.__name__ == \'T\'\n      T.__constraints__ == ()\n      T.__covariant__ == False\n      T.__contravariant__ = False\n      A.__constraints__ == (str, bytes)\n\n    Note that only type variables defined in global scope can be pickled.\n    """\n\n    __slots__ = (\'__name__\', \'__bound__\', \'__constraints__\',\n                 \'__covariant__\', \'__contravariant__\')\n\n    def __init__(self, name, *constraints, bound=None,\n                 covariant=False, contravariant=False):\n        self.__name__ = name\n        if covariant and contravariant:\n            raise ValueError("Bivariant types are not supported.")\n        self.__covariant__ = bool(covariant)\n        self.__contravariant__ = bool(contravariant)\n        if constraints and bound is not None:\n            raise TypeError("Constraints cannot be combined with bound=...")\n        if constraints and len(constraints) == 1:\n            raise TypeError("A single constraint is not allowed")\n        msg = "TypeVar(name, constraint, ...): constraints must be types."\n        self.__constraints__ = tuple(_type_check(t, msg) for t in constraints)\n        if bound:\n            self.__bound__ = _type_check(bound, "Bound must be a type.")\n        else:\n            self.__bound__ = None\n        try:\n            def_mod = sys._getframe(1).f_globals.get(\'__name__\', \'__main__\')  # for pickling\n        except (AttributeError, ValueError):\n            def_mod = None\n        if def_mod != \'typing\':\n            self.__module__ = def_mod\n\n    def __repr__(self):\n        if self.__covariant__:\n            prefix = \'+\'\n        elif self.__contravariant__:\n            prefix = \'-\'\n        else:\n            prefix = \'~\'\n        return prefix + self.__name__\n\n    def __reduce__(self):\n        return self.__name__\n\n\n# Special typing constructs Union, Optional, Generic, Callable and Tuple\n# use three special attributes for internal bookkeeping of generic types:\n# * __parameters__ is a tuple of unique free type parameters of a generic\n#   type, for example, Dict[T, T].__parameters__ == (T,);\n# * __origin__ keeps a reference to a type that was subscripted,\n#   e.g., Union[T, int].__origin__ == Union, or the non-generic version of\n#   the type.\n# * __args__ is a tuple of all arguments used in subscripting,\n#   e.g., Dict[T, int].__args__ == (T, int).\n\n\n# Mapping from non-generic type names that have a generic alias in typing\n# but with a different name.\n_normalize_alias = {\'list\': \'List\',\n                    \'tuple\': \'Tuple\',\n                    \'dict\': \'Dict\',\n                    \'set\': \'Set\',\n                    \'frozenset\': \'FrozenSet\',\n                    \'deque\': \'Deque\',\n                    \'defaultdict\': \'DefaultDict\',\n                    \'type\': \'Type\',\n                    \'Set\': \'AbstractSet\'}\n\ndef _is_dunder(attr):\n    return attr.startswith(\'__\') and attr.endswith(\'__\')\n\n\nclass _GenericAlias(_Final, _root=True):\n    """The central part of internal API.\n\n    This represents a generic version of type \'origin\' with type arguments \'params\'.\n    There are two kind of these aliases: user defined and special. The special ones\n    are wrappers around builtin collections and ABCs in collections.abc. These must\n    have \'name\' always set. If \'inst\' is False, then the alias can\'t be instantiated,\n    this is used by e.g. typing.List and typing.Dict.\n    """\n    def __init__(self, origin, params, *, inst=True, special=False, name=None):\n        self._inst = inst\n        self._special = special\n        if special and name is None:\n            orig_name = origin.__name__\n            name = _normalize_alias.get(orig_name, orig_name)\n        self._name = name\n        if not isinstance(params, tuple):\n            params = (params,)\n        self.__origin__ = origin\n        self.__args__ = tuple(... if a is _TypingEllipsis else\n                              () if a is _TypingEmpty else\n                              a for a in params)\n        self.__parameters__ = _collect_type_vars(params)\n        self.__slots__ = None  # This is not documented.\n        if not name:\n            self.__module__ = origin.__module__\n\n    @_tp_cache\n    def __getitem__(self, params):\n        if self.__origin__ in (Generic, Protocol):\n            # Can\'t subscript Generic[...] or Protocol[...].\n            raise TypeError(f"Cannot subscript already-subscripted {self}")\n        if not isinstance(params, tuple):\n            params = (params,)\n        msg = "Parameters to generic types must be types."\n        params = tuple(_type_check(p, msg) for p in params)\n        _check_generic(self, params)\n        return _subs_tvars(self, self.__parameters__, params)\n\n    def copy_with(self, params):\n        # We don\'t copy self._special.\n        return _GenericAlias(self.__origin__, params, name=self._name, inst=self._inst)\n\n    def __repr__(self):\n        if (self._name != \'Callable\' or\n                len(self.__args__) == 2 and self.__args__[0] is Ellipsis):\n            if self._name:\n                name = \'typing.\' + self._name\n            else:\n                name = _type_repr(self.__origin__)\n            if not self._special:\n                args = f\'[{", ".join([_type_repr(a) for a in self.__args__])}]\'\n            else:\n                args = \'\'\n            return (f\'{name}{args}\')\n        if self._special:\n            return \'typing.Callable\'\n        return (f\'typing.Callable\'\n                f\'[[{", ".join([_type_repr(a) for a in self.__args__[:-1]])}], \'\n                f\'{_type_repr(self.__args__[-1])}]\')\n\n    def __eq__(self, other):\n        if not isinstance(other, _GenericAlias):\n            return NotImplemented\n        if self.__origin__ != other.__origin__:\n            return False\n        if self.__origin__ is Union and other.__origin__ is Union:\n            return frozenset(self.__args__) == frozenset(other.__args__)\n        return self.__args__ == other.__args__\n\n    def __hash__(self):\n        if self.__origin__ is Union:\n            return hash((Union, frozenset(self.__args__)))\n        return hash((self.__origin__, self.__args__))\n\n    def __call__(self, *args, **kwargs):\n        if not self._inst:\n            raise TypeError(f"Type {self._name} cannot be instantiated; "\n                            f"use {self._name.lower()}() instead")\n        result = self.__origin__(*args, **kwargs)\n        try:\n            result.__orig_class__ = self\n        except AttributeError:\n            pass\n        return result\n\n    def __mro_entries__(self, bases):\n        if self._name:  # generic version of an ABC or built-in class\n            res = []\n            if self.__origin__ not in bases:\n                res.append(self.__origin__)\n            i = bases.index(self)\n            if not any(isinstance(b, _GenericAlias) or issubclass(b, Generic)\n                       for b in bases[i+1:]):\n                res.append(Generic)\n            return tuple(res)\n        if self.__origin__ is Generic:\n            if Protocol in bases:\n                return ()\n            i = bases.index(self)\n            for b in bases[i+1:]:\n                if isinstance(b, _GenericAlias) and b is not self:\n                    return ()\n        return (self.__origin__,)\n\n    def __getattr__(self, attr):\n        # We are careful for copy and pickle.\n        # Also for simplicity we just don\'t relay all dunder names\n        if \'__origin__\' in self.__dict__ and not _is_dunder(attr):\n            return getattr(self.__origin__, attr)\n        raise AttributeError(attr)\n\n    def __setattr__(self, attr, val):\n        if _is_dunder(attr) or attr in (\'_name\', \'_inst\', \'_special\'):\n            super().__setattr__(attr, val)\n        else:\n            setattr(self.__origin__, attr, val)\n\n    def __instancecheck__(self, obj):\n        return self.__subclasscheck__(type(obj))\n\n    def __subclasscheck__(self, cls):\n        if self._special:\n            if not isinstance(cls, _GenericAlias):\n                return issubclass(cls, self.__origin__)\n            if cls._special:\n                return issubclass(cls.__origin__, self.__origin__)\n        raise TypeError("Subscripted generics cannot be used with"\n                        " class and instance checks")\n\n    def __reduce__(self):\n        if self._special:\n            return self._name\n\n        if self._name:\n            origin = globals()[self._name]\n        else:\n            origin = self.__origin__\n        if (origin is Callable and\n            not (len(self.__args__) == 2 and self.__args__[0] is Ellipsis)):\n            args = list(self.__args__[:-1]), self.__args__[-1]\n        else:\n            args = tuple(self.__args__)\n            if len(args) == 1 and not isinstance(args[0], tuple):\n                args, = args\n        return operator.getitem, (origin, args)\n\n\nclass _VariadicGenericAlias(_GenericAlias, _root=True):\n    """Same as _GenericAlias above but for variadic aliases. Currently,\n    this is used only by special internal aliases: Tuple and Callable.\n    """\n    def __getitem__(self, params):\n        if self._name != \'Callable\' or not self._special:\n            return self.__getitem_inner__(params)\n        if not isinstance(params, tuple) or len(params) != 2:\n            raise TypeError("Callable must be used as "\n                            "Callable[[arg, ...], result].")\n        args, result = params\n        if args is Ellipsis:\n            params = (Ellipsis, result)\n        else:\n            if not isinstance(args, list):\n                raise TypeError(f"Callable[args, result]: args must be a list."\n                                f" Got {args}")\n            params = (tuple(args), result)\n        return self.__getitem_inner__(params)\n\n    @_tp_cache\n    def __getitem_inner__(self, params):\n        if self.__origin__ is tuple and self._special:\n            if params == ():\n                return self.copy_with((_TypingEmpty,))\n            if not isinstance(params, tuple):\n                params = (params,)\n            if len(params) == 2 and params[1] is ...:\n                msg = "Tuple[t, ...]: t must be a type."\n                p = _type_check(params[0], msg)\n                return self.copy_with((p, _TypingEllipsis))\n            msg = "Tuple[t0, t1, ...]: each t must be a type."\n            params = tuple(_type_check(p, msg) for p in params)\n            return self.copy_with(params)\n        if self.__origin__ is collections.abc.Callable and self._special:\n            args, result = params\n            msg = "Callable[args, result]: result must be a type."\n            result = _type_check(result, msg)\n            if args is Ellipsis:\n                return self.copy_with((_TypingEllipsis, result))\n            msg = "Callable[[arg, ...], result]: each arg must be a type."\n            args = tuple(_type_check(arg, msg) for arg in args)\n            params = args + (result,)\n            return self.copy_with(params)\n        return super().__getitem__(params)\n\n\nclass Generic:\n    """Abstract base class for generic types.\n\n    A generic type is typically declared by inheriting from\n    this class parameterized with one or more type variables.\n    For example, a generic mapping type might be defined as::\n\n      class Mapping(Generic[KT, VT]):\n          def __getitem__(self, key: KT) -> VT:\n              ...\n          # Etc.\n\n    This class can then be used as follows::\n\n      def lookup_name(mapping: Mapping[KT, VT], key: KT, default: VT) -> VT:\n          try:\n              return mapping[key]\n          except KeyError:\n              return default\n    """\n    __slots__ = ()\n    _is_protocol = False\n\n    def __new__(cls, *args, **kwds):\n        if cls in (Generic, Protocol):\n            raise TypeError(f"Type {cls.__name__} cannot be instantiated; "\n                            "it can be used only as a base class")\n        if super().__new__ is object.__new__ and cls.__init__ is not object.__init__:\n            obj = super().__new__(cls)\n        else:\n            obj = super().__new__(cls, *args, **kwds)\n        return obj\n\n    @_tp_cache\n    def __class_getitem__(cls, params):\n        if not isinstance(params, tuple):\n            params = (params,)\n        if not params and cls is not Tuple:\n            raise TypeError(\n                f"Parameter list to {cls.__qualname__}[...] cannot be empty")\n        msg = "Parameters to generic types must be types."\n        params = tuple(_type_check(p, msg) for p in params)\n        if cls in (Generic, Protocol):\n            # Generic and Protocol can only be subscripted with unique type variables.\n            if not all(isinstance(p, TypeVar) for p in params):\n                raise TypeError(\n                    f"Parameters to {cls.__name__}[...] must all be type variables")\n            if len(set(params)) != len(params):\n                raise TypeError(\n                    f"Parameters to {cls.__name__}[...] must all be unique")\n        else:\n            # Subscripting a regular Generic subclass.\n            _check_generic(cls, params)\n        return _GenericAlias(cls, params)\n\n    def __init_subclass__(cls, *args, **kwargs):\n        super().__init_subclass__(*args, **kwargs)\n        tvars = []\n        if \'__orig_bases__\' in cls.__dict__:\n            error = Generic in cls.__orig_bases__\n        else:\n            error = Generic in cls.__bases__ and cls.__name__ != \'Protocol\'\n        if error:\n            raise TypeError("Cannot inherit from plain Generic")\n        if \'__orig_bases__\' in cls.__dict__:\n            tvars = _collect_type_vars(cls.__orig_bases__)\n            # Look for Generic[T1, ..., Tn].\n            # If found, tvars must be a subset of it.\n            # If not found, tvars is it.\n            # Also check for and reject plain Generic,\n            # and reject multiple Generic[...].\n            gvars = None\n            for base in cls.__orig_bases__:\n                if (isinstance(base, _GenericAlias) and\n                        base.__origin__ is Generic):\n                    if gvars is not None:\n                        raise TypeError(\n                            "Cannot inherit from Generic[...] multiple types.")\n                    gvars = base.__parameters__\n            if gvars is not None:\n                tvarset = set(tvars)\n                gvarset = set(gvars)\n                if not tvarset <= gvarset:\n                    s_vars = \', \'.join(str(t) for t in tvars if t not in gvarset)\n                    s_args = \', \'.join(str(g) for g in gvars)\n                    raise TypeError(f"Some type variables ({s_vars}) are"\n                                    f" not listed in Generic[{s_args}]")\n                tvars = gvars\n        cls.__parameters__ = tuple(tvars)\n\n\nclass _TypingEmpty:\n    """Internal placeholder for () or []. Used by TupleMeta and CallableMeta\n    to allow empty list/tuple in specific places, without allowing them\n    to sneak in where prohibited.\n    """\n\n\nclass _TypingEllipsis:\n    """Internal placeholder for ... (ellipsis)."""\n\n\n_TYPING_INTERNALS = [\'__parameters__\', \'__orig_bases__\',  \'__orig_class__\',\n                     \'_is_protocol\', \'_is_runtime_protocol\']\n\n_SPECIAL_NAMES = [\'__abstractmethods__\', \'__annotations__\', \'__dict__\', \'__doc__\',\n                  \'__init__\', \'__module__\', \'__new__\', \'__slots__\',\n                  \'__subclasshook__\', \'__weakref__\']\n\n# These special attributes will be not collected as protocol members.\nEXCLUDED_ATTRIBUTES = _TYPING_INTERNALS + _SPECIAL_NAMES + [\'_MutableMapping__marker\']\n\n\ndef _get_protocol_attrs(cls):\n    """Collect protocol members from a protocol class objects.\n\n    This includes names actually defined in the class dictionary, as well\n    as names that appear in annotations. Special names (above) are skipped.\n    """\n    attrs = set()\n    for base in cls.__mro__[:-1]:  # without object\n        if base.__name__ in (\'Protocol\', \'Generic\'):\n            continue\n        annotations = getattr(base, \'__annotations__\', {})\n        for attr in list(base.__dict__.keys()) + list(annotations.keys()):\n            if not attr.startswith(\'_abc_\') and attr not in EXCLUDED_ATTRIBUTES:\n                attrs.add(attr)\n    return attrs\n\n\ndef _is_callable_members_only(cls):\n    # PEP 544 prohibits using issubclass() with protocols that have non-method members.\n    return all(callable(getattr(cls, attr, None)) for attr in _get_protocol_attrs(cls))\n\n\ndef _no_init(self, *args, **kwargs):\n    if type(self)._is_protocol:\n        raise TypeError(\'Protocols cannot be instantiated\')\n\n\ndef _allow_reckless_class_cheks():\n    """Allow instnance and class checks for special stdlib modules.\n\n    The abc and functools modules indiscriminately call isinstance() and\n    issubclass() on the whole MRO of a user class, which may contain protocols.\n    """\n    try:\n        return sys._getframe(3).f_globals[\'__name__\'] in [\'abc\', \'functools\']\n    except (AttributeError, ValueError):  # For platforms without _getframe().\n        return True\n\n\n_PROTO_WHITELIST = {\n    \'collections.abc\': [\n        \'Callable\', \'Awaitable\', \'Iterable\', \'Iterator\', \'AsyncIterable\',\n        \'Hashable\', \'Sized\', \'Container\', \'Collection\', \'Reversible\',\n    ],\n    \'contextlib\': [\'AbstractContextManager\', \'AbstractAsyncContextManager\'],\n}\n\n\nclass _ProtocolMeta(ABCMeta):\n    # This metaclass is really unfortunate and exists only because of\n    # the lack of __instancehook__.\n    def __instancecheck__(cls, instance):\n        # We need this method for situations where attributes are\n        # assigned in __init__.\n        if ((not getattr(cls, \'_is_protocol\', False) or\n                _is_callable_members_only(cls)) and\n                issubclass(instance.__class__, cls)):\n            return True\n        if cls._is_protocol:\n            if all(hasattr(instance, attr) and\n                    # All *methods* can be blocked by setting them to None.\n                    (not callable(getattr(cls, attr, None)) or\n                     getattr(instance, attr) is not None)\n                    for attr in _get_protocol_attrs(cls)):\n                return True\n        return super().__instancecheck__(instance)\n\n\nclass Protocol(Generic, metaclass=_ProtocolMeta):\n    """Base class for protocol classes.\n\n    Protocol classes are defined as::\n\n        class Proto(Protocol):\n            def meth(self) -> int:\n                ...\n\n    Such classes are primarily used with static type checkers that recognize\n    structural subtyping (static duck-typing), for example::\n\n        class C:\n            def meth(self) -> int:\n                return 0\n\n        def func(x: Proto) -> int:\n            return x.meth()\n\n        func(C())  # Passes static type check\n\n    See PEP 544 for details. Protocol classes decorated with\n    @typing.runtime_checkable act as simple-minded runtime protocols that check\n    only the presence of given attributes, ignoring their type signatures.\n    Protocol classes can be generic, they are defined as::\n\n        class GenProto(Protocol[T]):\n            def meth(self) -> T:\n                ...\n    """\n    __slots__ = ()\n    _is_protocol = True\n    _is_runtime_protocol = False\n\n    def __init_subclass__(cls, *args, **kwargs):\n        super().__init_subclass__(*args, **kwargs)\n\n        # Determine if this is a protocol or a concrete subclass.\n        if not cls.__dict__.get(\'_is_protocol\', False):\n            cls._is_protocol = any(b is Protocol for b in cls.__bases__)\n\n        # Set (or override) the protocol subclass hook.\n        def _proto_hook(other):\n            if not cls.__dict__.get(\'_is_protocol\', False):\n                return NotImplemented\n\n            # First, perform various sanity checks.\n            if not getattr(cls, \'_is_runtime_protocol\', False):\n                if _allow_reckless_class_cheks():\n                    return NotImplemented\n                raise TypeError("Instance and class checks can only be used with"\n                                " @runtime_checkable protocols")\n            if not _is_callable_members_only(cls):\n                if _allow_reckless_class_cheks():\n                    return NotImplemented\n                raise TypeError("Protocols with non-method members"\n                                " don\'t support issubclass()")\n            if not isinstance(other, type):\n                # Same error message as for issubclass(1, int).\n                raise TypeError(\'issubclass() arg 1 must be a class\')\n\n            # Second, perform the actual structural compatibility check.\n            for attr in _get_protocol_attrs(cls):\n                for base in other.__mro__:\n                    # Check if the members appears in the class dictionary...\n                    if attr in base.__dict__:\n                        if base.__dict__[attr] is None:\n                            return NotImplemented\n                        break\n\n                    # ...or in annotations, if it is a sub-protocol.\n                    annotations = getattr(base, \'__annotations__\', {})\n                    if (isinstance(annotations, collections.abc.Mapping) and\n                            attr in annotations and\n                            issubclass(other, Generic) and other._is_protocol):\n                        break\n                else:\n                    return NotImplemented\n            return True\n\n        if \'__subclasshook__\' not in cls.__dict__:\n            cls.__subclasshook__ = _proto_hook\n\n        # We have nothing more to do for non-protocols...\n        if not cls._is_protocol:\n            return\n\n        # ... otherwise check consistency of bases, and prohibit instantiation.\n        for base in cls.__bases__:\n            if not (base in (object, Generic) or\n                    base.__module__ in _PROTO_WHITELIST and\n                    base.__name__ in _PROTO_WHITELIST[base.__module__] or\n                    issubclass(base, Generic) and base._is_protocol):\n                raise TypeError(\'Protocols can only inherit from other\'\n                                \' protocols, got %r\' % base)\n        cls.__init__ = _no_init\n\n\ndef runtime_checkable(cls):\n    """Mark a protocol class as a runtime protocol.\n\n    Such protocol can be used with isinstance() and issubclass().\n    Raise TypeError if applied to a non-protocol class.\n    This allows a simple-minded structural check very similar to\n    one trick ponies in collections.abc such as Iterable.\n    For example::\n\n        @runtime_checkable\n        class Closable(Protocol):\n            def close(self): ...\n\n        assert isinstance(open(\'/some/file\'), Closable)\n\n    Warning: this will check only the presence of the required methods,\n    not their type signatures!\n    """\n    if not issubclass(cls, Generic) or not cls._is_protocol:\n        raise TypeError(\'@runtime_checkable can be only applied to protocol classes,\'\n                        \' got %r\' % cls)\n    cls._is_runtime_protocol = True\n    return cls\n\n\ndef cast(typ, val):\n    """Cast a value to a type.\n\n    This returns the value unchanged.  To the type checker this\n    signals that the return value has the designated type, but at\n    runtime we intentionally don\'t check anything (we want this\n    to be as fast as possible).\n    """\n    return val\n\n\ndef _get_defaults(func):\n    """Internal helper to extract the default arguments, by name."""\n    try:\n        code = func.__code__\n    except AttributeError:\n        # Some built-in functions don\'t have __code__, __defaults__, etc.\n        return {}\n    pos_count = code.co_argcount\n    arg_names = code.co_varnames\n    arg_names = arg_names[:pos_count]\n    defaults = func.__defaults__ or ()\n    kwdefaults = func.__kwdefaults__\n    res = dict(kwdefaults) if kwdefaults else {}\n    pos_offset = pos_count - len(defaults)\n    for name, value in zip(arg_names[pos_offset:], defaults):\n        assert name not in res\n        res[name] = value\n    return res\n\n\n_allowed_types = (types.FunctionType, types.BuiltinFunctionType,\n                  types.MethodType, types.ModuleType,\n                  WrapperDescriptorType, MethodWrapperType, MethodDescriptorType)\n\n\ndef get_type_hints(obj, globalns=None, localns=None):\n    """Return type hints for an object.\n\n    This is often the same as obj.__annotations__, but it handles\n    forward references encoded as string literals, and if necessary\n    adds Optional[t] if a default value equal to None is set.\n\n    The argument may be a module, class, method, or function. The annotations\n    are returned as a dictionary. For classes, annotations include also\n    inherited members.\n\n    TypeError is raised if the argument is not of a type that can contain\n    annotations, and an empty dictionary is returned if no annotations are\n    present.\n\n    BEWARE -- the behavior of globalns and localns is counterintuitive\n    (unless you are familiar with how eval() and exec() work).  The\n    search order is locals first, then globals.\n\n    - If no dict arguments are passed, an attempt is made to use the\n      globals from obj (or the respective module\'s globals for classes),\n      and these are also used as the locals.  If the object does not appear\n      to have globals, an empty dictionary is used.\n\n    - If one dict argument is passed, it is used for both globals and\n      locals.\n\n    - If two dict arguments are passed, they specify globals and\n      locals, respectively.\n    """\n\n    if getattr(obj, \'__no_type_check__\', None):\n        return {}\n    # Classes require a special treatment.\n    if isinstance(obj, type):\n        hints = {}\n        for base in reversed(obj.__mro__):\n            if globalns is None:\n                base_globals = sys.modules[base.__module__].__dict__\n            else:\n                base_globals = globalns\n            ann = base.__dict__.get(\'__annotations__\', {})\n            for name, value in ann.items():\n                if value is None:\n                    value = type(None)\n                if isinstance(value, str):\n                    value = ForwardRef(value, is_argument=False)\n                value = _eval_type(value, base_globals, localns)\n                hints[name] = value\n        return hints\n\n    if globalns is None:\n        if isinstance(obj, types.ModuleType):\n            globalns = obj.__dict__\n        else:\n            nsobj = obj\n            # Find globalns for the unwrapped object.\n            while hasattr(nsobj, \'__wrapped__\'):\n                nsobj = nsobj.__wrapped__\n            globalns = getattr(nsobj, \'__globals__\', {})\n        if localns is None:\n            localns = globalns\n    elif localns is None:\n        localns = globalns\n    hints = getattr(obj, \'__annotations__\', None)\n    if hints is None:\n        # Return empty annotations for something that _could_ have them.\n        if isinstance(obj, _allowed_types):\n            return {}\n        else:\n            raise TypeError(\'{!r} is not a module, class, method, \'\n                            \'or function.\'.format(obj))\n    defaults = _get_defaults(obj)\n    hints = dict(hints)\n    for name, value in hints.items():\n        if value is None:\n            value = type(None)\n        if isinstance(value, str):\n            value = ForwardRef(value)\n        value = _eval_type(value, globalns, localns)\n        if name in defaults and defaults[name] is None:\n            value = Optional[value]\n        hints[name] = value\n    return hints\n\n\ndef get_origin(tp):\n    """Get the unsubscripted version of a type.\n\n    This supports generic types, Callable, Tuple, Union, Literal, Final and ClassVar.\n    Return None for unsupported types. Examples::\n\n        get_origin(Literal[42]) is Literal\n        get_origin(int) is None\n        get_origin(ClassVar[int]) is ClassVar\n        get_origin(Generic) is Generic\n        get_origin(Generic[T]) is Generic\n        get_origin(Union[T, int]) is Union\n        get_origin(List[Tuple[T, T]][int]) == list\n    """\n    if isinstance(tp, _GenericAlias):\n        return tp.__origin__\n    if tp is Generic:\n        return Generic\n    return None\n\n\ndef get_args(tp):\n    """Get type arguments with all substitutions performed.\n\n    For unions, basic simplifications used by Union constructor are performed.\n    Examples::\n        get_args(Dict[str, int]) == (str, int)\n        get_args(int) == ()\n        get_args(Union[int, Union[T, int], str][int]) == (int, str)\n        get_args(Union[int, Tuple[T, int]][str]) == (int, Tuple[str, int])\n        get_args(Callable[[], T][int]) == ([], int)\n    """\n    if isinstance(tp, _GenericAlias) and not tp._special:\n        res = tp.__args__\n        if get_origin(tp) is collections.abc.Callable and res[0] is not Ellipsis:\n            res = (list(res[:-1]), res[-1])\n        return res\n    return ()\n\n\ndef no_type_check(arg):\n    """Decorator to indicate that annotations are not type hints.\n\n    The argument must be a class or function; if it is a class, it\n    applies recursively to all methods and classes defined in that class\n    (but not to methods defined in its superclasses or subclasses).\n\n    This mutates the function(s) or class(es) in place.\n    """\n    if isinstance(arg, type):\n        arg_attrs = arg.__dict__.copy()\n        for attr, val in arg.__dict__.items():\n            if val in arg.__bases__ + (arg,):\n                arg_attrs.pop(attr)\n        for obj in arg_attrs.values():\n            if isinstance(obj, types.FunctionType):\n                obj.__no_type_check__ = True\n            if isinstance(obj, type):\n                no_type_check(obj)\n    try:\n        arg.__no_type_check__ = True\n    except TypeError:  # built-in classes\n        pass\n    return arg\n\n\ndef no_type_check_decorator(decorator):\n    """Decorator to give another decorator the @no_type_check effect.\n\n    This wraps the decorator with something that wraps the decorated\n    function in @no_type_check.\n    """\n\n    @functools.wraps(decorator)\n    def wrapped_decorator(*args, **kwds):\n        func = decorator(*args, **kwds)\n        func = no_type_check(func)\n        return func\n\n    return wrapped_decorator\n\n\ndef _overload_dummy(*args, **kwds):\n    """Helper for @overload to raise when called."""\n    raise NotImplementedError(\n        "You should not call an overloaded function. "\n        "A series of @overload-decorated functions "\n        "outside a stub module should always be followed "\n        "by an implementation that is not @overload-ed.")\n\n\ndef overload(func):\n    """Decorator for overloaded functions/methods.\n\n    In a stub file, place two or more stub definitions for the same\n    function in a row, each decorated with @overload.  For example:\n\n      @overload\n      def utf8(value: None) -> None: ...\n      @overload\n      def utf8(value: bytes) -> bytes: ...\n      @overload\n      def utf8(value: str) -> bytes: ...\n\n    In a non-stub file (i.e. a regular .py file), do the same but\n    follow it with an implementation.  The implementation should *not*\n    be decorated with @overload.  For example:\n\n      @overload\n      def utf8(value: None) -> None: ...\n      @overload\n      def utf8(value: bytes) -> bytes: ...\n      @overload\n      def utf8(value: str) -> bytes: ...\n      def utf8(value):\n          # implementation goes here\n    """\n    return _overload_dummy\n\n\ndef final(f):\n    """A decorator to indicate final methods and final classes.\n\n    Use this decorator to indicate to type checkers that the decorated\n    method cannot be overridden, and decorated class cannot be subclassed.\n    For example:\n\n      class Base:\n          @final\n          def done(self) -> None:\n              ...\n      class Sub(Base):\n          def done(self) -> None:  # Error reported by type checker\n                ...\n\n      @final\n      class Leaf:\n          ...\n      class Other(Leaf):  # Error reported by type checker\n          ...\n\n    There is no runtime checking of these properties.\n    """\n    return f\n\n\n# Some unconstrained type variables.  These are used by the container types.\n# (These are not for export.)\nT = TypeVar(\'T\')  # Any type.\nKT = TypeVar(\'KT\')  # Key type.\nVT = TypeVar(\'VT\')  # Value type.\nT_co = TypeVar(\'T_co\', covariant=True)  # Any type covariant containers.\nV_co = TypeVar(\'V_co\', covariant=True)  # Any type covariant containers.\nVT_co = TypeVar(\'VT_co\', covariant=True)  # Value type covariant containers.\nT_contra = TypeVar(\'T_contra\', contravariant=True)  # Ditto contravariant.\n# Internal type variable used for Type[].\nCT_co = TypeVar(\'CT_co\', covariant=True, bound=type)\n\n# A useful type variable with constraints.  This represents string types.\n# (This one *is* for export!)\nAnyStr = TypeVar(\'AnyStr\', bytes, str)\n\n\n# Various ABCs mimicking those in collections.abc.\ndef _alias(origin, params, inst=True):\n    return _GenericAlias(origin, params, special=True, inst=inst)\n\nHashable = _alias(collections.abc.Hashable, ())  # Not generic.\nAwaitable = _alias(collections.abc.Awaitable, T_co)\nCoroutine = _alias(collections.abc.Coroutine, (T_co, T_contra, V_co))\nAsyncIterable = _alias(collections.abc.AsyncIterable, T_co)\nAsyncIterator = _alias(collections.abc.AsyncIterator, T_co)\nIterable = _alias(collections.abc.Iterable, T_co)\nIterator = _alias(collections.abc.Iterator, T_co)\nReversible = _alias(collections.abc.Reversible, T_co)\nSized = _alias(collections.abc.Sized, ())  # Not generic.\nContainer = _alias(collections.abc.Container, T_co)\nCollection = _alias(collections.abc.Collection, T_co)\nCallable = _VariadicGenericAlias(collections.abc.Callable, (), special=True)\nCallable.__doc__ = \\\n    """Callable type; Callable[[int], str] is a function of (int) -> str.\n\n    The subscription syntax must always be used with exactly two\n    values: the argument list and the return type.  The argument list\n    must be a list of types or ellipsis; the return type must be a single type.\n\n    There is no syntax to indicate optional or keyword arguments,\n    such function types are rarely used as callback types.\n    """\nAbstractSet = _alias(collections.abc.Set, T_co)\nMutableSet = _alias(collections.abc.MutableSet, T)\n# NOTE: Mapping is only covariant in the value type.\nMapping = _alias(collections.abc.Mapping, (KT, VT_co))\nMutableMapping = _alias(collections.abc.MutableMapping, (KT, VT))\nSequence = _alias(collections.abc.Sequence, T_co)\nMutableSequence = _alias(collections.abc.MutableSequence, T)\nByteString = _alias(collections.abc.ByteString, ())  # Not generic\nTuple = _VariadicGenericAlias(tuple, (), inst=False, special=True)\nTuple.__doc__ = \\\n    """Tuple type; Tuple[X, Y] is the cross-product type of X and Y.\n\n    Example: Tuple[T1, T2] is a tuple of two elements corresponding\n    to type variables T1 and T2.  Tuple[int, float, str] is a tuple\n    of an int, a float and a string.\n\n    To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].\n    """\nList = _alias(list, T, inst=False)\nDeque = _alias(collections.deque, T)\nSet = _alias(set, T, inst=False)\nFrozenSet = _alias(frozenset, T_co, inst=False)\nMappingView = _alias(collections.abc.MappingView, T_co)\nKeysView = _alias(collections.abc.KeysView, KT)\nItemsView = _alias(collections.abc.ItemsView, (KT, VT_co))\nValuesView = _alias(collections.abc.ValuesView, VT_co)\nContextManager = _alias(contextlib.AbstractContextManager, T_co)\nAsyncContextManager = _alias(contextlib.AbstractAsyncContextManager, T_co)\nDict = _alias(dict, (KT, VT), inst=False)\nDefaultDict = _alias(collections.defaultdict, (KT, VT))\nOrderedDict = _alias(collections.OrderedDict, (KT, VT))\nCounter = _alias(collections.Counter, T)\nChainMap = _alias(collections.ChainMap, (KT, VT))\nGenerator = _alias(collections.abc.Generator, (T_co, T_contra, V_co))\nAsyncGenerator = _alias(collections.abc.AsyncGenerator, (T_co, T_contra))\nType = _alias(type, CT_co, inst=False)\nType.__doc__ = \\\n    """A special construct usable to annotate class objects.\n\n    For example, suppose we have the following classes::\n\n      class User: ...  # Abstract base for User classes\n      class BasicUser(User): ...\n      class ProUser(User): ...\n      class TeamUser(User): ...\n\n    And a function that takes a class argument that\'s a subclass of\n    User and returns an instance of the corresponding class::\n\n      U = TypeVar(\'U\', bound=User)\n      def new_user(user_class: Type[U]) -> U:\n          user = user_class()\n          # (Here we could write the user object to a database)\n          return user\n\n      joe = new_user(BasicUser)\n\n    At this point the type checker knows that joe has type BasicUser.\n    """\n\n\n@runtime_checkable\nclass SupportsInt(Protocol):\n    """An ABC with one abstract method __int__."""\n    __slots__ = ()\n\n    @abstractmethod\n    def __int__(self) -> int:\n        pass\n\n\n@runtime_checkable\nclass SupportsFloat(Protocol):\n    """An ABC with one abstract method __float__."""\n    __slots__ = ()\n\n    @abstractmethod\n    def __float__(self) -> float:\n        pass\n\n\n@runtime_checkable\nclass SupportsComplex(Protocol):\n    """An ABC with one abstract method __complex__."""\n    __slots__ = ()\n\n    @abstractmethod\n    def __complex__(self) -> complex:\n        pass\n\n\n@runtime_checkable\nclass SupportsBytes(Protocol):\n    """An ABC with one abstract method __bytes__."""\n    __slots__ = ()\n\n    @abstractmethod\n    def __bytes__(self) -> bytes:\n        pass\n\n\n@runtime_checkable\nclass SupportsIndex(Protocol):\n    """An ABC with one abstract method __index__."""\n    __slots__ = ()\n\n    @abstractmethod\n    def __index__(self) -> int:\n        pass\n\n\n@runtime_checkable\nclass SupportsAbs(Protocol[T_co]):\n    """An ABC with one abstract method __abs__ that is covariant in its return type."""\n    __slots__ = ()\n\n    @abstractmethod\n    def __abs__(self) -> T_co:\n        pass\n\n\n@runtime_checkable\nclass SupportsRound(Protocol[T_co]):\n    """An ABC with one abstract method __round__ that is covariant in its return type."""\n    __slots__ = ()\n\n    @abstractmethod\n    def __round__(self, ndigits: int = 0) -> T_co:\n        pass\n\n\ndef _make_nmtuple(name, types):\n    msg = "NamedTuple(\'Name\', [(f0, t0), (f1, t1), ...]); each t must be a type"\n    types = [(n, _type_check(t, msg)) for n, t in types]\n    nm_tpl = collections.namedtuple(name, [n for n, t in types])\n    # Prior to PEP 526, only _field_types attribute was assigned.\n    # Now __annotations__ are used and _field_types is deprecated (remove in 3.9)\n    nm_tpl.__annotations__ = nm_tpl._field_types = dict(types)\n    try:\n        nm_tpl.__module__ = sys._getframe(2).f_globals.get(\'__name__\', \'__main__\')\n    except (AttributeError, ValueError):\n        pass\n    return nm_tpl\n\n\n# attributes prohibited to set in NamedTuple class syntax\n_prohibited = (\'__new__\', \'__init__\', \'__slots__\', \'__getnewargs__\',\n               \'_fields\', \'_field_defaults\', \'_field_types\',\n               \'_make\', \'_replace\', \'_asdict\', \'_source\')\n\n_special = (\'__module__\', \'__name__\', \'__annotations__\')\n\n\nclass NamedTupleMeta(type):\n\n    def __new__(cls, typename, bases, ns):\n        if ns.get(\'_root\', False):\n            return super().__new__(cls, typename, bases, ns)\n        types = ns.get(\'__annotations__\', {})\n        nm_tpl = _make_nmtuple(typename, types.items())\n        defaults = []\n        defaults_dict = {}\n        for field_name in types:\n            if field_name in ns:\n                default_value = ns[field_name]\n                defaults.append(default_value)\n                defaults_dict[field_name] = default_value\n            elif defaults:\n                raise TypeError("Non-default namedtuple field {field_name} cannot "\n                                "follow default field(s) {default_names}"\n                                .format(field_name=field_name,\n                                        default_names=\', \'.join(defaults_dict.keys())))\n        nm_tpl.__new__.__annotations__ = dict(types)\n        nm_tpl.__new__.__defaults__ = tuple(defaults)\n        nm_tpl._field_defaults = defaults_dict\n        # update from user namespace without overriding special namedtuple attributes\n        for key in ns:\n            if key in _prohibited:\n                raise AttributeError("Cannot overwrite NamedTuple attribute " + key)\n            elif key not in _special and key not in nm_tpl._fields:\n                setattr(nm_tpl, key, ns[key])\n        return nm_tpl\n\n\nclass NamedTuple(metaclass=NamedTupleMeta):\n    """Typed version of namedtuple.\n\n    Usage in Python versions >= 3.6::\n\n        class Employee(NamedTuple):\n            name: str\n            id: int\n\n    This is equivalent to::\n\n        Employee = collections.namedtuple(\'Employee\', [\'name\', \'id\'])\n\n    The resulting class has an extra __annotations__ attribute, giving a\n    dict that maps field names to types.  (The field names are also in\n    the _fields attribute, which is part of the namedtuple API.)\n    Alternative equivalent keyword syntax is also accepted::\n\n        Employee = NamedTuple(\'Employee\', name=str, id=int)\n\n    In Python versions <= 3.5 use::\n\n        Employee = NamedTuple(\'Employee\', [(\'name\', str), (\'id\', int)])\n    """\n    _root = True\n\n    def __new__(*args, **kwargs):\n        if not args:\n            raise TypeError(\'NamedTuple.__new__(): not enough arguments\')\n        cls, *args = args  # allow the "cls" keyword be passed\n        if args:\n            typename, *args = args # allow the "typename" keyword be passed\n        elif \'typename\' in kwargs:\n            typename = kwargs.pop(\'typename\')\n            import warnings\n            warnings.warn("Passing \'typename\' as keyword argument is deprecated",\n                          DeprecationWarning, stacklevel=2)\n        else:\n            raise TypeError("NamedTuple.__new__() missing 1 required positional "\n                            "argument: \'typename\'")\n        if args:\n            try:\n                fields, = args # allow the "fields" keyword be passed\n            except ValueError:\n                raise TypeError(f\'NamedTuple.__new__() takes from 2 to 3 \'\n                                f\'positional arguments but {len(args) + 2} \'\n                                f\'were given\') from None\n        elif \'fields\' in kwargs and len(kwargs) == 1:\n            fields = kwargs.pop(\'fields\')\n            import warnings\n            warnings.warn("Passing \'fields\' as keyword argument is deprecated",\n                          DeprecationWarning, stacklevel=2)\n        else:\n            fields = None\n\n        if fields is None:\n            fields = kwargs.items()\n        elif kwargs:\n            raise TypeError("Either list of fields or keywords"\n                            " can be provided to NamedTuple, not both")\n        return _make_nmtuple(typename, fields)\n    __new__.__text_signature__ = \'($cls, typename, fields=None, /, **kwargs)\'\n\n\ndef _dict_new(cls, /, *args, **kwargs):\n    return dict(*args, **kwargs)\n\n\ndef _typeddict_new(cls, typename, fields=None, /, *, total=True, **kwargs):\n    if fields is None:\n        fields = kwargs\n    elif kwargs:\n        raise TypeError("TypedDict takes either a dict or keyword arguments,"\n                        " but not both")\n\n    ns = {\'__annotations__\': dict(fields), \'__total__\': total}\n    try:\n        # Setting correct module is necessary to make typed dict classes pickleable.\n        ns[\'__module__\'] = sys._getframe(1).f_globals.get(\'__name__\', \'__main__\')\n    except (AttributeError, ValueError):\n        pass\n\n    return _TypedDictMeta(typename, (), ns)\n\n\ndef _check_fails(cls, other):\n    # Typed dicts are only for static structural subtyping.\n    raise TypeError(\'TypedDict does not support instance and class checks\')\n\n\nclass _TypedDictMeta(type):\n    def __new__(cls, name, bases, ns, total=True):\n        """Create new typed dict class object.\n\n        This method is called directly when TypedDict is subclassed,\n        or via _typeddict_new when TypedDict is instantiated. This way\n        TypedDict supports all three syntax forms described in its docstring.\n        Subclasses and instances of TypedDict return actual dictionaries\n        via _dict_new.\n        """\n        ns[\'__new__\'] = _typeddict_new if name == \'TypedDict\' else _dict_new\n        tp_dict = super(_TypedDictMeta, cls).__new__(cls, name, (dict,), ns)\n\n        anns = ns.get(\'__annotations__\', {})\n        msg = "TypedDict(\'Name\', {f0: t0, f1: t1, ...}); each t must be a type"\n        anns = {n: _type_check(tp, msg) for n, tp in anns.items()}\n        for base in bases:\n            anns.update(base.__dict__.get(\'__annotations__\', {}))\n        tp_dict.__annotations__ = anns\n        if not hasattr(tp_dict, \'__total__\'):\n            tp_dict.__total__ = total\n        return tp_dict\n\n    __instancecheck__ = __subclasscheck__ = _check_fails\n\n\nclass TypedDict(dict, metaclass=_TypedDictMeta):\n    """A simple typed namespace. At runtime it is equivalent to a plain dict.\n\n    TypedDict creates a dictionary type that expects all of its\n    instances to have a certain set of keys, where each key is\n    associated with a value of a consistent type. This expectation\n    is not checked at runtime but is only enforced by type checkers.\n    Usage::\n\n        class Point2D(TypedDict):\n            x: int\n            y: int\n            label: str\n\n        a: Point2D = {\'x\': 1, \'y\': 2, \'label\': \'good\'}  # OK\n        b: Point2D = {\'z\': 3, \'label\': \'bad\'}           # Fails type check\n\n        assert Point2D(x=1, y=2, label=\'first\') == dict(x=1, y=2, label=\'first\')\n\n    The type info can be accessed via Point2D.__annotations__. TypedDict\n    supports two additional equivalent forms::\n\n        Point2D = TypedDict(\'Point2D\', x=int, y=int, label=str)\n        Point2D = TypedDict(\'Point2D\', {\'x\': int, \'y\': int, \'label\': str})\n\n    By default, all keys must be present in a TypedDict. It is possible\n    to override this by specifying totality.\n    Usage::\n\n        class point2D(TypedDict, total=False):\n            x: int\n            y: int\n\n    This means that a point2D TypedDict can have any of the keys omitted.A type\n    checker is only expected to support a literal False or True as the value of\n    the total argument. True is the default, and makes all items defined in the\n    class body be required.\n\n    The class syntax is only supported in Python 3.6+, while two other\n    syntax forms work for Python 2.7 and 3.2+\n    """\n\n\ndef NewType(name, tp):\n    """NewType creates simple unique types with almost zero\n    runtime overhead. NewType(name, tp) is considered a subtype of tp\n    by static type checkers. At runtime, NewType(name, tp) returns\n    a dummy function that simply returns its argument. Usage::\n\n        UserId = NewType(\'UserId\', int)\n\n        def name_by_id(user_id: UserId) -> str:\n            ...\n\n        UserId(\'user\')          # Fails type check\n\n        name_by_id(42)          # Fails type check\n        name_by_id(UserId(42))  # OK\n\n        num = UserId(5) + 1     # type: int\n    """\n\n    def new_type(x):\n        return x\n\n    new_type.__name__ = name\n    new_type.__supertype__ = tp\n    return new_type\n\n\n# Python-version-specific alias (Python 2: unicode; Python 3: str)\nText = str\n\n\n# Constant that\'s True when type checking, but False here.\nTYPE_CHECKING = False\n\n\nclass IO(Generic[AnyStr]):\n    """Generic base class for TextIO and BinaryIO.\n\n    This is an abstract, generic version of the return of open().\n\n    NOTE: This does not distinguish between the different possible\n    classes (text vs. binary, read vs. write vs. read/write,\n    append-only, unbuffered).  The TextIO and BinaryIO subclasses\n    below capture the distinctions between text vs. binary, which is\n    pervasive in the interface; however we currently do not offer a\n    way to track the other distinctions in the type system.\n    """\n\n    __slots__ = ()\n\n    @property\n    @abstractmethod\n    def mode(self) -> str:\n        pass\n\n    @property\n    @abstractmethod\n    def name(self) -> str:\n        pass\n\n    @abstractmethod\n    def close(self) -> None:\n        pass\n\n    @property\n    @abstractmethod\n    def closed(self) -> bool:\n        pass\n\n    @abstractmethod\n    def fileno(self) -> int:\n        pass\n\n    @abstractmethod\n    def flush(self) -> None:\n        pass\n\n    @abstractmethod\n    def isatty(self) -> bool:\n        pass\n\n    @abstractmethod\n    def read(self, n: int = -1) -> AnyStr:\n        pass\n\n    @abstractmethod\n    def readable(self) -> bool:\n        pass\n\n    @abstractmethod\n    def readline(self, limit: int = -1) -> AnyStr:\n        pass\n\n    @abstractmethod\n    def readlines(self, hint: int = -1) -> List[AnyStr]:\n        pass\n\n    @abstractmethod\n    def seek(self, offset: int, whence: int = 0) -> int:\n        pass\n\n    @abstractmethod\n    def seekable(self) -> bool:\n        pass\n\n    @abstractmethod\n    def tell(self) -> int:\n        pass\n\n    @abstractmethod\n    def truncate(self, size: int = None) -> int:\n        pass\n\n    @abstractmethod\n    def writable(self) -> bool:\n        pass\n\n    @abstractmethod\n    def write(self, s: AnyStr) -> int:\n        pass\n\n    @abstractmethod\n    def writelines(self, lines: List[AnyStr]) -> None:\n        pass\n\n    @abstractmethod\n    def __enter__(self) -> \'IO[AnyStr]\':\n        pass\n\n    @abstractmethod\n    def __exit__(self, type, value, traceback) -> None:\n        pass\n\n\nclass BinaryIO(IO[bytes]):\n    """Typed version of the return of open() in binary mode."""\n\n    __slots__ = ()\n\n    @abstractmethod\n    def write(self, s: Union[bytes, bytearray]) -> int:\n        pass\n\n    @abstractmethod\n    def __enter__(self) -> \'BinaryIO\':\n        pass\n\n\nclass TextIO(IO[str]):\n    """Typed version of the return of open() in text mode."""\n\n    __slots__ = ()\n\n    @property\n    @abstractmethod\n    def buffer(self) -> BinaryIO:\n        pass\n\n    @property\n    @abstractmethod\n    def encoding(self) -> str:\n        pass\n\n    @property\n    @abstractmethod\n    def errors(self) -> Optional[str]:\n        pass\n\n    @property\n    @abstractmethod\n    def line_buffering(self) -> bool:\n        pass\n\n    @property\n    @abstractmethod\n    def newlines(self) -> Any:\n        pass\n\n    @abstractmethod\n    def __enter__(self) -> \'TextIO\':\n        pass\n\n\nclass io:\n    """Wrapper namespace for IO generic classes."""\n\n    __all__ = [\'IO\', \'TextIO\', \'BinaryIO\']\n    IO = IO\n    TextIO = TextIO\n    BinaryIO = BinaryIO\n\n\nio.__name__ = __name__ + \'.io\'\nsys.modules[io.__name__] = io\n\nPattern = _alias(stdlib_re.Pattern, AnyStr)\nMatch = _alias(stdlib_re.Match, AnyStr)\n\nclass re:\n    """Wrapper namespace for re type aliases."""\n\n    __all__ = [\'Pattern\', \'Match\']\n    Pattern = Pattern\n    Match = Match\n\n\nre.__name__ = __name__ + \'.re\'\nsys.modules[re.__name__] = re\n')
    __stickytape_write_module('collections/__init__.py', b'\'\'\'This module implements specialized container datatypes providing\nalternatives to Python\'s general purpose built-in containers, dict,\nlist, set, and tuple.\n\n* namedtuple   factory function for creating tuple subclasses with named fields\n* deque        list-like container with fast appends and pops on either end\n* ChainMap     dict-like class for creating a single view of multiple mappings\n* Counter      dict subclass for counting hashable objects\n* OrderedDict  dict subclass that remembers the order entries were added\n* defaultdict  dict subclass that calls a factory function to supply missing values\n* UserDict     wrapper around dictionary objects for easier dict subclassing\n* UserList     wrapper around list objects for easier list subclassing\n* UserString   wrapper around string objects for easier string subclassing\n\n\'\'\'\n\n__all__ = [\'deque\', \'defaultdict\', \'namedtuple\', \'UserDict\', \'UserList\',\n            \'UserString\', \'Counter\', \'OrderedDict\', \'ChainMap\']\n\nimport _collections_abc\nfrom operator import itemgetter as _itemgetter, eq as _eq\nfrom keyword import iskeyword as _iskeyword\nimport sys as _sys\nimport heapq as _heapq\nfrom _weakref import proxy as _proxy\nfrom itertools import repeat as _repeat, chain as _chain, starmap as _starmap\nfrom reprlib import recursive_repr as _recursive_repr\n\ntry:\n    from _collections import deque\nexcept ImportError:\n    pass\nelse:\n    _collections_abc.MutableSequence.register(deque)\n\ntry:\n    from _collections import defaultdict\nexcept ImportError:\n    pass\n\n\ndef __getattr__(name):\n    # For backwards compatibility, continue to make the collections ABCs\n    # through Python 3.6 available through the collections module.\n    # Note, no new collections ABCs were added in Python 3.7\n    if name in _collections_abc.__all__:\n        obj = getattr(_collections_abc, name)\n        import warnings\n        warnings.warn("Using or importing the ABCs from \'collections\' instead "\n                      "of from \'collections.abc\' is deprecated since Python 3.3, "\n                      "and in 3.10 it will stop working",\n                      DeprecationWarning, stacklevel=2)\n        globals()[name] = obj\n        return obj\n    raise AttributeError(f\'module {__name__!r} has no attribute {name!r}\')\n\n################################################################################\n### OrderedDict\n################################################################################\n\nclass _OrderedDictKeysView(_collections_abc.KeysView):\n\n    def __reversed__(self):\n        yield from reversed(self._mapping)\n\nclass _OrderedDictItemsView(_collections_abc.ItemsView):\n\n    def __reversed__(self):\n        for key in reversed(self._mapping):\n            yield (key, self._mapping[key])\n\nclass _OrderedDictValuesView(_collections_abc.ValuesView):\n\n    def __reversed__(self):\n        for key in reversed(self._mapping):\n            yield self._mapping[key]\n\nclass _Link(object):\n    __slots__ = \'prev\', \'next\', \'key\', \'__weakref__\'\n\nclass OrderedDict(dict):\n    \'Dictionary that remembers insertion order\'\n    # An inherited dict maps keys to values.\n    # The inherited dict provides __getitem__, __len__, __contains__, and get.\n    # The remaining methods are order-aware.\n    # Big-O running times for all methods are the same as regular dictionaries.\n\n    # The internal self.__map dict maps keys to links in a doubly linked list.\n    # The circular doubly linked list starts and ends with a sentinel element.\n    # The sentinel element never gets deleted (this simplifies the algorithm).\n    # The sentinel is in self.__hardroot with a weakref proxy in self.__root.\n    # The prev links are weakref proxies (to prevent circular references).\n    # Individual links are kept alive by the hard reference in self.__map.\n    # Those hard references disappear when a key is deleted from an OrderedDict.\n\n    def __init__(self, other=(), /, **kwds):\n        \'\'\'Initialize an ordered dictionary.  The signature is the same as\n        regular dictionaries.  Keyword argument order is preserved.\n        \'\'\'\n        try:\n            self.__root\n        except AttributeError:\n            self.__hardroot = _Link()\n            self.__root = root = _proxy(self.__hardroot)\n            root.prev = root.next = root\n            self.__map = {}\n        self.__update(other, **kwds)\n\n    def __setitem__(self, key, value,\n                    dict_setitem=dict.__setitem__, proxy=_proxy, Link=_Link):\n        \'od.__setitem__(i, y) <==> od[i]=y\'\n        # Setting a new item creates a new link at the end of the linked list,\n        # and the inherited dictionary is updated with the new key/value pair.\n        if key not in self:\n            self.__map[key] = link = Link()\n            root = self.__root\n            last = root.prev\n            link.prev, link.next, link.key = last, root, key\n            last.next = link\n            root.prev = proxy(link)\n        dict_setitem(self, key, value)\n\n    def __delitem__(self, key, dict_delitem=dict.__delitem__):\n        \'od.__delitem__(y) <==> del od[y]\'\n        # Deleting an existing item uses self.__map to find the link which gets\n        # removed by updating the links in the predecessor and successor nodes.\n        dict_delitem(self, key)\n        link = self.__map.pop(key)\n        link_prev = link.prev\n        link_next = link.next\n        link_prev.next = link_next\n        link_next.prev = link_prev\n        link.prev = None\n        link.next = None\n\n    def __iter__(self):\n        \'od.__iter__() <==> iter(od)\'\n        # Traverse the linked list in order.\n        root = self.__root\n        curr = root.next\n        while curr is not root:\n            yield curr.key\n            curr = curr.next\n\n    def __reversed__(self):\n        \'od.__reversed__() <==> reversed(od)\'\n        # Traverse the linked list in reverse order.\n        root = self.__root\n        curr = root.prev\n        while curr is not root:\n            yield curr.key\n            curr = curr.prev\n\n    def clear(self):\n        \'od.clear() -> None.  Remove all items from od.\'\n        root = self.__root\n        root.prev = root.next = root\n        self.__map.clear()\n        dict.clear(self)\n\n    def popitem(self, last=True):\n        \'\'\'Remove and return a (key, value) pair from the dictionary.\n\n        Pairs are returned in LIFO order if last is true or FIFO order if false.\n        \'\'\'\n        if not self:\n            raise KeyError(\'dictionary is empty\')\n        root = self.__root\n        if last:\n            link = root.prev\n            link_prev = link.prev\n            link_prev.next = root\n            root.prev = link_prev\n        else:\n            link = root.next\n            link_next = link.next\n            root.next = link_next\n            link_next.prev = root\n        key = link.key\n        del self.__map[key]\n        value = dict.pop(self, key)\n        return key, value\n\n    def move_to_end(self, key, last=True):\n        \'\'\'Move an existing element to the end (or beginning if last is false).\n\n        Raise KeyError if the element does not exist.\n        \'\'\'\n        link = self.__map[key]\n        link_prev = link.prev\n        link_next = link.next\n        soft_link = link_next.prev\n        link_prev.next = link_next\n        link_next.prev = link_prev\n        root = self.__root\n        if last:\n            last = root.prev\n            link.prev = last\n            link.next = root\n            root.prev = soft_link\n            last.next = link\n        else:\n            first = root.next\n            link.prev = root\n            link.next = first\n            first.prev = soft_link\n            root.next = link\n\n    def __sizeof__(self):\n        sizeof = _sys.getsizeof\n        n = len(self) + 1                       # number of links including root\n        size = sizeof(self.__dict__)            # instance dictionary\n        size += sizeof(self.__map) * 2          # internal dict and inherited dict\n        size += sizeof(self.__hardroot) * n     # link objects\n        size += sizeof(self.__root) * n         # proxy objects\n        return size\n\n    update = __update = _collections_abc.MutableMapping.update\n\n    def keys(self):\n        "D.keys() -> a set-like object providing a view on D\'s keys"\n        return _OrderedDictKeysView(self)\n\n    def items(self):\n        "D.items() -> a set-like object providing a view on D\'s items"\n        return _OrderedDictItemsView(self)\n\n    def values(self):\n        "D.values() -> an object providing a view on D\'s values"\n        return _OrderedDictValuesView(self)\n\n    __ne__ = _collections_abc.MutableMapping.__ne__\n\n    __marker = object()\n\n    def pop(self, key, default=__marker):\n        \'\'\'od.pop(k[,d]) -> v, remove specified key and return the corresponding\n        value.  If key is not found, d is returned if given, otherwise KeyError\n        is raised.\n\n        \'\'\'\n        if key in self:\n            result = self[key]\n            del self[key]\n            return result\n        if default is self.__marker:\n            raise KeyError(key)\n        return default\n\n    def setdefault(self, key, default=None):\n        \'\'\'Insert key with a value of default if key is not in the dictionary.\n\n        Return the value for key if key is in the dictionary, else default.\n        \'\'\'\n        if key in self:\n            return self[key]\n        self[key] = default\n        return default\n\n    @_recursive_repr()\n    def __repr__(self):\n        \'od.__repr__() <==> repr(od)\'\n        if not self:\n            return \'%s()\' % (self.__class__.__name__,)\n        return \'%s(%r)\' % (self.__class__.__name__, list(self.items()))\n\n    def __reduce__(self):\n        \'Return state information for pickling\'\n        inst_dict = vars(self).copy()\n        for k in vars(OrderedDict()):\n            inst_dict.pop(k, None)\n        return self.__class__, (), inst_dict or None, None, iter(self.items())\n\n    def copy(self):\n        \'od.copy() -> a shallow copy of od\'\n        return self.__class__(self)\n\n    @classmethod\n    def fromkeys(cls, iterable, value=None):\n        \'\'\'Create a new ordered dictionary with keys from iterable and values set to value.\n        \'\'\'\n        self = cls()\n        for key in iterable:\n            self[key] = value\n        return self\n\n    def __eq__(self, other):\n        \'\'\'od.__eq__(y) <==> od==y.  Comparison to another OD is order-sensitive\n        while comparison to a regular mapping is order-insensitive.\n\n        \'\'\'\n        if isinstance(other, OrderedDict):\n            return dict.__eq__(self, other) and all(map(_eq, self, other))\n        return dict.__eq__(self, other)\n\n\ntry:\n    from _collections import OrderedDict\nexcept ImportError:\n    # Leave the pure Python version in place.\n    pass\n\n\n################################################################################\n### namedtuple\n################################################################################\n\ntry:\n    from _collections import _tuplegetter\nexcept ImportError:\n    _tuplegetter = lambda index, doc: property(_itemgetter(index), doc=doc)\n\ndef namedtuple(typename, field_names, *, rename=False, defaults=None, module=None):\n    """Returns a new subclass of tuple with named fields.\n\n    >>> Point = namedtuple(\'Point\', [\'x\', \'y\'])\n    >>> Point.__doc__                   # docstring for the new class\n    \'Point(x, y)\'\n    >>> p = Point(11, y=22)             # instantiate with positional args or keywords\n    >>> p[0] + p[1]                     # indexable like a plain tuple\n    33\n    >>> x, y = p                        # unpack like a regular tuple\n    >>> x, y\n    (11, 22)\n    >>> p.x + p.y                       # fields also accessible by name\n    33\n    >>> d = p._asdict()                 # convert to a dictionary\n    >>> d[\'x\']\n    11\n    >>> Point(**d)                      # convert from a dictionary\n    Point(x=11, y=22)\n    >>> p._replace(x=100)               # _replace() is like str.replace() but targets named fields\n    Point(x=100, y=22)\n\n    """\n\n    # Validate the field names.  At the user\'s option, either generate an error\n    # message or automatically replace the field name with a valid name.\n    if isinstance(field_names, str):\n        field_names = field_names.replace(\',\', \' \').split()\n    field_names = list(map(str, field_names))\n    typename = _sys.intern(str(typename))\n\n    if rename:\n        seen = set()\n        for index, name in enumerate(field_names):\n            if (not name.isidentifier()\n                or _iskeyword(name)\n                or name.startswith(\'_\')\n                or name in seen):\n                field_names[index] = f\'_{index}\'\n            seen.add(name)\n\n    for name in [typename] + field_names:\n        if type(name) is not str:\n            raise TypeError(\'Type names and field names must be strings\')\n        if not name.isidentifier():\n            raise ValueError(\'Type names and field names must be valid \'\n                             f\'identifiers: {name!r}\')\n        if _iskeyword(name):\n            raise ValueError(\'Type names and field names cannot be a \'\n                             f\'keyword: {name!r}\')\n\n    seen = set()\n    for name in field_names:\n        if name.startswith(\'_\') and not rename:\n            raise ValueError(\'Field names cannot start with an underscore: \'\n                             f\'{name!r}\')\n        if name in seen:\n            raise ValueError(f\'Encountered duplicate field name: {name!r}\')\n        seen.add(name)\n\n    field_defaults = {}\n    if defaults is not None:\n        defaults = tuple(defaults)\n        if len(defaults) > len(field_names):\n            raise TypeError(\'Got more default values than field names\')\n        field_defaults = dict(reversed(list(zip(reversed(field_names),\n                                                reversed(defaults)))))\n\n    # Variables used in the methods and docstrings\n    field_names = tuple(map(_sys.intern, field_names))\n    num_fields = len(field_names)\n    arg_list = repr(field_names).replace("\'", "")[1:-1]\n    repr_fmt = \'(\' + \', \'.join(f\'{name}=%r\' for name in field_names) + \')\'\n    tuple_new = tuple.__new__\n    _dict, _tuple, _len, _map, _zip = dict, tuple, len, map, zip\n\n    # Create all the named tuple methods to be added to the class namespace\n\n    s = f\'def __new__(_cls, {arg_list}): return _tuple_new(_cls, ({arg_list}))\'\n    namespace = {\'_tuple_new\': tuple_new, \'__name__\': f\'namedtuple_{typename}\'}\n    # Note: exec() has the side-effect of interning the field names\n    exec(s, namespace)\n    __new__ = namespace[\'__new__\']\n    __new__.__doc__ = f\'Create new instance of {typename}({arg_list})\'\n    if defaults is not None:\n        __new__.__defaults__ = defaults\n\n    @classmethod\n    def _make(cls, iterable):\n        result = tuple_new(cls, iterable)\n        if _len(result) != num_fields:\n            raise TypeError(f\'Expected {num_fields} arguments, got {len(result)}\')\n        return result\n\n    _make.__func__.__doc__ = (f\'Make a new {typename} object from a sequence \'\n                              \'or iterable\')\n\n    def _replace(self, /, **kwds):\n        result = self._make(_map(kwds.pop, field_names, self))\n        if kwds:\n            raise ValueError(f\'Got unexpected field names: {list(kwds)!r}\')\n        return result\n\n    _replace.__doc__ = (f\'Return a new {typename} object replacing specified \'\n                        \'fields with new values\')\n\n    def __repr__(self):\n        \'Return a nicely formatted representation string\'\n        return self.__class__.__name__ + repr_fmt % self\n\n    def _asdict(self):\n        \'Return a new dict which maps field names to their values.\'\n        return _dict(_zip(self._fields, self))\n\n    def __getnewargs__(self):\n        \'Return self as a plain tuple.  Used by copy and pickle.\'\n        return _tuple(self)\n\n    # Modify function metadata to help with introspection and debugging\n    for method in (__new__, _make.__func__, _replace,\n                   __repr__, _asdict, __getnewargs__):\n        method.__qualname__ = f\'{typename}.{method.__name__}\'\n\n    # Build-up the class namespace dictionary\n    # and use type() to build the result class\n    class_namespace = {\n        \'__doc__\': f\'{typename}({arg_list})\',\n        \'__slots__\': (),\n        \'_fields\': field_names,\n        \'_field_defaults\': field_defaults,\n        # alternate spelling for backward compatibility\n        \'_fields_defaults\': field_defaults,\n        \'__new__\': __new__,\n        \'_make\': _make,\n        \'_replace\': _replace,\n        \'__repr__\': __repr__,\n        \'_asdict\': _asdict,\n        \'__getnewargs__\': __getnewargs__,\n    }\n    for index, name in enumerate(field_names):\n        doc = _sys.intern(f\'Alias for field number {index}\')\n        class_namespace[name] = _tuplegetter(index, doc)\n\n    result = type(typename, (tuple,), class_namespace)\n\n    # For pickling to work, the __module__ variable needs to be set to the frame\n    # where the named tuple is created.  Bypass this step in environments where\n    # sys._getframe is not defined (Jython for example) or sys._getframe is not\n    # defined for arguments greater than 0 (IronPython), or where the user has\n    # specified a particular module.\n    if module is None:\n        try:\n            module = _sys._getframe(1).f_globals.get(\'__name__\', \'__main__\')\n        except (AttributeError, ValueError):\n            pass\n    if module is not None:\n        result.__module__ = module\n\n    return result\n\n\n########################################################################\n###  Counter\n########################################################################\n\ndef _count_elements(mapping, iterable):\n    \'Tally elements from the iterable.\'\n    mapping_get = mapping.get\n    for elem in iterable:\n        mapping[elem] = mapping_get(elem, 0) + 1\n\ntry:                                    # Load C helper function if available\n    from _collections import _count_elements\nexcept ImportError:\n    pass\n\nclass Counter(dict):\n    \'\'\'Dict subclass for counting hashable items.  Sometimes called a bag\n    or multiset.  Elements are stored as dictionary keys and their counts\n    are stored as dictionary values.\n\n    >>> c = Counter(\'abcdeabcdabcaba\')  # count elements from a string\n\n    >>> c.most_common(3)                # three most common elements\n    [(\'a\', 5), (\'b\', 4), (\'c\', 3)]\n    >>> sorted(c)                       # list all unique elements\n    [\'a\', \'b\', \'c\', \'d\', \'e\']\n    >>> \'\'.join(sorted(c.elements()))   # list elements with repetitions\n    \'aaaaabbbbcccdde\'\n    >>> sum(c.values())                 # total of all counts\n    15\n\n    >>> c[\'a\']                          # count of letter \'a\'\n    5\n    >>> for elem in \'shazam\':           # update counts from an iterable\n    ...     c[elem] += 1                # by adding 1 to each element\'s count\n    >>> c[\'a\']                          # now there are seven \'a\'\n    7\n    >>> del c[\'b\']                      # remove all \'b\'\n    >>> c[\'b\']                          # now there are zero \'b\'\n    0\n\n    >>> d = Counter(\'simsalabim\')       # make another counter\n    >>> c.update(d)                     # add in the second counter\n    >>> c[\'a\']                          # now there are nine \'a\'\n    9\n\n    >>> c.clear()                       # empty the counter\n    >>> c\n    Counter()\n\n    Note:  If a count is set to zero or reduced to zero, it will remain\n    in the counter until the entry is deleted or the counter is cleared:\n\n    >>> c = Counter(\'aaabbc\')\n    >>> c[\'b\'] -= 2                     # reduce the count of \'b\' by two\n    >>> c.most_common()                 # \'b\' is still in, but its count is zero\n    [(\'a\', 3), (\'c\', 1), (\'b\', 0)]\n\n    \'\'\'\n    # References:\n    #   http://en.wikipedia.org/wiki/Multiset\n    #   http://www.gnu.org/software/smalltalk/manual-base/html_node/Bag.html\n    #   http://www.demo2s.com/Tutorial/Cpp/0380__set-multiset/Catalog0380__set-multiset.htm\n    #   http://code.activestate.com/recipes/259174/\n    #   Knuth, TAOCP Vol. II section 4.6.3\n\n    def __init__(self, iterable=None, /, **kwds):\n        \'\'\'Create a new, empty Counter object.  And if given, count elements\n        from an input iterable.  Or, initialize the count from another mapping\n        of elements to their counts.\n\n        >>> c = Counter()                           # a new, empty counter\n        >>> c = Counter(\'gallahad\')                 # a new counter from an iterable\n        >>> c = Counter({\'a\': 4, \'b\': 2})           # a new counter from a mapping\n        >>> c = Counter(a=4, b=2)                   # a new counter from keyword args\n\n        \'\'\'\n        super(Counter, self).__init__()\n        self.update(iterable, **kwds)\n\n    def __missing__(self, key):\n        \'The count of elements not in the Counter is zero.\'\n        # Needed so that self[missing_item] does not raise KeyError\n        return 0\n\n    def most_common(self, n=None):\n        \'\'\'List the n most common elements and their counts from the most\n        common to the least.  If n is None, then list all element counts.\n\n        >>> Counter(\'abracadabra\').most_common(3)\n        [(\'a\', 5), (\'b\', 2), (\'r\', 2)]\n\n        \'\'\'\n        # Emulate Bag.sortedByCount from Smalltalk\n        if n is None:\n            return sorted(self.items(), key=_itemgetter(1), reverse=True)\n        return _heapq.nlargest(n, self.items(), key=_itemgetter(1))\n\n    def elements(self):\n        \'\'\'Iterator over elements repeating each as many times as its count.\n\n        >>> c = Counter(\'ABCABC\')\n        >>> sorted(c.elements())\n        [\'A\', \'A\', \'B\', \'B\', \'C\', \'C\']\n\n        # Knuth\'s example for prime factors of 1836:  2**2 * 3**3 * 17**1\n        >>> prime_factors = Counter({2: 2, 3: 3, 17: 1})\n        >>> product = 1\n        >>> for factor in prime_factors.elements():     # loop over factors\n        ...     product *= factor                       # and multiply them\n        >>> product\n        1836\n\n        Note, if an element\'s count has been set to zero or is a negative\n        number, elements() will ignore it.\n\n        \'\'\'\n        # Emulate Bag.do from Smalltalk and Multiset.begin from C++.\n        return _chain.from_iterable(_starmap(_repeat, self.items()))\n\n    # Override dict methods where necessary\n\n    @classmethod\n    def fromkeys(cls, iterable, v=None):\n        # There is no equivalent method for counters because the semantics\n        # would be ambiguous in cases such as Counter.fromkeys(\'aaabbc\', v=2).\n        # Initializing counters to zero values isn\'t necessary because zero\n        # is already the default value for counter lookups.  Initializing\n        # to one is easily accomplished with Counter(set(iterable)).  For\n        # more exotic cases, create a dictionary first using a dictionary\n        # comprehension or dict.fromkeys().\n        raise NotImplementedError(\n            \'Counter.fromkeys() is undefined.  Use Counter(iterable) instead.\')\n\n    def update(self, iterable=None, /, **kwds):\n        \'\'\'Like dict.update() but add counts instead of replacing them.\n\n        Source can be an iterable, a dictionary, or another Counter instance.\n\n        >>> c = Counter(\'which\')\n        >>> c.update(\'witch\')           # add elements from another iterable\n        >>> d = Counter(\'watch\')\n        >>> c.update(d)                 # add elements from another counter\n        >>> c[\'h\']                      # four \'h\' in which, witch, and watch\n        4\n\n        \'\'\'\n        # The regular dict.update() operation makes no sense here because the\n        # replace behavior results in the some of original untouched counts\n        # being mixed-in with all of the other counts for a mismash that\n        # doesn\'t have a straight-forward interpretation in most counting\n        # contexts.  Instead, we implement straight-addition.  Both the inputs\n        # and outputs are allowed to contain zero and negative counts.\n\n        if iterable is not None:\n            if isinstance(iterable, _collections_abc.Mapping):\n                if self:\n                    self_get = self.get\n                    for elem, count in iterable.items():\n                        self[elem] = count + self_get(elem, 0)\n                else:\n                    super(Counter, self).update(iterable) # fast path when counter is empty\n            else:\n                _count_elements(self, iterable)\n        if kwds:\n            self.update(kwds)\n\n    def subtract(self, iterable=None, /, **kwds):\n        \'\'\'Like dict.update() but subtracts counts instead of replacing them.\n        Counts can be reduced below zero.  Both the inputs and outputs are\n        allowed to contain zero and negative counts.\n\n        Source can be an iterable, a dictionary, or another Counter instance.\n\n        >>> c = Counter(\'which\')\n        >>> c.subtract(\'witch\')             # subtract elements from another iterable\n        >>> c.subtract(Counter(\'watch\'))    # subtract elements from another counter\n        >>> c[\'h\']                          # 2 in which, minus 1 in witch, minus 1 in watch\n        0\n        >>> c[\'w\']                          # 1 in which, minus 1 in witch, minus 1 in watch\n        -1\n\n        \'\'\'\n        if iterable is not None:\n            self_get = self.get\n            if isinstance(iterable, _collections_abc.Mapping):\n                for elem, count in iterable.items():\n                    self[elem] = self_get(elem, 0) - count\n            else:\n                for elem in iterable:\n                    self[elem] = self_get(elem, 0) - 1\n        if kwds:\n            self.subtract(kwds)\n\n    def copy(self):\n        \'Return a shallow copy.\'\n        return self.__class__(self)\n\n    def __reduce__(self):\n        return self.__class__, (dict(self),)\n\n    def __delitem__(self, elem):\n        \'Like dict.__delitem__() but does not raise KeyError for missing values.\'\n        if elem in self:\n            super().__delitem__(elem)\n\n    def __repr__(self):\n        if not self:\n            return \'%s()\' % self.__class__.__name__\n        try:\n            items = \', \'.join(map(\'%r: %r\'.__mod__, self.most_common()))\n            return \'%s({%s})\' % (self.__class__.__name__, items)\n        except TypeError:\n            # handle case where values are not orderable\n            return \'{0}({1!r})\'.format(self.__class__.__name__, dict(self))\n\n    # Multiset-style mathematical operations discussed in:\n    #       Knuth TAOCP Volume II section 4.6.3 exercise 19\n    #       and at http://en.wikipedia.org/wiki/Multiset\n    #\n    # Outputs guaranteed to only include positive counts.\n    #\n    # To strip negative and zero counts, add-in an empty counter:\n    #       c += Counter()\n    #\n    # Rich comparison operators for multiset subset and superset tests\n    # are deliberately omitted due to semantic conflicts with the\n    # existing inherited dict equality method.  Subset and superset\n    # semantics ignore zero counts and require that p\xe2\x89\xa4q \xe2\x88\xa7 p\xe2\x89\xa5q \xe2\x86\x92 p=q;\n    # however, that would not be the case for p=Counter(a=1, b=0)\n    # and q=Counter(a=1) where the dictionaries are not equal.\n\n    def __add__(self, other):\n        \'\'\'Add counts from two counters.\n\n        >>> Counter(\'abbb\') + Counter(\'bcc\')\n        Counter({\'b\': 4, \'c\': 2, \'a\': 1})\n\n        \'\'\'\n        if not isinstance(other, Counter):\n            return NotImplemented\n        result = Counter()\n        for elem, count in self.items():\n            newcount = count + other[elem]\n            if newcount > 0:\n                result[elem] = newcount\n        for elem, count in other.items():\n            if elem not in self and count > 0:\n                result[elem] = count\n        return result\n\n    def __sub__(self, other):\n        \'\'\' Subtract count, but keep only results with positive counts.\n\n        >>> Counter(\'abbbc\') - Counter(\'bccd\')\n        Counter({\'b\': 2, \'a\': 1})\n\n        \'\'\'\n        if not isinstance(other, Counter):\n            return NotImplemented\n        result = Counter()\n        for elem, count in self.items():\n            newcount = count - other[elem]\n            if newcount > 0:\n                result[elem] = newcount\n        for elem, count in other.items():\n            if elem not in self and count < 0:\n                result[elem] = 0 - count\n        return result\n\n    def __or__(self, other):\n        \'\'\'Union is the maximum of value in either of the input counters.\n\n        >>> Counter(\'abbb\') | Counter(\'bcc\')\n        Counter({\'b\': 3, \'c\': 2, \'a\': 1})\n\n        \'\'\'\n        if not isinstance(other, Counter):\n            return NotImplemented\n        result = Counter()\n        for elem, count in self.items():\n            other_count = other[elem]\n            newcount = other_count if count < other_count else count\n            if newcount > 0:\n                result[elem] = newcount\n        for elem, count in other.items():\n            if elem not in self and count > 0:\n                result[elem] = count\n        return result\n\n    def __and__(self, other):\n        \'\'\' Intersection is the minimum of corresponding counts.\n\n        >>> Counter(\'abbb\') & Counter(\'bcc\')\n        Counter({\'b\': 1})\n\n        \'\'\'\n        if not isinstance(other, Counter):\n            return NotImplemented\n        result = Counter()\n        for elem, count in self.items():\n            other_count = other[elem]\n            newcount = count if count < other_count else other_count\n            if newcount > 0:\n                result[elem] = newcount\n        return result\n\n    def __pos__(self):\n        \'Adds an empty counter, effectively stripping negative and zero counts\'\n        result = Counter()\n        for elem, count in self.items():\n            if count > 0:\n                result[elem] = count\n        return result\n\n    def __neg__(self):\n        \'\'\'Subtracts from an empty counter.  Strips positive and zero counts,\n        and flips the sign on negative counts.\n\n        \'\'\'\n        result = Counter()\n        for elem, count in self.items():\n            if count < 0:\n                result[elem] = 0 - count\n        return result\n\n    def _keep_positive(self):\n        \'\'\'Internal method to strip elements with a negative or zero count\'\'\'\n        nonpositive = [elem for elem, count in self.items() if not count > 0]\n        for elem in nonpositive:\n            del self[elem]\n        return self\n\n    def __iadd__(self, other):\n        \'\'\'Inplace add from another counter, keeping only positive counts.\n\n        >>> c = Counter(\'abbb\')\n        >>> c += Counter(\'bcc\')\n        >>> c\n        Counter({\'b\': 4, \'c\': 2, \'a\': 1})\n\n        \'\'\'\n        for elem, count in other.items():\n            self[elem] += count\n        return self._keep_positive()\n\n    def __isub__(self, other):\n        \'\'\'Inplace subtract counter, but keep only results with positive counts.\n\n        >>> c = Counter(\'abbbc\')\n        >>> c -= Counter(\'bccd\')\n        >>> c\n        Counter({\'b\': 2, \'a\': 1})\n\n        \'\'\'\n        for elem, count in other.items():\n            self[elem] -= count\n        return self._keep_positive()\n\n    def __ior__(self, other):\n        \'\'\'Inplace union is the maximum of value from either counter.\n\n        >>> c = Counter(\'abbb\')\n        >>> c |= Counter(\'bcc\')\n        >>> c\n        Counter({\'b\': 3, \'c\': 2, \'a\': 1})\n\n        \'\'\'\n        for elem, other_count in other.items():\n            count = self[elem]\n            if other_count > count:\n                self[elem] = other_count\n        return self._keep_positive()\n\n    def __iand__(self, other):\n        \'\'\'Inplace intersection is the minimum of corresponding counts.\n\n        >>> c = Counter(\'abbb\')\n        >>> c &= Counter(\'bcc\')\n        >>> c\n        Counter({\'b\': 1})\n\n        \'\'\'\n        for elem, count in self.items():\n            other_count = other[elem]\n            if other_count < count:\n                self[elem] = other_count\n        return self._keep_positive()\n\n\n########################################################################\n###  ChainMap\n########################################################################\n\nclass ChainMap(_collections_abc.MutableMapping):\n    \'\'\' A ChainMap groups multiple dicts (or other mappings) together\n    to create a single, updateable view.\n\n    The underlying mappings are stored in a list.  That list is public and can\n    be accessed or updated using the *maps* attribute.  There is no other\n    state.\n\n    Lookups search the underlying mappings successively until a key is found.\n    In contrast, writes, updates, and deletions only operate on the first\n    mapping.\n\n    \'\'\'\n\n    def __init__(self, *maps):\n        \'\'\'Initialize a ChainMap by setting *maps* to the given mappings.\n        If no mappings are provided, a single empty dictionary is used.\n\n        \'\'\'\n        self.maps = list(maps) or [{}]          # always at least one map\n\n    def __missing__(self, key):\n        raise KeyError(key)\n\n    def __getitem__(self, key):\n        for mapping in self.maps:\n            try:\n                return mapping[key]             # can\'t use \'key in mapping\' with defaultdict\n            except KeyError:\n                pass\n        return self.__missing__(key)            # support subclasses that define __missing__\n\n    def get(self, key, default=None):\n        return self[key] if key in self else default\n\n    def __len__(self):\n        return len(set().union(*self.maps))     # reuses stored hash values if possible\n\n    def __iter__(self):\n        d = {}\n        for mapping in reversed(self.maps):\n            d.update(mapping)                   # reuses stored hash values if possible\n        return iter(d)\n\n    def __contains__(self, key):\n        return any(key in m for m in self.maps)\n\n    def __bool__(self):\n        return any(self.maps)\n\n    @_recursive_repr()\n    def __repr__(self):\n        return f\'{self.__class__.__name__}({", ".join(map(repr, self.maps))})\'\n\n    @classmethod\n    def fromkeys(cls, iterable, *args):\n        \'Create a ChainMap with a single dict created from the iterable.\'\n        return cls(dict.fromkeys(iterable, *args))\n\n    def copy(self):\n        \'New ChainMap or subclass with a new copy of maps[0] and refs to maps[1:]\'\n        return self.__class__(self.maps[0].copy(), *self.maps[1:])\n\n    __copy__ = copy\n\n    def new_child(self, m=None):                # like Django\'s Context.push()\n        \'\'\'New ChainMap with a new map followed by all previous maps.\n        If no map is provided, an empty dict is used.\n        \'\'\'\n        if m is None:\n            m = {}\n        return self.__class__(m, *self.maps)\n\n    @property\n    def parents(self):                          # like Django\'s Context.pop()\n        \'New ChainMap from maps[1:].\'\n        return self.__class__(*self.maps[1:])\n\n    def __setitem__(self, key, value):\n        self.maps[0][key] = value\n\n    def __delitem__(self, key):\n        try:\n            del self.maps[0][key]\n        except KeyError:\n            raise KeyError(\'Key not found in the first mapping: {!r}\'.format(key))\n\n    def popitem(self):\n        \'Remove and return an item pair from maps[0]. Raise KeyError is maps[0] is empty.\'\n        try:\n            return self.maps[0].popitem()\n        except KeyError:\n            raise KeyError(\'No keys found in the first mapping.\')\n\n    def pop(self, key, *args):\n        \'Remove *key* from maps[0] and return its value. Raise KeyError if *key* not in maps[0].\'\n        try:\n            return self.maps[0].pop(key, *args)\n        except KeyError:\n            raise KeyError(\'Key not found in the first mapping: {!r}\'.format(key))\n\n    def clear(self):\n        \'Clear maps[0], leaving maps[1:] intact.\'\n        self.maps[0].clear()\n\n\n################################################################################\n### UserDict\n################################################################################\n\nclass UserDict(_collections_abc.MutableMapping):\n\n    # Start by filling-out the abstract methods\n    def __init__(*args, **kwargs):\n        if not args:\n            raise TypeError("descriptor \'__init__\' of \'UserDict\' object "\n                            "needs an argument")\n        self, *args = args\n        if len(args) > 1:\n            raise TypeError(\'expected at most 1 arguments, got %d\' % len(args))\n        if args:\n            dict = args[0]\n        elif \'dict\' in kwargs:\n            dict = kwargs.pop(\'dict\')\n            import warnings\n            warnings.warn("Passing \'dict\' as keyword argument is deprecated",\n                          DeprecationWarning, stacklevel=2)\n        else:\n            dict = None\n        self.data = {}\n        if dict is not None:\n            self.update(dict)\n        if kwargs:\n            self.update(kwargs)\n    __init__.__text_signature__ = \'($self, dict=None, /, **kwargs)\'\n\n    def __len__(self): return len(self.data)\n    def __getitem__(self, key):\n        if key in self.data:\n            return self.data[key]\n        if hasattr(self.__class__, "__missing__"):\n            return self.__class__.__missing__(self, key)\n        raise KeyError(key)\n    def __setitem__(self, key, item): self.data[key] = item\n    def __delitem__(self, key): del self.data[key]\n    def __iter__(self):\n        return iter(self.data)\n\n    # Modify __contains__ to work correctly when __missing__ is present\n    def __contains__(self, key):\n        return key in self.data\n\n    # Now, add the methods in dicts but not in MutableMapping\n    def __repr__(self): return repr(self.data)\n    def __copy__(self):\n        inst = self.__class__.__new__(self.__class__)\n        inst.__dict__.update(self.__dict__)\n        # Create a copy and avoid triggering descriptors\n        inst.__dict__["data"] = self.__dict__["data"].copy()\n        return inst\n\n    def copy(self):\n        if self.__class__ is UserDict:\n            return UserDict(self.data.copy())\n        import copy\n        data = self.data\n        try:\n            self.data = {}\n            c = copy.copy(self)\n        finally:\n            self.data = data\n        c.update(self)\n        return c\n\n    @classmethod\n    def fromkeys(cls, iterable, value=None):\n        d = cls()\n        for key in iterable:\n            d[key] = value\n        return d\n\n\n\n################################################################################\n### UserList\n################################################################################\n\nclass UserList(_collections_abc.MutableSequence):\n    """A more or less complete user-defined wrapper around list objects."""\n    def __init__(self, initlist=None):\n        self.data = []\n        if initlist is not None:\n            # XXX should this accept an arbitrary sequence?\n            if type(initlist) == type(self.data):\n                self.data[:] = initlist\n            elif isinstance(initlist, UserList):\n                self.data[:] = initlist.data[:]\n            else:\n                self.data = list(initlist)\n    def __repr__(self): return repr(self.data)\n    def __lt__(self, other): return self.data <  self.__cast(other)\n    def __le__(self, other): return self.data <= self.__cast(other)\n    def __eq__(self, other): return self.data == self.__cast(other)\n    def __gt__(self, other): return self.data >  self.__cast(other)\n    def __ge__(self, other): return self.data >= self.__cast(other)\n    def __cast(self, other):\n        return other.data if isinstance(other, UserList) else other\n    def __contains__(self, item): return item in self.data\n    def __len__(self): return len(self.data)\n    def __getitem__(self, i):\n        if isinstance(i, slice):\n            return self.__class__(self.data[i])\n        else:\n            return self.data[i]\n    def __setitem__(self, i, item): self.data[i] = item\n    def __delitem__(self, i): del self.data[i]\n    def __add__(self, other):\n        if isinstance(other, UserList):\n            return self.__class__(self.data + other.data)\n        elif isinstance(other, type(self.data)):\n            return self.__class__(self.data + other)\n        return self.__class__(self.data + list(other))\n    def __radd__(self, other):\n        if isinstance(other, UserList):\n            return self.__class__(other.data + self.data)\n        elif isinstance(other, type(self.data)):\n            return self.__class__(other + self.data)\n        return self.__class__(list(other) + self.data)\n    def __iadd__(self, other):\n        if isinstance(other, UserList):\n            self.data += other.data\n        elif isinstance(other, type(self.data)):\n            self.data += other\n        else:\n            self.data += list(other)\n        return self\n    def __mul__(self, n):\n        return self.__class__(self.data*n)\n    __rmul__ = __mul__\n    def __imul__(self, n):\n        self.data *= n\n        return self\n    def __copy__(self):\n        inst = self.__class__.__new__(self.__class__)\n        inst.__dict__.update(self.__dict__)\n        # Create a copy and avoid triggering descriptors\n        inst.__dict__["data"] = self.__dict__["data"][:]\n        return inst\n    def append(self, item): self.data.append(item)\n    def insert(self, i, item): self.data.insert(i, item)\n    def pop(self, i=-1): return self.data.pop(i)\n    def remove(self, item): self.data.remove(item)\n    def clear(self): self.data.clear()\n    def copy(self): return self.__class__(self)\n    def count(self, item): return self.data.count(item)\n    def index(self, item, *args): return self.data.index(item, *args)\n    def reverse(self): self.data.reverse()\n    def sort(self, /, *args, **kwds): self.data.sort(*args, **kwds)\n    def extend(self, other):\n        if isinstance(other, UserList):\n            self.data.extend(other.data)\n        else:\n            self.data.extend(other)\n\n\n\n################################################################################\n### UserString\n################################################################################\n\nclass UserString(_collections_abc.Sequence):\n    def __init__(self, seq):\n        if isinstance(seq, str):\n            self.data = seq\n        elif isinstance(seq, UserString):\n            self.data = seq.data[:]\n        else:\n            self.data = str(seq)\n    def __str__(self): return str(self.data)\n    def __repr__(self): return repr(self.data)\n    def __int__(self): return int(self.data)\n    def __float__(self): return float(self.data)\n    def __complex__(self): return complex(self.data)\n    def __hash__(self): return hash(self.data)\n    def __getnewargs__(self):\n        return (self.data[:],)\n\n    def __eq__(self, string):\n        if isinstance(string, UserString):\n            return self.data == string.data\n        return self.data == string\n    def __lt__(self, string):\n        if isinstance(string, UserString):\n            return self.data < string.data\n        return self.data < string\n    def __le__(self, string):\n        if isinstance(string, UserString):\n            return self.data <= string.data\n        return self.data <= string\n    def __gt__(self, string):\n        if isinstance(string, UserString):\n            return self.data > string.data\n        return self.data > string\n    def __ge__(self, string):\n        if isinstance(string, UserString):\n            return self.data >= string.data\n        return self.data >= string\n\n    def __contains__(self, char):\n        if isinstance(char, UserString):\n            char = char.data\n        return char in self.data\n\n    def __len__(self): return len(self.data)\n    def __getitem__(self, index): return self.__class__(self.data[index])\n    def __add__(self, other):\n        if isinstance(other, UserString):\n            return self.__class__(self.data + other.data)\n        elif isinstance(other, str):\n            return self.__class__(self.data + other)\n        return self.__class__(self.data + str(other))\n    def __radd__(self, other):\n        if isinstance(other, str):\n            return self.__class__(other + self.data)\n        return self.__class__(str(other) + self.data)\n    def __mul__(self, n):\n        return self.__class__(self.data*n)\n    __rmul__ = __mul__\n    def __mod__(self, args):\n        return self.__class__(self.data % args)\n    def __rmod__(self, template):\n        return self.__class__(str(template) % self)\n    # the following methods are defined in alphabetical order:\n    def capitalize(self): return self.__class__(self.data.capitalize())\n    def casefold(self):\n        return self.__class__(self.data.casefold())\n    def center(self, width, *args):\n        return self.__class__(self.data.center(width, *args))\n    def count(self, sub, start=0, end=_sys.maxsize):\n        if isinstance(sub, UserString):\n            sub = sub.data\n        return self.data.count(sub, start, end)\n    def encode(self, encoding=\'utf-8\', errors=\'strict\'):\n        encoding = \'utf-8\' if encoding is None else encoding\n        errors = \'strict\' if errors is None else errors\n        return self.data.encode(encoding, errors)\n    def endswith(self, suffix, start=0, end=_sys.maxsize):\n        return self.data.endswith(suffix, start, end)\n    def expandtabs(self, tabsize=8):\n        return self.__class__(self.data.expandtabs(tabsize))\n    def find(self, sub, start=0, end=_sys.maxsize):\n        if isinstance(sub, UserString):\n            sub = sub.data\n        return self.data.find(sub, start, end)\n    def format(self, /, *args, **kwds):\n        return self.data.format(*args, **kwds)\n    def format_map(self, mapping):\n        return self.data.format_map(mapping)\n    def index(self, sub, start=0, end=_sys.maxsize):\n        return self.data.index(sub, start, end)\n    def isalpha(self): return self.data.isalpha()\n    def isalnum(self): return self.data.isalnum()\n    def isascii(self): return self.data.isascii()\n    def isdecimal(self): return self.data.isdecimal()\n    def isdigit(self): return self.data.isdigit()\n    def isidentifier(self): return self.data.isidentifier()\n    def islower(self): return self.data.islower()\n    def isnumeric(self): return self.data.isnumeric()\n    def isprintable(self): return self.data.isprintable()\n    def isspace(self): return self.data.isspace()\n    def istitle(self): return self.data.istitle()\n    def isupper(self): return self.data.isupper()\n    def join(self, seq): return self.data.join(seq)\n    def ljust(self, width, *args):\n        return self.__class__(self.data.ljust(width, *args))\n    def lower(self): return self.__class__(self.data.lower())\n    def lstrip(self, chars=None): return self.__class__(self.data.lstrip(chars))\n    maketrans = str.maketrans\n    def partition(self, sep):\n        return self.data.partition(sep)\n    def replace(self, old, new, maxsplit=-1):\n        if isinstance(old, UserString):\n            old = old.data\n        if isinstance(new, UserString):\n            new = new.data\n        return self.__class__(self.data.replace(old, new, maxsplit))\n    def rfind(self, sub, start=0, end=_sys.maxsize):\n        if isinstance(sub, UserString):\n            sub = sub.data\n        return self.data.rfind(sub, start, end)\n    def rindex(self, sub, start=0, end=_sys.maxsize):\n        return self.data.rindex(sub, start, end)\n    def rjust(self, width, *args):\n        return self.__class__(self.data.rjust(width, *args))\n    def rpartition(self, sep):\n        return self.data.rpartition(sep)\n    def rstrip(self, chars=None):\n        return self.__class__(self.data.rstrip(chars))\n    def split(self, sep=None, maxsplit=-1):\n        return self.data.split(sep, maxsplit)\n    def rsplit(self, sep=None, maxsplit=-1):\n        return self.data.rsplit(sep, maxsplit)\n    def splitlines(self, keepends=False): return self.data.splitlines(keepends)\n    def startswith(self, prefix, start=0, end=_sys.maxsize):\n        return self.data.startswith(prefix, start, end)\n    def strip(self, chars=None): return self.__class__(self.data.strip(chars))\n    def swapcase(self): return self.__class__(self.data.swapcase())\n    def title(self): return self.__class__(self.data.title())\n    def translate(self, *args):\n        return self.__class__(self.data.translate(*args))\n    def upper(self): return self.__class__(self.data.upper())\n    def zfill(self, width): return self.__class__(self.data.zfill(width))\n')
    __stickytape_write_module('_collections_abc.py', b'# Copyright 2007 Google, Inc. All Rights Reserved.\n# Licensed to PSF under a Contributor Agreement.\n\n"""Abstract Base Classes (ABCs) for collections, according to PEP 3119.\n\nUnit tests are in test_collections.\n"""\n\nfrom abc import ABCMeta, abstractmethod\nimport sys\n\n__all__ = ["Awaitable", "Coroutine",\n           "AsyncIterable", "AsyncIterator", "AsyncGenerator",\n           "Hashable", "Iterable", "Iterator", "Generator", "Reversible",\n           "Sized", "Container", "Callable", "Collection",\n           "Set", "MutableSet",\n           "Mapping", "MutableMapping",\n           "MappingView", "KeysView", "ItemsView", "ValuesView",\n           "Sequence", "MutableSequence",\n           "ByteString",\n           ]\n\n# This module has been renamed from collections.abc to _collections_abc to\n# speed up interpreter startup. Some of the types such as MutableMapping are\n# required early but collections module imports a lot of other modules.\n# See issue #19218\n__name__ = "collections.abc"\n\n# Private list of types that we want to register with the various ABCs\n# so that they will pass tests like:\n#       it = iter(somebytearray)\n#       assert isinstance(it, Iterable)\n# Note:  in other implementations, these types might not be distinct\n# and they may have their own implementation specific types that\n# are not included on this list.\nbytes_iterator = type(iter(b\'\'))\nbytearray_iterator = type(iter(bytearray()))\n#callable_iterator = ???\ndict_keyiterator = type(iter({}.keys()))\ndict_valueiterator = type(iter({}.values()))\ndict_itemiterator = type(iter({}.items()))\nlist_iterator = type(iter([]))\nlist_reverseiterator = type(iter(reversed([])))\nrange_iterator = type(iter(range(0)))\nlongrange_iterator = type(iter(range(1 << 1000)))\nset_iterator = type(iter(set()))\nstr_iterator = type(iter(""))\ntuple_iterator = type(iter(()))\nzip_iterator = type(iter(zip()))\n## views ##\ndict_keys = type({}.keys())\ndict_values = type({}.values())\ndict_items = type({}.items())\n## misc ##\nmappingproxy = type(type.__dict__)\ngenerator = type((lambda: (yield))())\n## coroutine ##\nasync def _coro(): pass\n_coro = _coro()\ncoroutine = type(_coro)\n_coro.close()  # Prevent ResourceWarning\ndel _coro\n## asynchronous generator ##\nasync def _ag(): yield\n_ag = _ag()\nasync_generator = type(_ag)\ndel _ag\n\n\n### ONE-TRICK PONIES ###\n\ndef _check_methods(C, *methods):\n    mro = C.__mro__\n    for method in methods:\n        for B in mro:\n            if method in B.__dict__:\n                if B.__dict__[method] is None:\n                    return NotImplemented\n                break\n        else:\n            return NotImplemented\n    return True\n\nclass Hashable(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __hash__(self):\n        return 0\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Hashable:\n            return _check_methods(C, "__hash__")\n        return NotImplemented\n\n\nclass Awaitable(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __await__(self):\n        yield\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Awaitable:\n            return _check_methods(C, "__await__")\n        return NotImplemented\n\n\nclass Coroutine(Awaitable):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def send(self, value):\n        """Send a value into the coroutine.\n        Return next yielded value or raise StopIteration.\n        """\n        raise StopIteration\n\n    @abstractmethod\n    def throw(self, typ, val=None, tb=None):\n        """Raise an exception in the coroutine.\n        Return next yielded value or raise StopIteration.\n        """\n        if val is None:\n            if tb is None:\n                raise typ\n            val = typ()\n        if tb is not None:\n            val = val.with_traceback(tb)\n        raise val\n\n    def close(self):\n        """Raise GeneratorExit inside coroutine.\n        """\n        try:\n            self.throw(GeneratorExit)\n        except (GeneratorExit, StopIteration):\n            pass\n        else:\n            raise RuntimeError("coroutine ignored GeneratorExit")\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Coroutine:\n            return _check_methods(C, \'__await__\', \'send\', \'throw\', \'close\')\n        return NotImplemented\n\n\nCoroutine.register(coroutine)\n\n\nclass AsyncIterable(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __aiter__(self):\n        return AsyncIterator()\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is AsyncIterable:\n            return _check_methods(C, "__aiter__")\n        return NotImplemented\n\n\nclass AsyncIterator(AsyncIterable):\n\n    __slots__ = ()\n\n    @abstractmethod\n    async def __anext__(self):\n        """Return the next item or raise StopAsyncIteration when exhausted."""\n        raise StopAsyncIteration\n\n    def __aiter__(self):\n        return self\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is AsyncIterator:\n            return _check_methods(C, "__anext__", "__aiter__")\n        return NotImplemented\n\n\nclass AsyncGenerator(AsyncIterator):\n\n    __slots__ = ()\n\n    async def __anext__(self):\n        """Return the next item from the asynchronous generator.\n        When exhausted, raise StopAsyncIteration.\n        """\n        return await self.asend(None)\n\n    @abstractmethod\n    async def asend(self, value):\n        """Send a value into the asynchronous generator.\n        Return next yielded value or raise StopAsyncIteration.\n        """\n        raise StopAsyncIteration\n\n    @abstractmethod\n    async def athrow(self, typ, val=None, tb=None):\n        """Raise an exception in the asynchronous generator.\n        Return next yielded value or raise StopAsyncIteration.\n        """\n        if val is None:\n            if tb is None:\n                raise typ\n            val = typ()\n        if tb is not None:\n            val = val.with_traceback(tb)\n        raise val\n\n    async def aclose(self):\n        """Raise GeneratorExit inside coroutine.\n        """\n        try:\n            await self.athrow(GeneratorExit)\n        except (GeneratorExit, StopAsyncIteration):\n            pass\n        else:\n            raise RuntimeError("asynchronous generator ignored GeneratorExit")\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is AsyncGenerator:\n            return _check_methods(C, \'__aiter__\', \'__anext__\',\n                                  \'asend\', \'athrow\', \'aclose\')\n        return NotImplemented\n\n\nAsyncGenerator.register(async_generator)\n\n\nclass Iterable(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __iter__(self):\n        while False:\n            yield None\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Iterable:\n            return _check_methods(C, "__iter__")\n        return NotImplemented\n\n\nclass Iterator(Iterable):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __next__(self):\n        \'Return the next item from the iterator. When exhausted, raise StopIteration\'\n        raise StopIteration\n\n    def __iter__(self):\n        return self\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Iterator:\n            return _check_methods(C, \'__iter__\', \'__next__\')\n        return NotImplemented\n\nIterator.register(bytes_iterator)\nIterator.register(bytearray_iterator)\n#Iterator.register(callable_iterator)\nIterator.register(dict_keyiterator)\nIterator.register(dict_valueiterator)\nIterator.register(dict_itemiterator)\nIterator.register(list_iterator)\nIterator.register(list_reverseiterator)\nIterator.register(range_iterator)\nIterator.register(longrange_iterator)\nIterator.register(set_iterator)\nIterator.register(str_iterator)\nIterator.register(tuple_iterator)\nIterator.register(zip_iterator)\n\n\nclass Reversible(Iterable):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __reversed__(self):\n        while False:\n            yield None\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Reversible:\n            return _check_methods(C, "__reversed__", "__iter__")\n        return NotImplemented\n\n\nclass Generator(Iterator):\n\n    __slots__ = ()\n\n    def __next__(self):\n        """Return the next item from the generator.\n        When exhausted, raise StopIteration.\n        """\n        return self.send(None)\n\n    @abstractmethod\n    def send(self, value):\n        """Send a value into the generator.\n        Return next yielded value or raise StopIteration.\n        """\n        raise StopIteration\n\n    @abstractmethod\n    def throw(self, typ, val=None, tb=None):\n        """Raise an exception in the generator.\n        Return next yielded value or raise StopIteration.\n        """\n        if val is None:\n            if tb is None:\n                raise typ\n            val = typ()\n        if tb is not None:\n            val = val.with_traceback(tb)\n        raise val\n\n    def close(self):\n        """Raise GeneratorExit inside generator.\n        """\n        try:\n            self.throw(GeneratorExit)\n        except (GeneratorExit, StopIteration):\n            pass\n        else:\n            raise RuntimeError("generator ignored GeneratorExit")\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Generator:\n            return _check_methods(C, \'__iter__\', \'__next__\',\n                                  \'send\', \'throw\', \'close\')\n        return NotImplemented\n\nGenerator.register(generator)\n\n\nclass Sized(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __len__(self):\n        return 0\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Sized:\n            return _check_methods(C, "__len__")\n        return NotImplemented\n\n\nclass Container(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __contains__(self, x):\n        return False\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Container:\n            return _check_methods(C, "__contains__")\n        return NotImplemented\n\nclass Collection(Sized, Iterable, Container):\n\n    __slots__ = ()\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Collection:\n            return _check_methods(C,  "__len__", "__iter__", "__contains__")\n        return NotImplemented\n\nclass Callable(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __call__(self, *args, **kwds):\n        return False\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Callable:\n            return _check_methods(C, "__call__")\n        return NotImplemented\n\n\n### SETS ###\n\n\nclass Set(Collection):\n\n    """A set is a finite, iterable container.\n\n    This class provides concrete generic implementations of all\n    methods except for __contains__, __iter__ and __len__.\n\n    To override the comparisons (presumably for speed, as the\n    semantics are fixed), redefine __le__ and __ge__,\n    then the other operations will automatically follow suit.\n    """\n\n    __slots__ = ()\n\n    def __le__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        if len(self) > len(other):\n            return False\n        for elem in self:\n            if elem not in other:\n                return False\n        return True\n\n    def __lt__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        return len(self) < len(other) and self.__le__(other)\n\n    def __gt__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        return len(self) > len(other) and self.__ge__(other)\n\n    def __ge__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        if len(self) < len(other):\n            return False\n        for elem in other:\n            if elem not in self:\n                return False\n        return True\n\n    def __eq__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        return len(self) == len(other) and self.__le__(other)\n\n    @classmethod\n    def _from_iterable(cls, it):\n        \'\'\'Construct an instance of the class from any iterable input.\n\n        Must override this method if the class constructor signature\n        does not accept an iterable for an input.\n        \'\'\'\n        return cls(it)\n\n    def __and__(self, other):\n        if not isinstance(other, Iterable):\n            return NotImplemented\n        return self._from_iterable(value for value in other if value in self)\n\n    __rand__ = __and__\n\n    def isdisjoint(self, other):\n        \'Return True if two sets have a null intersection.\'\n        for value in other:\n            if value in self:\n                return False\n        return True\n\n    def __or__(self, other):\n        if not isinstance(other, Iterable):\n            return NotImplemented\n        chain = (e for s in (self, other) for e in s)\n        return self._from_iterable(chain)\n\n    __ror__ = __or__\n\n    def __sub__(self, other):\n        if not isinstance(other, Set):\n            if not isinstance(other, Iterable):\n                return NotImplemented\n            other = self._from_iterable(other)\n        return self._from_iterable(value for value in self\n                                   if value not in other)\n\n    def __rsub__(self, other):\n        if not isinstance(other, Set):\n            if not isinstance(other, Iterable):\n                return NotImplemented\n            other = self._from_iterable(other)\n        return self._from_iterable(value for value in other\n                                   if value not in self)\n\n    def __xor__(self, other):\n        if not isinstance(other, Set):\n            if not isinstance(other, Iterable):\n                return NotImplemented\n            other = self._from_iterable(other)\n        return (self - other) | (other - self)\n\n    __rxor__ = __xor__\n\n    def _hash(self):\n        """Compute the hash value of a set.\n\n        Note that we don\'t define __hash__: not all sets are hashable.\n        But if you define a hashable set type, its __hash__ should\n        call this function.\n\n        This must be compatible __eq__.\n\n        All sets ought to compare equal if they contain the same\n        elements, regardless of how they are implemented, and\n        regardless of the order of the elements; so there\'s not much\n        freedom for __eq__ or __hash__.  We match the algorithm used\n        by the built-in frozenset type.\n        """\n        MAX = sys.maxsize\n        MASK = 2 * MAX + 1\n        n = len(self)\n        h = 1927868237 * (n + 1)\n        h &= MASK\n        for x in self:\n            hx = hash(x)\n            h ^= (hx ^ (hx << 16) ^ 89869747)  * 3644798167\n            h &= MASK\n        h = h * 69069 + 907133923\n        h &= MASK\n        if h > MAX:\n            h -= MASK + 1\n        if h == -1:\n            h = 590923713\n        return h\n\nSet.register(frozenset)\n\n\nclass MutableSet(Set):\n    """A mutable set is a finite, iterable container.\n\n    This class provides concrete generic implementations of all\n    methods except for __contains__, __iter__, __len__,\n    add(), and discard().\n\n    To override the comparisons (presumably for speed, as the\n    semantics are fixed), all you have to do is redefine __le__ and\n    then the other operations will automatically follow suit.\n    """\n\n    __slots__ = ()\n\n    @abstractmethod\n    def add(self, value):\n        """Add an element."""\n        raise NotImplementedError\n\n    @abstractmethod\n    def discard(self, value):\n        """Remove an element.  Do not raise an exception if absent."""\n        raise NotImplementedError\n\n    def remove(self, value):\n        """Remove an element. If not a member, raise a KeyError."""\n        if value not in self:\n            raise KeyError(value)\n        self.discard(value)\n\n    def pop(self):\n        """Return the popped value.  Raise KeyError if empty."""\n        it = iter(self)\n        try:\n            value = next(it)\n        except StopIteration:\n            raise KeyError from None\n        self.discard(value)\n        return value\n\n    def clear(self):\n        """This is slow (creates N new iterators!) but effective."""\n        try:\n            while True:\n                self.pop()\n        except KeyError:\n            pass\n\n    def __ior__(self, it):\n        for value in it:\n            self.add(value)\n        return self\n\n    def __iand__(self, it):\n        for value in (self - it):\n            self.discard(value)\n        return self\n\n    def __ixor__(self, it):\n        if it is self:\n            self.clear()\n        else:\n            if not isinstance(it, Set):\n                it = self._from_iterable(it)\n            for value in it:\n                if value in self:\n                    self.discard(value)\n                else:\n                    self.add(value)\n        return self\n\n    def __isub__(self, it):\n        if it is self:\n            self.clear()\n        else:\n            for value in it:\n                self.discard(value)\n        return self\n\nMutableSet.register(set)\n\n\n### MAPPINGS ###\n\n\nclass Mapping(Collection):\n\n    __slots__ = ()\n\n    """A Mapping is a generic container for associating key/value\n    pairs.\n\n    This class provides concrete generic implementations of all\n    methods except for __getitem__, __iter__, and __len__.\n\n    """\n\n    @abstractmethod\n    def __getitem__(self, key):\n        raise KeyError\n\n    def get(self, key, default=None):\n        \'D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\'\n        try:\n            return self[key]\n        except KeyError:\n            return default\n\n    def __contains__(self, key):\n        try:\n            self[key]\n        except KeyError:\n            return False\n        else:\n            return True\n\n    def keys(self):\n        "D.keys() -> a set-like object providing a view on D\'s keys"\n        return KeysView(self)\n\n    def items(self):\n        "D.items() -> a set-like object providing a view on D\'s items"\n        return ItemsView(self)\n\n    def values(self):\n        "D.values() -> an object providing a view on D\'s values"\n        return ValuesView(self)\n\n    def __eq__(self, other):\n        if not isinstance(other, Mapping):\n            return NotImplemented\n        return dict(self.items()) == dict(other.items())\n\n    __reversed__ = None\n\nMapping.register(mappingproxy)\n\n\nclass MappingView(Sized):\n\n    __slots__ = \'_mapping\',\n\n    def __init__(self, mapping):\n        self._mapping = mapping\n\n    def __len__(self):\n        return len(self._mapping)\n\n    def __repr__(self):\n        return \'{0.__class__.__name__}({0._mapping!r})\'.format(self)\n\n\nclass KeysView(MappingView, Set):\n\n    __slots__ = ()\n\n    @classmethod\n    def _from_iterable(self, it):\n        return set(it)\n\n    def __contains__(self, key):\n        return key in self._mapping\n\n    def __iter__(self):\n        yield from self._mapping\n\nKeysView.register(dict_keys)\n\n\nclass ItemsView(MappingView, Set):\n\n    __slots__ = ()\n\n    @classmethod\n    def _from_iterable(self, it):\n        return set(it)\n\n    def __contains__(self, item):\n        key, value = item\n        try:\n            v = self._mapping[key]\n        except KeyError:\n            return False\n        else:\n            return v is value or v == value\n\n    def __iter__(self):\n        for key in self._mapping:\n            yield (key, self._mapping[key])\n\nItemsView.register(dict_items)\n\n\nclass ValuesView(MappingView, Collection):\n\n    __slots__ = ()\n\n    def __contains__(self, value):\n        for key in self._mapping:\n            v = self._mapping[key]\n            if v is value or v == value:\n                return True\n        return False\n\n    def __iter__(self):\n        for key in self._mapping:\n            yield self._mapping[key]\n\nValuesView.register(dict_values)\n\n\nclass MutableMapping(Mapping):\n\n    __slots__ = ()\n\n    """A MutableMapping is a generic container for associating\n    key/value pairs.\n\n    This class provides concrete generic implementations of all\n    methods except for __getitem__, __setitem__, __delitem__,\n    __iter__, and __len__.\n\n    """\n\n    @abstractmethod\n    def __setitem__(self, key, value):\n        raise KeyError\n\n    @abstractmethod\n    def __delitem__(self, key):\n        raise KeyError\n\n    __marker = object()\n\n    def pop(self, key, default=__marker):\n        \'\'\'D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n          If key is not found, d is returned if given, otherwise KeyError is raised.\n        \'\'\'\n        try:\n            value = self[key]\n        except KeyError:\n            if default is self.__marker:\n                raise\n            return default\n        else:\n            del self[key]\n            return value\n\n    def popitem(self):\n        \'\'\'D.popitem() -> (k, v), remove and return some (key, value) pair\n           as a 2-tuple; but raise KeyError if D is empty.\n        \'\'\'\n        try:\n            key = next(iter(self))\n        except StopIteration:\n            raise KeyError from None\n        value = self[key]\n        del self[key]\n        return key, value\n\n    def clear(self):\n        \'D.clear() -> None.  Remove all items from D.\'\n        try:\n            while True:\n                self.popitem()\n        except KeyError:\n            pass\n\n    def update(self, other=(), /, **kwds):\n        \'\'\' D.update([E, ]**F) -> None.  Update D from mapping/iterable E and F.\n            If E present and has a .keys() method, does:     for k in E: D[k] = E[k]\n            If E present and lacks .keys() method, does:     for (k, v) in E: D[k] = v\n            In either case, this is followed by: for k, v in F.items(): D[k] = v\n        \'\'\'\n        if isinstance(other, Mapping):\n            for key in other:\n                self[key] = other[key]\n        elif hasattr(other, "keys"):\n            for key in other.keys():\n                self[key] = other[key]\n        else:\n            for key, value in other:\n                self[key] = value\n        for key, value in kwds.items():\n            self[key] = value\n\n    def setdefault(self, key, default=None):\n        \'D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\'\n        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n        return default\n\nMutableMapping.register(dict)\n\n\n### SEQUENCES ###\n\n\nclass Sequence(Reversible, Collection):\n\n    """All the operations on a read-only sequence.\n\n    Concrete subclasses must override __new__ or __init__,\n    __getitem__, and __len__.\n    """\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __getitem__(self, index):\n        raise IndexError\n\n    def __iter__(self):\n        i = 0\n        try:\n            while True:\n                v = self[i]\n                yield v\n                i += 1\n        except IndexError:\n            return\n\n    def __contains__(self, value):\n        for v in self:\n            if v is value or v == value:\n                return True\n        return False\n\n    def __reversed__(self):\n        for i in reversed(range(len(self))):\n            yield self[i]\n\n    def index(self, value, start=0, stop=None):\n        \'\'\'S.index(value, [start, [stop]]) -> integer -- return first index of value.\n           Raises ValueError if the value is not present.\n\n           Supporting start and stop arguments is optional, but\n           recommended.\n        \'\'\'\n        if start is not None and start < 0:\n            start = max(len(self) + start, 0)\n        if stop is not None and stop < 0:\n            stop += len(self)\n\n        i = start\n        while stop is None or i < stop:\n            try:\n                v = self[i]\n                if v is value or v == value:\n                    return i\n            except IndexError:\n                break\n            i += 1\n        raise ValueError\n\n    def count(self, value):\n        \'S.count(value) -> integer -- return number of occurrences of value\'\n        return sum(1 for v in self if v is value or v == value)\n\nSequence.register(tuple)\nSequence.register(str)\nSequence.register(range)\nSequence.register(memoryview)\n\n\nclass ByteString(Sequence):\n\n    """This unifies bytes and bytearray.\n\n    XXX Should add all their methods.\n    """\n\n    __slots__ = ()\n\nByteString.register(bytes)\nByteString.register(bytearray)\n\n\nclass MutableSequence(Sequence):\n\n    __slots__ = ()\n\n    """All the operations on a read-write sequence.\n\n    Concrete subclasses must provide __new__ or __init__,\n    __getitem__, __setitem__, __delitem__, __len__, and insert().\n\n    """\n\n    @abstractmethod\n    def __setitem__(self, index, value):\n        raise IndexError\n\n    @abstractmethod\n    def __delitem__(self, index):\n        raise IndexError\n\n    @abstractmethod\n    def insert(self, index, value):\n        \'S.insert(index, value) -- insert value before index\'\n        raise IndexError\n\n    def append(self, value):\n        \'S.append(value) -- append value to the end of the sequence\'\n        self.insert(len(self), value)\n\n    def clear(self):\n        \'S.clear() -> None -- remove all items from S\'\n        try:\n            while True:\n                self.pop()\n        except IndexError:\n            pass\n\n    def reverse(self):\n        \'S.reverse() -- reverse *IN PLACE*\'\n        n = len(self)\n        for i in range(n//2):\n            self[i], self[n-i-1] = self[n-i-1], self[i]\n\n    def extend(self, values):\n        \'S.extend(iterable) -- extend sequence by appending elements from the iterable\'\n        if values is self:\n            values = list(values)\n        for v in values:\n            self.append(v)\n\n    def pop(self, index=-1):\n        \'\'\'S.pop([index]) -> item -- remove and return item at index (default last).\n           Raise IndexError if list is empty or index is out of range.\n        \'\'\'\n        v = self[index]\n        del self[index]\n        return v\n\n    def remove(self, value):\n        \'\'\'S.remove(value) -- remove first occurrence of value.\n           Raise ValueError if the value is not present.\n        \'\'\'\n        del self[self.index(value)]\n\n    def __iadd__(self, values):\n        self.extend(values)\n        return self\n\nMutableSequence.register(list)\nMutableSequence.register(bytearray)  # Multiply inheriting, see ByteString\n')
    __stickytape_write_module('reprlib.py', b'"""Redo the builtin repr() (representation) but with limits on most sizes."""\n\n__all__ = ["Repr", "repr", "recursive_repr"]\n\nimport builtins\nfrom itertools import islice\nfrom _thread import get_ident\n\ndef recursive_repr(fillvalue=\'...\'):\n    \'Decorator to make a repr function return fillvalue for a recursive call\'\n\n    def decorating_function(user_function):\n        repr_running = set()\n\n        def wrapper(self):\n            key = id(self), get_ident()\n            if key in repr_running:\n                return fillvalue\n            repr_running.add(key)\n            try:\n                result = user_function(self)\n            finally:\n                repr_running.discard(key)\n            return result\n\n        # Can\'t use functools.wraps() here because of bootstrap issues\n        wrapper.__module__ = getattr(user_function, \'__module__\')\n        wrapper.__doc__ = getattr(user_function, \'__doc__\')\n        wrapper.__name__ = getattr(user_function, \'__name__\')\n        wrapper.__qualname__ = getattr(user_function, \'__qualname__\')\n        wrapper.__annotations__ = getattr(user_function, \'__annotations__\', {})\n        return wrapper\n\n    return decorating_function\n\nclass Repr:\n\n    def __init__(self):\n        self.maxlevel = 6\n        self.maxtuple = 6\n        self.maxlist = 6\n        self.maxarray = 5\n        self.maxdict = 4\n        self.maxset = 6\n        self.maxfrozenset = 6\n        self.maxdeque = 6\n        self.maxstring = 30\n        self.maxlong = 40\n        self.maxother = 30\n\n    def repr(self, x):\n        return self.repr1(x, self.maxlevel)\n\n    def repr1(self, x, level):\n        typename = type(x).__name__\n        if \' \' in typename:\n            parts = typename.split()\n            typename = \'_\'.join(parts)\n        if hasattr(self, \'repr_\' + typename):\n            return getattr(self, \'repr_\' + typename)(x, level)\n        else:\n            return self.repr_instance(x, level)\n\n    def _repr_iterable(self, x, level, left, right, maxiter, trail=\'\'):\n        n = len(x)\n        if level <= 0 and n:\n            s = \'...\'\n        else:\n            newlevel = level - 1\n            repr1 = self.repr1\n            pieces = [repr1(elem, newlevel) for elem in islice(x, maxiter)]\n            if n > maxiter:  pieces.append(\'...\')\n            s = \', \'.join(pieces)\n            if n == 1 and trail:  right = trail + right\n        return \'%s%s%s\' % (left, s, right)\n\n    def repr_tuple(self, x, level):\n        return self._repr_iterable(x, level, \'(\', \')\', self.maxtuple, \',\')\n\n    def repr_list(self, x, level):\n        return self._repr_iterable(x, level, \'[\', \']\', self.maxlist)\n\n    def repr_array(self, x, level):\n        if not x:\n            return "array(\'%s\')" % x.typecode\n        header = "array(\'%s\', [" % x.typecode\n        return self._repr_iterable(x, level, header, \'])\', self.maxarray)\n\n    def repr_set(self, x, level):\n        if not x:\n            return \'set()\'\n        x = _possibly_sorted(x)\n        return self._repr_iterable(x, level, \'{\', \'}\', self.maxset)\n\n    def repr_frozenset(self, x, level):\n        if not x:\n            return \'frozenset()\'\n        x = _possibly_sorted(x)\n        return self._repr_iterable(x, level, \'frozenset({\', \'})\',\n                                   self.maxfrozenset)\n\n    def repr_deque(self, x, level):\n        return self._repr_iterable(x, level, \'deque([\', \'])\', self.maxdeque)\n\n    def repr_dict(self, x, level):\n        n = len(x)\n        if n == 0: return \'{}\'\n        if level <= 0: return \'{...}\'\n        newlevel = level - 1\n        repr1 = self.repr1\n        pieces = []\n        for key in islice(_possibly_sorted(x), self.maxdict):\n            keyrepr = repr1(key, newlevel)\n            valrepr = repr1(x[key], newlevel)\n            pieces.append(\'%s: %s\' % (keyrepr, valrepr))\n        if n > self.maxdict: pieces.append(\'...\')\n        s = \', \'.join(pieces)\n        return \'{%s}\' % (s,)\n\n    def repr_str(self, x, level):\n        s = builtins.repr(x[:self.maxstring])\n        if len(s) > self.maxstring:\n            i = max(0, (self.maxstring-3)//2)\n            j = max(0, self.maxstring-3-i)\n            s = builtins.repr(x[:i] + x[len(x)-j:])\n            s = s[:i] + \'...\' + s[len(s)-j:]\n        return s\n\n    def repr_int(self, x, level):\n        s = builtins.repr(x) # XXX Hope this isn\'t too slow...\n        if len(s) > self.maxlong:\n            i = max(0, (self.maxlong-3)//2)\n            j = max(0, self.maxlong-3-i)\n            s = s[:i] + \'...\' + s[len(s)-j:]\n        return s\n\n    def repr_instance(self, x, level):\n        try:\n            s = builtins.repr(x)\n            # Bugs in x.__repr__() can cause arbitrary\n            # exceptions -- then make up something\n        except Exception:\n            return \'<%s instance at %#x>\' % (x.__class__.__name__, id(x))\n        if len(s) > self.maxother:\n            i = max(0, (self.maxother-3)//2)\n            j = max(0, self.maxother-3-i)\n            s = s[:i] + \'...\' + s[len(s)-j:]\n        return s\n\n\ndef _possibly_sorted(x):\n    # Since not all sequences of items can be sorted and comparison\n    # functions may raise arbitrary exceptions, return an unsorted\n    # sequence in that case.\n    try:\n        return sorted(x)\n    except Exception:\n        return list(x)\n\naRepr = Repr()\nrepr = aRepr.repr\n')
    __stickytape_write_module('collections/abc.py', b'from _collections_abc import *\nfrom _collections_abc import __all__\n')
    __stickytape_write_module('funcparserlib/__init__.py', b'')
    __stickytape_write_module('funcparserlib/lexer.py', b'# -*- coding: utf-8 -*-\n\n# Copyright \xc2\xa9 2009/2021 Andrey Vlasovskikh\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this\n# software and associated documentation files (the "Software"), to deal in the Software\n# without restriction, including without limitation the rights to use, copy, modify,\n# merge, publish, distribute, sublicense, and/or sell copies of the Software, and to\n# permit persons to whom the Software is furnished to do so, subject to the following\n# conditions:\n#\n# The above copyright notice and this permission notice shall be included in all copies\n# or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,\n# INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A\n# PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\n# HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF\n# CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE\n# OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom __future__ import unicode_literals\n\n__all__ = ["make_tokenizer", "TokenSpec", "Token", "LexerError"]\n\nimport re\n\n\nclass LexerError(Exception):\n    def __init__(self, place, msg):\n        self.place = place\n        self.msg = msg\n\n    def __str__(self):\n        s = "cannot tokenize data"\n        line, pos = self.place\n        return \'%s: %d,%d: "%s"\' % (s, line, pos, self.msg)\n\n\nclass TokenSpec(object):\n    """A token specification for generating a lexer via `make_tokenizer()`."""\n\n    def __init__(self, type, pattern, flags=0):\n        """Initialize a `TokenSpec` object.\n\n        Parameters:\n            type (str): User-defined type of the token (e.g. `"name"`, `"number"`,\n                `"operator"`)\n            pattern (str): Regexp for matching this token type\n            flags (int, optional): Regexp flags, the second argument of `re.compile()`\n        """\n        self.type = type\n        self.pattern = pattern\n        self.flags = flags\n\n    def __repr__(self):\n        return "TokenSpec(%r, %r, %r)" % (self.type, self.pattern, self.flags)\n\n\nclass Token(object):\n    """A token object that represents a substring of certain type in your text.\n\n    You can compare tokens for equality using the `==` operator. Tokens also define\n    custom `repr()` and `str()`.\n\n    Attributes:\n        type (str): User-defined type of the token (e.g. `"name"`, `"number"`,\n            `"operator"`)\n        value (str): Text value of the token\n        start (Optional[Tuple[int, int]]): Start position (_line_, _column_)\n        end (Optional[Tuple[int, int]]): End position (_line_, _column_)\n    """\n\n    def __init__(self, type, value, start=None, end=None):\n        """Initialize a `Token` object."""\n        self.type = type\n        self.value = value\n        self.start = start\n        self.end = end\n\n    def __repr__(self):\n        return "Token(%r, %r)" % (self.type, self.value)\n\n    def __eq__(self, other):\n        # FIXME: Case sensitivity is assumed here\n        if other is None:\n            return False\n        else:\n            return self.type == other.type and self.value == other.value\n\n    def _pos_str(self):\n        if self.start is None or self.end is None:\n            return ""\n        else:\n            sl, sp = self.start\n            el, ep = self.end\n            return "%d,%d-%d,%d:" % (sl, sp, el, ep)\n\n    def __str__(self):\n        s = "%s %s \'%s\'" % (self._pos_str(), self.type, self.value)\n        return s.strip()\n\n    @property\n    def name(self):\n        return self.value\n\n    def pformat(self):\n        return "%s %s \'%s\'" % (\n            self._pos_str().ljust(20),  # noqa\n            self.type.ljust(14),\n            self.value,\n        )\n\n\ndef make_tokenizer(specs):\n    # noinspection GrazieInspection\n    """Make a function that tokenizes text based on the regexp specs.\n\n    Type: `(Sequence[TokenSpec | Tuple]) -> Callable[[str], Iterable[Token]]`\n\n    A token spec is `TokenSpec` instance.\n\n    !!! Note\n\n        For legacy reasons, a token spec may also be a tuple of (_type_, _args_), where\n        _type_ sets the value of `Token.type` for the token, and _args_ are the\n        positional arguments for `re.compile()`: either just (_pattern_,) or\n        (_pattern_, _flags_).\n\n    It returns a tokenizer function that takes a string and returns an iterable of\n    `Token` objects, or raises `LexerError` if it cannot tokenize the string according\n    to its token specs.\n\n    Examples:\n\n    ```pycon\n    >>> tokenize = make_tokenizer([\n    ...     TokenSpec("space", r"\\\\s+"),\n    ...     TokenSpec("id", r"\\\\w+"),\n    ...     TokenSpec("op", r"[,!]"),\n    ... ])\n    >>> text = "Hello, World!"\n    >>> [t for t in tokenize(text) if t.type != "space"]  # noqa\n    [Token(\'id\', \'Hello\'), Token(\'op\', \',\'), Token(\'id\', \'World\'), Token(\'op\', \'!\')]\n    >>> text = "Bye?"\n    >>> list(tokenize(text))\n    Traceback (most recent call last):\n        ...\n    lexer.LexerError: cannot tokenize data: 1,4: "Bye?"\n\n    ```\n    """\n    compiled = []\n    for spec in specs:\n        if isinstance(spec, TokenSpec):\n            c = spec.type, re.compile(spec.pattern, spec.flags)\n        else:\n            name, args = spec\n            c = name, re.compile(*args)\n        compiled.append(c)\n\n    def match_specs(s, i, position):\n        line, pos = position\n        for type, regexp in compiled:\n            m = regexp.match(s, i)\n            if m is not None:\n                value = m.group()\n                nls = value.count("\\n")\n                n_line = line + nls\n                if nls == 0:\n                    n_pos = pos + len(value)\n                else:\n                    n_pos = len(value) - value.rfind("\\n") - 1\n                return Token(type, value, (line, pos + 1), (n_line, n_pos))\n        else:\n            err_line = s.splitlines()[line - 1]\n            raise LexerError((line, pos + 1), err_line)\n\n    def f(s):\n        length = len(s)\n        line, pos = 1, 0\n        i = 0\n        while i < length:\n            t = match_specs(s, i, (line, pos))\n            yield t\n            line, pos = t.end\n            i += len(t.value)\n\n    return f\n\n\n# This is an example of token specs. See also [this article][1] for a\n# discussion of searching for multiline comments using regexps (including `*?`).\n#\n#   [1]: http://ostermiller.org/findcomment.html\n_example_token_specs = [\n    TokenSpec("COMMENT", r"\\(\\*(.|[\\r\\n])*?\\*\\)", re.MULTILINE),\n    TokenSpec("COMMENT", r"\\{(.|[\\r\\n])*?\\}", re.MULTILINE),\n    TokenSpec("COMMENT", r"//.*"),\n    TokenSpec("NL", r"[\\r\\n]+"),\n    TokenSpec("SPACE", r"[ \\t\\r\\n]+"),\n    TokenSpec("NAME", r"[A-Za-z_][A-Za-z_0-9]*"),\n    TokenSpec("REAL", r"[0-9]+\\.[0-9]*([Ee][+\\-]?[0-9]+)*"),\n    TokenSpec("INT", r"[0-9]+"),\n    TokenSpec("INT", r"\\$[0-9A-Fa-f]+"),\n    TokenSpec("OP", r"(\\.\\.)|(<>)|(<=)|(>=)|(:=)|[;,=\\(\\):\\[\\]\\.+\\-<>\\*/@\\^]"),\n    TokenSpec("STRING", r"\'([^\']|(\'\'))*\'"),\n    TokenSpec("CHAR", r"#[0-9]+"),\n    TokenSpec("CHAR", r"#\\$[0-9A-Fa-f]+"),\n]\n# tokenize = make_tokenizer(_example_token_specs)\n')
    __stickytape_write_module('funcparserlib/parser.py', b'# -*- coding: utf-8 -*-\n\n# Copyright \xc2\xa9 2009/2021 Andrey Vlasovskikh\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this\n# software and associated documentation files (the "Software"), to deal in the Software\n# without restriction, including without limitation the rights to use, copy, modify,\n# merge, publish, distribute, sublicense, and/or sell copies of the Software, and to\n# permit persons to whom the Software is furnished to do so, subject to the following\n# conditions:\n#\n# The above copyright notice and this permission notice shall be included in all copies\n# or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,\n# INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A\n# PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\n# HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF\n# CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE\n# OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n"""Functional parsing combinators.\n\nParsing combinators define an internal domain-specific language (DSL) for describing\nthe parsing rules of a grammar. The DSL allows you to start with a few primitive\nparsers, then combine your parsers to get more complex ones, and finally cover\nthe whole grammar you want to parse.\n\nThe structure of the language:\n\n* Class `Parser`\n    * All the primitives and combinators of the language return `Parser` objects\n    * It defines the main `Parser.parse(tokens)` method\n* Primitive parsers\n    * `tok(type, value)`, `a(value)`, `some(pred)`, `forward_decl()`, `finished`\n* Parser combinators\n    * `p1 + p2`, `p1 | p2`, `p >> f`, `-p`, `maybe(p)`, `many(p)`, `oneplus(p)`,\n      `skip(p)`\n* Abstraction\n    * Use regular Python variables `p = ...  # Expression of type Parser` to define new\n      rules (non-terminals) of your grammar\n\nEvery time you apply one of the combinators, you get a new `Parser` object. In other\nwords, the set of `Parser` objects is closed under the means of combination.\n\n!!! Note\n\n    We took the parsing combinators language from the book [Introduction to Functional\n    Programming][1] and translated it from ML into Python.\n\n  [1]: https://www.cl.cam.ac.uk/teaching/Lectures/funprog-jrh-1996/\n"""\n\nfrom __future__ import unicode_literals\n\n__all__ = [\n    "some",\n    "a",\n    "tok",\n    "many",\n    "pure",\n    "finished",\n    "maybe",\n    "skip",\n    "oneplus",\n    "forward_decl",\n    "NoParseError",\n    "Parser",\n]\n\nimport sys\nimport logging\nimport warnings\n\nfrom funcparserlib.lexer import Token\n\nlog = logging.getLogger("funcparserlib")\n\ndebug = False\nif sys.version_info < (3,):\n    string_types = (str, unicode)  # noqa\nelse:\n    string_types = str\n\n\nclass Parser(object):\n    """A parser object that can parse a sequence of tokens or can be combined with\n    other parsers using `+`, `|`, `>>`, `many()`, and other parsing combinators.\n\n    Type: `Parser[A, B]`\n\n    The generic variables in the type are: `A` \xe2\x80\x94 the type of the tokens in the\n    sequence to parse,`B` \xe2\x80\x94 the type of the parsed value.\n\n    In order to define a parser for your grammar:\n\n    1. You start with primitive parsers by calling `a(value)`, `some(pred)`,\n       `forward_decl()`, `finished`\n    2. You use parsing combinators `p1 + p2`, `p1 | p2`, `p >> f`, `many(p)`, and\n       others to combine parsers into a more complex parser\n    3. You can assign complex parsers to variables to define names that correspond to\n       the rules of your grammar\n\n    !!! Note\n\n        The constructor `Parser.__init__()` is considered **internal** and may be\n        changed in future versions. Use primitive parsers and parsing combinators to\n        construct new parsers.\n    """\n\n    def __init__(self, p):\n        """Wrap the parser function `p` into a `Parser` object."""\n        self.name = ""\n        self.define(p)\n\n    def named(self, name):\n        # noinspection GrazieInspection\n        """Specify the name of the parser for easier debugging.\n\n        Type: `(str) -> Parser[A, B]`\n\n        This name is used in the debug-level parsing log. You can also get it via the\n        `Parser.name` attribute.\n\n        Examples:\n\n        ```pycon\n        >>> expr = (a("x") + a("y")).named("expr")\n        >>> expr.name\n        \'expr\'\n\n        ```\n\n        ```pycon\n        >>> expr = a("x") + a("y")\n        >>> expr.name\n        "(\'x\', \'y\')"\n\n        ```\n\n        !!! Note\n\n            You can enable the parsing log this way:\n\n            ```python\n            import logging\n            logging.basicConfig(level=logging.DEBUG)\n            import funcparserlib.parser\n            funcparserlib.parser.debug = True\n            ```\n\n            The way to enable the parsing log may be changed in future versions.\n        """\n        self.name = name\n        return self\n\n    def define(self, p):\n        """Define the parser created earlier as a forward declaration.\n\n        Type: `(Parser[A, B]) -> None`\n\n        Use `p = forward_decl()` in combination with `p.define(...)` to define\n        recursive parsers.\n\n        See the examples in the docs for `forward_decl()`.\n        """\n        f = getattr(p, "run", p)\n        if debug:\n            setattr(self, "_run", f)\n        else:\n            setattr(self, "run", f)\n        self.named(getattr(p, "name", p.__doc__))\n\n    def run(self, tokens, s):\n        """Run the parser against the tokens with the specified parsing state.\n\n        Type: `(Sequence[A], State) -> Tuple[B, State]`\n\n        The parsing state includes the current position in the sequence being parsed,\n        and the position of the rightmost token that has been consumed while parsing for\n        better error messages.\n\n        If the parser fails to parse the tokens, it raises `NoParseError`.\n\n        !!! Warning\n\n            This is method is **internal** and may be changed in future versions. Use\n            `Parser.parse(tokens)` instead and let the parser object take care of\n            updating the parsing state.\n        """\n        if debug:\n            log.debug("trying %s" % self.name)\n        return self._run(tokens, s)  # noqa\n\n    def _run(self, tokens, s):\n        raise NotImplementedError("you must define() a parser")\n\n    def parse(self, tokens):\n        """Parse the sequence of tokens and return the parsed value.\n\n        Type: `(Sequence[A]) -> B`\n\n        It takes a sequence of tokens of arbitrary type `A` and returns the parsed value\n        of arbitrary type `B`.\n\n        If the parser fails to parse the tokens, it raises `NoParseError`.\n\n        !!! Note\n\n            Although `Parser.parse()` can parse sequences of any objects (including\n            `str` which is a sequence of `str` chars), **the recommended way** is\n            parsing sequences of `Token` objects.\n\n            You **should** use a regexp-based tokenizer `make_tokenizer()` defined in\n            `funcparserlib.lexer` to convert your text into a sequence of `Token`\n            objects before parsing it. You will get more readable parsing error messages\n            (as `Token` objects contain their position in the source file) and good\n            separation of the lexical and syntactic levels of the grammar.\n        """\n        try:\n            (tree, _) = self.run(tokens, State(0, 0, None))\n            return tree\n        except NoParseError as e:\n            max = e.state.max\n            if len(tokens) > max:\n                t = tokens[max]\n                if isinstance(t, Token):\n                    if t.start is None or t.end is None:\n                        loc = ""\n                    else:\n                        s_line, s_pos = t.start\n                        e_line, e_pos = t.end\n                        loc = "%d,%d-%d,%d: " % (s_line, s_pos, e_line, e_pos)\n                    msg = "%s%s: %r" % (loc, e.msg, t.value)\n                elif isinstance(t, string_types):\n                    msg = "%s: %r" % (e.msg, t)\n                else:\n                    msg = "%s: %s" % (e.msg, t)\n            else:\n                msg = "got unexpected end of input"\n            if e.state.parser is not None:\n                msg = "%s, expected: %s" % (msg, e.state.parser.name)\n            e.msg = msg\n            raise\n\n    def __add__(self, other):\n        """Sequential combination of parsers. It runs this parser, then the other\n        parser.\n\n        The return value of the resulting parser is a tuple of each parsed value in\n        the sum of parsers. We merge all parsing results of `p1 + p2 + ... + pN` into a\n        single tuple. It means that the parsing result may be a 2-tuple, a 3-tuple,\n        a 4-tuple, etc. of parsed values. You avoid this by transforming the parsed\n        pair into a new value using the `>>` combinator.\n\n        You can also skip some parsing results in the resulting parsers by using `-p`\n        or `skip(p)` for some parsers in your sum of parsers. It means that the parsing\n        result might be a single value, not a tuple of parsed values. See the docs\n        for `Parser.__neg__()` for more examples.\n\n        Overloaded types (lots of them to provide stricter checking for the quite\n        dynamic return type of this method):\n\n        * `(self: Parser[A, B], _IgnoredParser[A]) -> Parser[A, B]`\n        * `(self: Parser[A, B], Parser[A, C]) -> _TupleParser[A, Tuple[B, C]]`\n        * `(self: _TupleParser[A, B], _IgnoredParser[A]) -> _TupleParser[A, B]`\n        * `(self: _TupleParser[A, B], Parser[A, Any]) -> Parser[A, Any]`\n        * `(self: _IgnoredParser[A], _IgnoredParser[A]) -> _IgnoredParser[A]`\n        * `(self: _IgnoredParser[A], Parser[A, C]) -> Parser[A, C]`\n\n        Examples:\n\n        ```pycon\n        >>> expr = a("x") + a("y")\n        >>> expr.parse("xy")\n        (\'x\', \'y\')\n\n        ```\n\n        ```pycon\n        >>> expr = a("x") + a("y") + a("z")\n        >>> expr.parse("xyz")\n        (\'x\', \'y\', \'z\')\n\n        ```\n\n        ```pycon\n        >>> expr = a("x") + a("y")\n        >>> expr.parse("xz")\n        Traceback (most recent call last):\n            ...\n        parser.NoParseError: got unexpected token: \'z\', expected: \'y\'\n\n        ```\n        """\n\n        def magic(v1, v2):\n            if isinstance(v1, _Tuple):\n                return _Tuple(v1 + (v2,))\n            else:\n                return _Tuple((v1, v2))\n\n        @_TupleParser\n        def _add(tokens, s):\n            (v1, s2) = self.run(tokens, s)\n            (v2, s3) = other.run(tokens, s2)\n            return magic(v1, v2), s3\n\n        @Parser\n        def ignored_right(tokens, s):\n            v, s2 = self.run(tokens, s)\n            _, s3 = other.run(tokens, s2)\n            return v, s3\n\n        name = "(%s, %s)" % (self.name, other.name)\n        if isinstance(other, _IgnoredParser):\n            return ignored_right.named(name)\n        else:\n            return _add.named(name)\n\n    def __or__(self, other):\n        """Choice combination of parsers.\n\n        It runs this parser and returns its result. If the parser fails, it runs the\n        other parser.\n\n        Examples:\n\n        ```pycon\n        >>> expr = a("x") | a("y")\n        >>> expr.parse("x")\n        \'x\'\n        >>> expr.parse("y")\n        \'y\'\n        >>> expr.parse("z")\n        Traceback (most recent call last):\n            ...\n        parser.NoParseError: got unexpected token: \'z\', expected: \'x\' or \'y\'\n\n        ```\n        """\n\n        @Parser\n        def _or(tokens, s):\n            try:\n                return self.run(tokens, s)\n            except NoParseError as e:\n                state = e.state\n            try:\n                return other.run(tokens, State(s.pos, state.max, state.parser))\n            except NoParseError as e:\n                if s.pos == e.state.max:\n                    e.state = State(e.state.pos, e.state.max, _or)\n                raise\n\n        _or.name = "%s or %s" % (self.name, other.name)\n        return _or\n\n    def __rshift__(self, f):\n        """Transform the parsing result by applying the specified function.\n\n        Type: `(Callable[[B], C]) -> Parser[A, C]`\n\n        You can use it for transforming the parsed value into another value before\n        including it into the parse tree (the AST).\n\n        Examples:\n\n        ```pycon\n        >>> def make_canonical_name(s):\n        ...     return s.lower()\n        >>> expr = (a("D") | a("d")) >> make_canonical_name\n        >>> expr.parse("D")\n        \'d\'\n        >>> expr.parse("d")\n        \'d\'\n\n        ```\n        """\n\n        @Parser\n        def _shift(tokens, s):\n            (v, s2) = self.run(tokens, s)\n            return f(v), s2\n\n        return _shift.named(self.name)\n\n    def bind(self, f):\n        """Bind the parser to a monadic function that returns a new parser.\n\n        Type: `(Callable[[B], Parser[A, C]]) -> Parser[A, C]`\n\n        Also known as `>>=` in Haskell.\n\n        !!! Note\n\n            You can parse any context-free grammar without resorting to `bind`. Due\n            to its poor performance please use it only when you really need it.\n        """\n\n        @Parser\n        def _bind(tokens, s):\n            (v, s2) = self.run(tokens, s)\n            return f(v).run(tokens, s2)\n\n        _bind.name = "(%s >>=)" % (self.name,)\n        return _bind\n\n    def __neg__(self):\n        """Return a parser that parses the same tokens, but its parsing result is\n        ignored by the sequential `+` combinator.\n\n        Type: `(Parser[A, B]) -> _IgnoredParser[A]`\n\n        You can use it for throwing away elements of concrete syntax (e.g. `","`,\n        `";"`).\n\n        Examples:\n\n        ```pycon\n        >>> expr = -a("x") + a("y")\n        >>> expr.parse("xy")\n        \'y\'\n\n        ```\n\n        ```pycon\n        >>> expr = a("x") + -a("y")\n        >>> expr.parse("xy")\n        \'x\'\n\n        ```\n\n        ```pycon\n        >>> expr = a("x") + -a("y") + a("z")\n        >>> expr.parse("xyz")\n        (\'x\', \'z\')\n\n        ```\n\n        ```pycon\n        >>> expr = -a("x") + a("y") + -a("z")\n        >>> expr.parse("xyz")\n        \'y\'\n\n        ```\n\n        ```pycon\n        >>> expr = -a("x") + a("y")\n        >>> expr.parse("yz")\n        Traceback (most recent call last):\n            ...\n        parser.NoParseError: got unexpected token: \'y\', expected: \'x\'\n\n        ```\n\n        ```pycon\n        >>> expr = a("x") + -a("y")\n        >>> expr.parse("xz")\n        Traceback (most recent call last):\n            ...\n        parser.NoParseError: got unexpected token: \'z\', expected: \'y\'\n\n        ```\n\n        !!! Note\n\n            You **should not** pass the resulting parser to any combinators other than\n            `+`. You **should** have at least one non-skipped value in your\n            `p1 + p2 + ... + pN`. The parsed value of `-p` is an **internal** `_Ignored`\n            object, not intended for actual use.\n        """\n        return _IgnoredParser(self)\n\n    def __class_getitem__(cls, key):\n        return cls\n\n\nclass State(object):\n    """Parsing state that is maintained basically for error reporting.\n\n    It consists of the current position `pos` in the sequence being parsed, and the\n    position `max` of the rightmost token that has been consumed while parsing.\n    """\n\n    def __init__(self, pos, max, parser=None):\n        self.pos = pos\n        self.max = max\n        self.parser = parser\n\n    def __str__(self):\n        return str((self.pos, self.max))\n\n    def __repr__(self):\n        return "State(%r, %r)" % (self.pos, self.max)\n\n\nclass NoParseError(Exception):\n    def __init__(self, msg, state):\n        self.msg = msg\n        self.state = state\n\n    def __str__(self):\n        return self.msg\n\n\nclass _Tuple(tuple):\n    pass\n\n\nclass _TupleParser(Parser):\n    pass\n\n\nclass _Ignored(object):\n    def __init__(self, value):\n        self.value = value\n\n    def __repr__(self):\n        return "_Ignored(%s)" % repr(self.value)\n\n    def __eq__(self, other):\n        return isinstance(other, _Ignored) and self.value == other.value\n\n\n@Parser\ndef finished(tokens, s):\n    """A parser that throws an exception if there are any unparsed tokens left in the\n    sequence."""\n    if s.pos >= len(tokens):\n        return None, s\n    else:\n        s2 = State(s.pos, s.max, finished if s.pos == s.max else s.parser)\n        raise NoParseError("got unexpected token", s2)\n\n\nfinished.name = "end of input"\n\n\ndef many(p):\n    """Return a parser that applies the parser `p` as many times as it succeeds at\n    parsing the tokens.\n\n    Return a parser that infinitely applies the parser `p` to the input sequence\n    of tokens as long as it successfully parses them. The parsed value is a list of\n    the sequentially parsed values.\n\n    Examples:\n\n    ```pycon\n    >>> expr = many(a("x"))\n    >>> expr.parse("x")\n    [\'x\']\n    >>> expr.parse("xx")\n    [\'x\', \'x\']\n    >>> expr.parse("xxxy")  # noqa\n    [\'x\', \'x\', \'x\']\n    >>> expr.parse("y")\n    []\n\n    ```\n    """\n\n    @Parser\n    def _many(tokens, s):\n        res = []\n        try:\n            while True:\n                (v, s) = p.run(tokens, s)\n                res.append(v)\n        except NoParseError as e:\n            s2 = State(s.pos, e.state.max, e.state.parser)\n            if debug:\n                log.debug(\n                    "*matched* %d instances of %s, new state = %s"\n                    % (len(res), _many.name, s2)\n                )\n            return res, s2\n\n    _many.name = "{ %s }" % p.name\n    return _many\n\n\ndef some(pred):\n    """Return a parser that parses a token if it satisfies the predicate `pred`.\n\n    Type: `(Callable[[A], bool]) -> Parser[A, A]`\n\n    Examples:\n\n    ```pycon\n    >>> expr = some(lambda s: s.isalpha()).named(\'alpha\')\n    >>> expr.parse("x")\n    \'x\'\n    >>> expr.parse("y")\n    \'y\'\n    >>> expr.parse("1")\n    Traceback (most recent call last):\n        ...\n    parser.NoParseError: got unexpected token: \'1\', expected: alpha\n\n    ```\n\n    !!! Warning\n\n        The `some()` combinator is quite slow and may be changed or removed in future\n        versions. If you need a parser for a token by its type (e.g. any identifier)\n        and maybe its value, use `tok(type[, value])` instead. You should use\n        `make_tokenizer()` from `funcparserlib.lexer` to tokenize your text first.\n    """\n\n    @Parser\n    def _some(tokens, s):\n        if s.pos >= len(tokens):\n            s2 = State(s.pos, s.max, _some if s.pos == s.max else s.parser)\n            raise NoParseError("got unexpected end of input", s2)\n        else:\n            t = tokens[s.pos]\n            if pred(t):\n                pos = s.pos + 1\n                s2 = State(pos, max(pos, s.max), s.parser)\n                if debug:\n                    log.debug("*matched* %r, new state = %s" % (t, s2))\n                return t, s2\n            else:\n                s2 = State(s.pos, s.max, _some if s.pos == s.max else s.parser)\n                if debug:\n                    log.debug(\n                        "failed %r, state = %s, expected = %s" % (t, s2, s2.parser.name)\n                    )\n                raise NoParseError("got unexpected token", s2)\n\n    _some.name = "some(...)"\n    return _some\n\n\ndef a(value):\n    """Return a parser that parses a token if it\'s equal to `value`.\n\n    Type: `(A) -> Parser[A, A]`\n\n    Examples:\n\n    ```pycon\n    >>> expr = a("x")\n    >>> expr.parse("x")\n    \'x\'\n    >>> expr.parse("y")\n    Traceback (most recent call last):\n        ...\n    parser.NoParseError: got unexpected token: \'y\', expected: \'x\'\n\n    ```\n\n    !!! Note\n\n        Although `Parser.parse()` can parse sequences of any objects (including\n        `str` which is a sequence of `str` chars), **the recommended way** is\n        parsing sequences of `Token` objects.\n\n        You **should** use a regexp-based tokenizer `make_tokenizer()` defined in\n        `funcparserlib.lexer` to convert your text into a sequence of `Token` objects\n        before parsing it. You will get more readable parsing error messages (as `Token`\n        objects contain their position in the source file) and good separation of the\n        lexical and syntactic levels of the grammar.\n    """\n    name = getattr(value, "name", value)\n    return some(lambda t: t == value).named(repr(name))\n\n\ndef tok(type, value=None):\n    """Return a parser that parses a `Token` and returns the string value of the token.\n\n    Type: `(str, Optional[str]) -> Parser[Token, str]`\n\n    You can match any token of the specified `type` or you can match a specific token by\n    its `type` and `value`.\n\n    Examples:\n\n    ```pycon\n    >>> expr = tok("expr")\n    >>> expr.parse([Token("expr", "foo")])\n    \'foo\'\n    >>> expr.parse([Token("expr", "bar")])\n    \'bar\'\n    >>> expr.parse([Token("op", "=")])\n    Traceback (most recent call last):\n        ...\n    parser.NoParseError: got unexpected token: \'=\', expected: expr\n\n    ```\n\n    ```pycon\n    >>> expr = tok("op", "=")\n    >>> expr.parse([Token("op", "=")])\n    \'=\'\n    >>> expr.parse([Token("op", "+")])\n    Traceback (most recent call last):\n        ...\n    parser.NoParseError: got unexpected token: \'+\', expected: \'=\'\n\n    ```\n\n    !!! Note\n\n        In order to convert your text to parse into a sequence of `Token` objects,\n        use a regexp-based tokenizer `make_tokenizer()` defined in\n        `funcparserlib.lexer`. You will get more readable parsing error messages (as\n        `Token` objects contain their position in the source file) and good separation\n        of the lexical and syntactic levels of the grammar.\n    """\n    if value is not None:\n        p = a(Token(type, value))\n    else:\n        p = some(lambda t: t.type == type).named(type)\n    return (p >> (lambda t: t.value)).named(p.name)\n\n\ndef pure(x):\n    """Wrap any object into a parser.\n\n    Type: `(A) -> Parser[A, A]`\n\n    A pure parser doesn\'t touch the tokens sequence, it just returns its pure `x`\n    value.\n\n    Also known as `return` in Haskell.\n    """\n\n    @Parser\n    def _pure(_, s):\n        return x, s\n\n    _pure.name = "(pure %r)" % (x,)\n    return _pure\n\n\ndef maybe(p):\n    """Return a parser that returns `None` if the parser `p` fails.\n\n    Examples:\n\n    ```pycon\n    >>> expr = maybe(a("x"))\n    >>> expr.parse("x")\n    \'x\'\n    >>> expr.parse("y") is None\n    True\n\n    ```\n    """\n    return (p | pure(None)).named("[ %s ]" % (p.name,))\n\n\ndef skip(p):\n    """An alias for `-p`.\n\n    See also the docs for `Parser.__neg__()`.\n    """\n    return -p\n\n\nclass _IgnoredParser(Parser):\n    def __init__(self, p):\n        super(_IgnoredParser, self).__init__(p)\n        run = self._run if debug else self.run\n\n        def ignored(tokens, s):\n            v, s2 = run(tokens, s)\n            return v if isinstance(v, _Ignored) else _Ignored(v), s2\n\n        self.define(ignored)\n        self.name = getattr(p, "name", p.__doc__)\n\n    def __add__(self, other):\n        def ignored_left(tokens, s):\n            _, s2 = self.run(tokens, s)\n            v, s3 = other.run(tokens, s2)\n            return v, s3\n\n        if isinstance(other, _IgnoredParser):\n            return _IgnoredParser(ignored_left).named(\n                "(%s, %s)" % (self.name, other.name)\n            )\n        else:\n            return Parser(ignored_left).named("(%s, %s)" % (self.name, other.name))\n\n\ndef oneplus(p):\n    """Return a parser that applies the parser `p` one or more times.\n\n    A similar parser combinator `many(p)` means apply `p` zero or more times, whereas\n    `oneplus(p)` means apply `p` one or more times.\n\n    Examples:\n\n    ```pycon\n    >>> expr = oneplus(a("x"))\n    >>> expr.parse("x")\n    [\'x\']\n    >>> expr.parse("xx")\n    [\'x\', \'x\']\n    >>> expr.parse("y")\n    Traceback (most recent call last):\n        ...\n    parser.NoParseError: got unexpected token: \'y\', expected: \'x\'\n\n    ```\n    """\n\n    @Parser\n    def _oneplus(tokens, s):\n        (v1, s2) = p.run(tokens, s)\n        (v2, s3) = many(p).run(tokens, s2)\n        return [v1] + v2, s3\n\n    _oneplus.name = "(%s, { %s })" % (p.name, p.name)\n    return _oneplus\n\n\ndef with_forward_decls(suspension):\n    warnings.warn(\n        "Use forward_decl() instead:\\n"\n        "\\n"\n        "    p = forward_decl()\\n"\n        "    ...\\n"\n        "    p.define(parser_value)\\n",\n        DeprecationWarning,\n    )\n\n    @Parser\n    def f(tokens, s):\n        return suspension().run(tokens, s)\n\n    return f\n\n\ndef forward_decl():\n    """Return an undefined parser that can be used as a forward declaration.\n\n    Type: `Parser[Any, Any]`\n\n    Use `p = forward_decl()` in combination with `p.define(...)` to define recursive\n    parsers.\n\n\n    Examples:\n\n    ```pycon\n    >>> expr = forward_decl()\n    >>> expr.define(a("x") + maybe(expr) + a("y"))\n    >>> expr.parse("xxyy")  # noqa\n    (\'x\', (\'x\', None, \'y\'), \'y\')\n    >>> expr.parse("xxy")\n    Traceback (most recent call last):\n        ...\n    parser.NoParseError: got unexpected end of input, expected: \'y\'\n\n    ```\n\n    !!! Note\n\n        If you care about static types, you should add a type hint for your forward\n        declaration, so that your type checker can check types in `p.define(...)` later:\n\n        ```python\n        p: Parser[str, int] = forward_decl()\n        p.define(a("x"))  # Type checker error\n        p.define(a("1") >> int)  # OK\n        ```\n    """\n\n    @Parser\n    def f(_tokens, _s):\n        raise NotImplementedError("you must define() a forward_decl somewhere")\n\n    f.name = "forward_decl()"\n    return f\n\n\nif __name__ == "__main__":\n    import doctest\n\n    doctest.testmod()\n')
    __stickytape_write_module('funcparserlib/util.py', b'# -*- coding: utf-8 -*-\n\n# Copyright \xc2\xa9 2009/2021 Andrey Vlasovskikh\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this\n# software and associated documentation files (the "Software"), to deal in the Software\n# without restriction, including without limitation the rights to use, copy, modify,\n# merge, publish, distribute, sublicense, and/or sell copies of the Software, and to\n# permit persons to whom the Software is furnished to do so, subject to the following\n# conditions:\n#\n# The above copyright notice and this permission notice shall be included in all copies\n# or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,\n# INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A\n# PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\n# HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF\n# CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE\n# OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom __future__ import unicode_literals\n\n\ndef pretty_tree(x, kids, show):\n    """Return a pseudo-graphic tree representation of the object `x` similar to the\n    `tree` command in Unix.\n\n    Type: `(T, Callable[[T], List[T]], Callable[[T], str]) -> str`\n\n    It applies the parameter `show` (which is a function of type `(T) -> str`) to get a\n    textual representation of the objects to show.\n\n    It applies the parameter `kids` (which is a function of type `(T) -> List[T]`) to\n    list the children of the object to show.\n\n    Examples:\n\n    ```pycon\n    >>> print(pretty_tree(\n    ...     ["foo", ["bar", "baz"], "quux"],\n    ...     lambda obj: obj if isinstance(obj, list) else [],\n    ...     lambda obj: "[]" if isinstance(obj, list) else str(obj),\n    ... ))\n    []\n    |-- foo\n    |-- []\n    |   |-- bar\n    |   `-- baz\n    `-- quux\n\n    ```\n    """\n    (MID, END, CONT, LAST, ROOT) = ("|-- ", "`-- ", "|   ", "    ", "")\n\n    def rec(obj, indent, sym):\n        line = indent + sym + show(obj)\n        obj_kids = kids(obj)\n        if len(obj_kids) == 0:\n            return line\n        else:\n            if sym == MID:\n                next_indent = indent + CONT\n            elif sym == ROOT:\n                next_indent = indent + ROOT\n            else:\n                next_indent = indent + LAST\n            chars = [MID] * (len(obj_kids) - 1) + [END]\n            lines = [rec(kid, next_indent, sym) for kid, sym in zip(obj_kids, chars)]\n            return "\\n".join([line] + lines)\n\n    return rec(x, "", ROOT)\n')
    __stickytape_write_module('dataclasses.py', b'import re\nimport sys\nimport copy\nimport types\nimport inspect\nimport keyword\nimport builtins\nimport functools\nimport _thread\n\n\n__all__ = [\'dataclass\',\n           \'field\',\n           \'Field\',\n           \'FrozenInstanceError\',\n           \'InitVar\',\n           \'MISSING\',\n\n           # Helper functions.\n           \'fields\',\n           \'asdict\',\n           \'astuple\',\n           \'make_dataclass\',\n           \'replace\',\n           \'is_dataclass\',\n           ]\n\n# Conditions for adding methods.  The boxes indicate what action the\n# dataclass decorator takes.  For all of these tables, when I talk\n# about init=, repr=, eq=, order=, unsafe_hash=, or frozen=, I\'m\n# referring to the arguments to the @dataclass decorator.  When\n# checking if a dunder method already exists, I mean check for an\n# entry in the class\'s __dict__.  I never check to see if an attribute\n# is defined in a base class.\n\n# Key:\n# +=========+=========================================+\n# + Value   | Meaning                                 |\n# +=========+=========================================+\n# | <blank> | No action: no method is added.          |\n# +---------+-----------------------------------------+\n# | add     | Generated method is added.              |\n# +---------+-----------------------------------------+\n# | raise   | TypeError is raised.                    |\n# +---------+-----------------------------------------+\n# | None    | Attribute is set to None.               |\n# +=========+=========================================+\n\n# __init__\n#\n#   +--- init= parameter\n#   |\n#   v     |       |       |\n#         |  no   |  yes  |  <--- class has __init__ in __dict__?\n# +=======+=======+=======+\n# | False |       |       |\n# +-------+-------+-------+\n# | True  | add   |       |  <- the default\n# +=======+=======+=======+\n\n# __repr__\n#\n#    +--- repr= parameter\n#    |\n#    v    |       |       |\n#         |  no   |  yes  |  <--- class has __repr__ in __dict__?\n# +=======+=======+=======+\n# | False |       |       |\n# +-------+-------+-------+\n# | True  | add   |       |  <- the default\n# +=======+=======+=======+\n\n\n# __setattr__\n# __delattr__\n#\n#    +--- frozen= parameter\n#    |\n#    v    |       |       |\n#         |  no   |  yes  |  <--- class has __setattr__ or __delattr__ in __dict__?\n# +=======+=======+=======+\n# | False |       |       |  <- the default\n# +-------+-------+-------+\n# | True  | add   | raise |\n# +=======+=======+=======+\n# Raise because not adding these methods would break the "frozen-ness"\n# of the class.\n\n# __eq__\n#\n#    +--- eq= parameter\n#    |\n#    v    |       |       |\n#         |  no   |  yes  |  <--- class has __eq__ in __dict__?\n# +=======+=======+=======+\n# | False |       |       |\n# +-------+-------+-------+\n# | True  | add   |       |  <- the default\n# +=======+=======+=======+\n\n# __lt__\n# __le__\n# __gt__\n# __ge__\n#\n#    +--- order= parameter\n#    |\n#    v    |       |       |\n#         |  no   |  yes  |  <--- class has any comparison method in __dict__?\n# +=======+=======+=======+\n# | False |       |       |  <- the default\n# +-------+-------+-------+\n# | True  | add   | raise |\n# +=======+=======+=======+\n# Raise because to allow this case would interfere with using\n# functools.total_ordering.\n\n# __hash__\n\n#    +------------------- unsafe_hash= parameter\n#    |       +----------- eq= parameter\n#    |       |       +--- frozen= parameter\n#    |       |       |\n#    v       v       v    |        |        |\n#                         |   no   |  yes   |  <--- class has explicitly defined __hash__\n# +=======+=======+=======+========+========+\n# | False | False | False |        |        | No __eq__, use the base class __hash__\n# +-------+-------+-------+--------+--------+\n# | False | False | True  |        |        | No __eq__, use the base class __hash__\n# +-------+-------+-------+--------+--------+\n# | False | True  | False | None   |        | <-- the default, not hashable\n# +-------+-------+-------+--------+--------+\n# | False | True  | True  | add    |        | Frozen, so hashable, allows override\n# +-------+-------+-------+--------+--------+\n# | True  | False | False | add    | raise  | Has no __eq__, but hashable\n# +-------+-------+-------+--------+--------+\n# | True  | False | True  | add    | raise  | Has no __eq__, but hashable\n# +-------+-------+-------+--------+--------+\n# | True  | True  | False | add    | raise  | Not frozen, but hashable\n# +-------+-------+-------+--------+--------+\n# | True  | True  | True  | add    | raise  | Frozen, so hashable\n# +=======+=======+=======+========+========+\n# For boxes that are blank, __hash__ is untouched and therefore\n# inherited from the base class.  If the base is object, then\n# id-based hashing is used.\n#\n# Note that a class may already have __hash__=None if it specified an\n# __eq__ method in the class body (not one that was created by\n# @dataclass).\n#\n# See _hash_action (below) for a coded version of this table.\n\n\n# Raised when an attempt is made to modify a frozen class.\nclass FrozenInstanceError(AttributeError): pass\n\n# A sentinel object for default values to signal that a default\n# factory will be used.  This is given a nice repr() which will appear\n# in the function signature of dataclasses\' constructors.\nclass _HAS_DEFAULT_FACTORY_CLASS:\n    def __repr__(self):\n        return \'<factory>\'\n_HAS_DEFAULT_FACTORY = _HAS_DEFAULT_FACTORY_CLASS()\n\n# A sentinel object to detect if a parameter is supplied or not.  Use\n# a class to give it a better repr.\nclass _MISSING_TYPE:\n    pass\nMISSING = _MISSING_TYPE()\n\n# Since most per-field metadata will be unused, create an empty\n# read-only proxy that can be shared among all fields.\n_EMPTY_METADATA = types.MappingProxyType({})\n\n# Markers for the various kinds of fields and pseudo-fields.\nclass _FIELD_BASE:\n    def __init__(self, name):\n        self.name = name\n    def __repr__(self):\n        return self.name\n_FIELD = _FIELD_BASE(\'_FIELD\')\n_FIELD_CLASSVAR = _FIELD_BASE(\'_FIELD_CLASSVAR\')\n_FIELD_INITVAR = _FIELD_BASE(\'_FIELD_INITVAR\')\n\n# The name of an attribute on the class where we store the Field\n# objects.  Also used to check if a class is a Data Class.\n_FIELDS = \'__dataclass_fields__\'\n\n# The name of an attribute on the class that stores the parameters to\n# @dataclass.\n_PARAMS = \'__dataclass_params__\'\n\n# The name of the function, that if it exists, is called at the end of\n# __init__.\n_POST_INIT_NAME = \'__post_init__\'\n\n# String regex that string annotations for ClassVar or InitVar must match.\n# Allows "identifier.identifier[" or "identifier[".\n# https://bugs.python.org/issue33453 for details.\n_MODULE_IDENTIFIER_RE = re.compile(r\'^(?:\\s*(\\w+)\\s*\\.)?\\s*(\\w+)\')\n\nclass _InitVarMeta(type):\n    def __getitem__(self, params):\n        return InitVar(params)\n\nclass InitVar(metaclass=_InitVarMeta):\n    __slots__ = (\'type\', )\n\n    def __init__(self, type):\n        self.type = type\n\n    def __repr__(self):\n        if isinstance(self.type, type):\n            type_name = self.type.__name__\n        else:\n            # typing objects, e.g. List[int]\n            type_name = repr(self.type)\n        return f\'dataclasses.InitVar[{type_name}]\'\n\n\n# Instances of Field are only ever created from within this module,\n# and only from the field() function, although Field instances are\n# exposed externally as (conceptually) read-only objects.\n#\n# name and type are filled in after the fact, not in __init__.\n# They\'re not known at the time this class is instantiated, but it\'s\n# convenient if they\'re available later.\n#\n# When cls._FIELDS is filled in with a list of Field objects, the name\n# and type fields will have been populated.\nclass Field:\n    __slots__ = (\'name\',\n                 \'type\',\n                 \'default\',\n                 \'default_factory\',\n                 \'repr\',\n                 \'hash\',\n                 \'init\',\n                 \'compare\',\n                 \'metadata\',\n                 \'_field_type\',  # Private: not to be used by user code.\n                 )\n\n    def __init__(self, default, default_factory, init, repr, hash, compare,\n                 metadata):\n        self.name = None\n        self.type = None\n        self.default = default\n        self.default_factory = default_factory\n        self.init = init\n        self.repr = repr\n        self.hash = hash\n        self.compare = compare\n        self.metadata = (_EMPTY_METADATA\n                         if metadata is None else\n                         types.MappingProxyType(metadata))\n        self._field_type = None\n\n    def __repr__(self):\n        return (\'Field(\'\n                f\'name={self.name!r},\'\n                f\'type={self.type!r},\'\n                f\'default={self.default!r},\'\n                f\'default_factory={self.default_factory!r},\'\n                f\'init={self.init!r},\'\n                f\'repr={self.repr!r},\'\n                f\'hash={self.hash!r},\'\n                f\'compare={self.compare!r},\'\n                f\'metadata={self.metadata!r},\'\n                f\'_field_type={self._field_type}\'\n                \')\')\n\n    # This is used to support the PEP 487 __set_name__ protocol in the\n    # case where we\'re using a field that contains a descriptor as a\n    # default value.  For details on __set_name__, see\n    # https://www.python.org/dev/peps/pep-0487/#implementation-details.\n    #\n    # Note that in _process_class, this Field object is overwritten\n    # with the default value, so the end result is a descriptor that\n    # had __set_name__ called on it at the right time.\n    def __set_name__(self, owner, name):\n        func = getattr(type(self.default), \'__set_name__\', None)\n        if func:\n            # There is a __set_name__ method on the descriptor, call\n            # it.\n            func(self.default, owner, name)\n\n\nclass _DataclassParams:\n    __slots__ = (\'init\',\n                 \'repr\',\n                 \'eq\',\n                 \'order\',\n                 \'unsafe_hash\',\n                 \'frozen\',\n                 )\n\n    def __init__(self, init, repr, eq, order, unsafe_hash, frozen):\n        self.init = init\n        self.repr = repr\n        self.eq = eq\n        self.order = order\n        self.unsafe_hash = unsafe_hash\n        self.frozen = frozen\n\n    def __repr__(self):\n        return (\'_DataclassParams(\'\n                f\'init={self.init!r},\'\n                f\'repr={self.repr!r},\'\n                f\'eq={self.eq!r},\'\n                f\'order={self.order!r},\'\n                f\'unsafe_hash={self.unsafe_hash!r},\'\n                f\'frozen={self.frozen!r}\'\n                \')\')\n\n\n# This function is used instead of exposing Field creation directly,\n# so that a type checker can be told (via overloads) that this is a\n# function whose type depends on its parameters.\ndef field(*, default=MISSING, default_factory=MISSING, init=True, repr=True,\n          hash=None, compare=True, metadata=None):\n    """Return an object to identify dataclass fields.\n\n    default is the default value of the field.  default_factory is a\n    0-argument function called to initialize a field\'s value.  If init\n    is True, the field will be a parameter to the class\'s __init__()\n    function.  If repr is True, the field will be included in the\n    object\'s repr().  If hash is True, the field will be included in\n    the object\'s hash().  If compare is True, the field will be used\n    in comparison functions.  metadata, if specified, must be a\n    mapping which is stored but not otherwise examined by dataclass.\n\n    It is an error to specify both default and default_factory.\n    """\n\n    if default is not MISSING and default_factory is not MISSING:\n        raise ValueError(\'cannot specify both default and default_factory\')\n    return Field(default, default_factory, init, repr, hash, compare,\n                 metadata)\n\n\ndef _tuple_str(obj_name, fields):\n    # Return a string representing each field of obj_name as a tuple\n    # member.  So, if fields is [\'x\', \'y\'] and obj_name is "self",\n    # return "(self.x,self.y)".\n\n    # Special case for the 0-tuple.\n    if not fields:\n        return \'()\'\n    # Note the trailing comma, needed if this turns out to be a 1-tuple.\n    return f\'({",".join([f"{obj_name}.{f.name}" for f in fields])},)\'\n\n\n# This function\'s logic is copied from "recursive_repr" function in\n# reprlib module to avoid dependency.\ndef _recursive_repr(user_function):\n    # Decorator to make a repr function return "..." for a recursive\n    # call.\n    repr_running = set()\n\n    @functools.wraps(user_function)\n    def wrapper(self):\n        key = id(self), _thread.get_ident()\n        if key in repr_running:\n            return \'...\'\n        repr_running.add(key)\n        try:\n            result = user_function(self)\n        finally:\n            repr_running.discard(key)\n        return result\n    return wrapper\n\n\ndef _create_fn(name, args, body, *, globals=None, locals=None,\n               return_type=MISSING):\n    # Note that we mutate locals when exec() is called.  Caller\n    # beware!  The only callers are internal to this module, so no\n    # worries about external callers.\n    if locals is None:\n        locals = {}\n    if \'BUILTINS\' not in locals:\n        locals[\'BUILTINS\'] = builtins\n    return_annotation = \'\'\n    if return_type is not MISSING:\n        locals[\'_return_type\'] = return_type\n        return_annotation = \'->_return_type\'\n    args = \',\'.join(args)\n    body = \'\\n\'.join(f\'  {b}\' for b in body)\n\n    # Compute the text of the entire function.\n    txt = f\' def {name}({args}){return_annotation}:\\n{body}\'\n\n    local_vars = \', \'.join(locals.keys())\n    txt = f"def __create_fn__({local_vars}):\\n{txt}\\n return {name}"\n\n    ns = {}\n    exec(txt, globals, ns)\n    return ns[\'__create_fn__\'](**locals)\n\n\ndef _field_assign(frozen, name, value, self_name):\n    # If we\'re a frozen class, then assign to our fields in __init__\n    # via object.__setattr__.  Otherwise, just use a simple\n    # assignment.\n    #\n    # self_name is what "self" is called in this function: don\'t\n    # hard-code "self", since that might be a field name.\n    if frozen:\n        return f\'BUILTINS.object.__setattr__({self_name},{name!r},{value})\'\n    return f\'{self_name}.{name}={value}\'\n\n\ndef _field_init(f, frozen, globals, self_name):\n    # Return the text of the line in the body of __init__ that will\n    # initialize this field.\n\n    default_name = f\'_dflt_{f.name}\'\n    if f.default_factory is not MISSING:\n        if f.init:\n            # This field has a default factory.  If a parameter is\n            # given, use it.  If not, call the factory.\n            globals[default_name] = f.default_factory\n            value = (f\'{default_name}() \'\n                     f\'if {f.name} is _HAS_DEFAULT_FACTORY \'\n                     f\'else {f.name}\')\n        else:\n            # This is a field that\'s not in the __init__ params, but\n            # has a default factory function.  It needs to be\n            # initialized here by calling the factory function,\n            # because there\'s no other way to initialize it.\n\n            # For a field initialized with a default=defaultvalue, the\n            # class dict just has the default value\n            # (cls.fieldname=defaultvalue).  But that won\'t work for a\n            # default factory, the factory must be called in __init__\n            # and we must assign that to self.fieldname.  We can\'t\n            # fall back to the class dict\'s value, both because it\'s\n            # not set, and because it might be different per-class\n            # (which, after all, is why we have a factory function!).\n\n            globals[default_name] = f.default_factory\n            value = f\'{default_name}()\'\n    else:\n        # No default factory.\n        if f.init:\n            if f.default is MISSING:\n                # There\'s no default, just do an assignment.\n                value = f.name\n            elif f.default is not MISSING:\n                globals[default_name] = f.default\n                value = f.name\n        else:\n            # This field does not need initialization.  Signify that\n            # to the caller by returning None.\n            return None\n\n    # Only test this now, so that we can create variables for the\n    # default.  However, return None to signify that we\'re not going\n    # to actually do the assignment statement for InitVars.\n    if f._field_type is _FIELD_INITVAR:\n        return None\n\n    # Now, actually generate the field assignment.\n    return _field_assign(frozen, f.name, value, self_name)\n\n\ndef _init_param(f):\n    # Return the __init__ parameter string for this field.  For\n    # example, the equivalent of \'x:int=3\' (except instead of \'int\',\n    # reference a variable set to int, and instead of \'3\', reference a\n    # variable set to 3).\n    if f.default is MISSING and f.default_factory is MISSING:\n        # There\'s no default, and no default_factory, just output the\n        # variable name and type.\n        default = \'\'\n    elif f.default is not MISSING:\n        # There\'s a default, this will be the name that\'s used to look\n        # it up.\n        default = f\'=_dflt_{f.name}\'\n    elif f.default_factory is not MISSING:\n        # There\'s a factory function.  Set a marker.\n        default = \'=_HAS_DEFAULT_FACTORY\'\n    return f\'{f.name}:_type_{f.name}{default}\'\n\n\ndef _init_fn(fields, frozen, has_post_init, self_name, globals):\n    # fields contains both real fields and InitVar pseudo-fields.\n\n    # Make sure we don\'t have fields without defaults following fields\n    # with defaults.  This actually would be caught when exec-ing the\n    # function source code, but catching it here gives a better error\n    # message, and future-proofs us in case we build up the function\n    # using ast.\n    seen_default = False\n    for f in fields:\n        # Only consider fields in the __init__ call.\n        if f.init:\n            if not (f.default is MISSING and f.default_factory is MISSING):\n                seen_default = True\n            elif seen_default:\n                raise TypeError(f\'non-default argument {f.name!r} \'\n                                \'follows default argument\')\n\n    locals = {f\'_type_{f.name}\': f.type for f in fields}\n    locals.update({\n        \'MISSING\': MISSING,\n        \'_HAS_DEFAULT_FACTORY\': _HAS_DEFAULT_FACTORY,\n    })\n\n    body_lines = []\n    for f in fields:\n        line = _field_init(f, frozen, locals, self_name)\n        # line is None means that this field doesn\'t require\n        # initialization (it\'s a pseudo-field).  Just skip it.\n        if line:\n            body_lines.append(line)\n\n    # Does this class have a post-init function?\n    if has_post_init:\n        params_str = \',\'.join(f.name for f in fields\n                              if f._field_type is _FIELD_INITVAR)\n        body_lines.append(f\'{self_name}.{_POST_INIT_NAME}({params_str})\')\n\n    # If no body lines, use \'pass\'.\n    if not body_lines:\n        body_lines = [\'pass\']\n\n    return _create_fn(\'__init__\',\n                      [self_name] + [_init_param(f) for f in fields if f.init],\n                      body_lines,\n                      locals=locals,\n                      globals=globals,\n                      return_type=None)\n\n\ndef _repr_fn(fields, globals):\n    fn = _create_fn(\'__repr__\',\n                    (\'self\',),\n                    [\'return self.__class__.__qualname__ + f"(\' +\n                     \', \'.join([f"{f.name}={{self.{f.name}!r}}"\n                                for f in fields]) +\n                     \')"\'],\n                     globals=globals)\n    return _recursive_repr(fn)\n\n\ndef _frozen_get_del_attr(cls, fields, globals):\n    locals = {\'cls\': cls,\n              \'FrozenInstanceError\': FrozenInstanceError}\n    if fields:\n        fields_str = \'(\' + \',\'.join(repr(f.name) for f in fields) + \',)\'\n    else:\n        # Special case for the zero-length tuple.\n        fields_str = \'()\'\n    return (_create_fn(\'__setattr__\',\n                      (\'self\', \'name\', \'value\'),\n                      (f\'if type(self) is cls or name in {fields_str}:\',\n                        \' raise FrozenInstanceError(f"cannot assign to field {name!r}")\',\n                       f\'super(cls, self).__setattr__(name, value)\'),\n                       locals=locals,\n                       globals=globals),\n            _create_fn(\'__delattr__\',\n                      (\'self\', \'name\'),\n                      (f\'if type(self) is cls or name in {fields_str}:\',\n                        \' raise FrozenInstanceError(f"cannot delete field {name!r}")\',\n                       f\'super(cls, self).__delattr__(name)\'),\n                       locals=locals,\n                       globals=globals),\n            )\n\n\ndef _cmp_fn(name, op, self_tuple, other_tuple, globals):\n    # Create a comparison function.  If the fields in the object are\n    # named \'x\' and \'y\', then self_tuple is the string\n    # \'(self.x,self.y)\' and other_tuple is the string\n    # \'(other.x,other.y)\'.\n\n    return _create_fn(name,\n                      (\'self\', \'other\'),\n                      [ \'if other.__class__ is self.__class__:\',\n                       f\' return {self_tuple}{op}{other_tuple}\',\n                        \'return NotImplemented\'],\n                      globals=globals)\n\n\ndef _hash_fn(fields, globals):\n    self_tuple = _tuple_str(\'self\', fields)\n    return _create_fn(\'__hash__\',\n                      (\'self\',),\n                      [f\'return hash({self_tuple})\'],\n                      globals=globals)\n\n\ndef _is_classvar(a_type, typing):\n    # This test uses a typing internal class, but it\'s the best way to\n    # test if this is a ClassVar.\n    return (a_type is typing.ClassVar\n            or (type(a_type) is typing._GenericAlias\n                and a_type.__origin__ is typing.ClassVar))\n\n\ndef _is_initvar(a_type, dataclasses):\n    # The module we\'re checking against is the module we\'re\n    # currently in (dataclasses.py).\n    return (a_type is dataclasses.InitVar\n            or type(a_type) is dataclasses.InitVar)\n\n\ndef _is_type(annotation, cls, a_module, a_type, is_type_predicate):\n    # Given a type annotation string, does it refer to a_type in\n    # a_module?  For example, when checking that annotation denotes a\n    # ClassVar, then a_module is typing, and a_type is\n    # typing.ClassVar.\n\n    # It\'s possible to look up a_module given a_type, but it involves\n    # looking in sys.modules (again!), and seems like a waste since\n    # the caller already knows a_module.\n\n    # - annotation is a string type annotation\n    # - cls is the class that this annotation was found in\n    # - a_module is the module we want to match\n    # - a_type is the type in that module we want to match\n    # - is_type_predicate is a function called with (obj, a_module)\n    #   that determines if obj is of the desired type.\n\n    # Since this test does not do a local namespace lookup (and\n    # instead only a module (global) lookup), there are some things it\n    # gets wrong.\n\n    # With string annotations, cv0 will be detected as a ClassVar:\n    #   CV = ClassVar\n    #   @dataclass\n    #   class C0:\n    #     cv0: CV\n\n    # But in this example cv1 will not be detected as a ClassVar:\n    #   @dataclass\n    #   class C1:\n    #     CV = ClassVar\n    #     cv1: CV\n\n    # In C1, the code in this function (_is_type) will look up "CV" in\n    # the module and not find it, so it will not consider cv1 as a\n    # ClassVar.  This is a fairly obscure corner case, and the best\n    # way to fix it would be to eval() the string "CV" with the\n    # correct global and local namespaces.  However that would involve\n    # a eval() penalty for every single field of every dataclass\n    # that\'s defined.  It was judged not worth it.\n\n    match = _MODULE_IDENTIFIER_RE.match(annotation)\n    if match:\n        ns = None\n        module_name = match.group(1)\n        if not module_name:\n            # No module name, assume the class\'s module did\n            # "from dataclasses import InitVar".\n            ns = sys.modules.get(cls.__module__).__dict__\n        else:\n            # Look up module_name in the class\'s module.\n            module = sys.modules.get(cls.__module__)\n            if module and module.__dict__.get(module_name) is a_module:\n                ns = sys.modules.get(a_type.__module__).__dict__\n        if ns and is_type_predicate(ns.get(match.group(2)), a_module):\n            return True\n    return False\n\n\ndef _get_field(cls, a_name, a_type):\n    # Return a Field object for this field name and type.  ClassVars\n    # and InitVars are also returned, but marked as such (see\n    # f._field_type).\n\n    # If the default value isn\'t derived from Field, then it\'s only a\n    # normal default value.  Convert it to a Field().\n    default = getattr(cls, a_name, MISSING)\n    if isinstance(default, Field):\n        f = default\n    else:\n        if isinstance(default, types.MemberDescriptorType):\n            # This is a field in __slots__, so it has no default value.\n            default = MISSING\n        f = field(default=default)\n\n    # Only at this point do we know the name and the type.  Set them.\n    f.name = a_name\n    f.type = a_type\n\n    # Assume it\'s a normal field until proven otherwise.  We\'re next\n    # going to decide if it\'s a ClassVar or InitVar, everything else\n    # is just a normal field.\n    f._field_type = _FIELD\n\n    # In addition to checking for actual types here, also check for\n    # string annotations.  get_type_hints() won\'t always work for us\n    # (see https://github.com/python/typing/issues/508 for example),\n    # plus it\'s expensive and would require an eval for every string\n    # annotation.  So, make a best effort to see if this is a ClassVar\n    # or InitVar using regex\'s and checking that the thing referenced\n    # is actually of the correct type.\n\n    # For the complete discussion, see https://bugs.python.org/issue33453\n\n    # If typing has not been imported, then it\'s impossible for any\n    # annotation to be a ClassVar.  So, only look for ClassVar if\n    # typing has been imported by any module (not necessarily cls\'s\n    # module).\n    typing = sys.modules.get(\'typing\')\n    if typing:\n        if (_is_classvar(a_type, typing)\n            or (isinstance(f.type, str)\n                and _is_type(f.type, cls, typing, typing.ClassVar,\n                             _is_classvar))):\n            f._field_type = _FIELD_CLASSVAR\n\n    # If the type is InitVar, or if it\'s a matching string annotation,\n    # then it\'s an InitVar.\n    if f._field_type is _FIELD:\n        # The module we\'re checking against is the module we\'re\n        # currently in (dataclasses.py).\n        dataclasses = sys.modules[__name__]\n        if (_is_initvar(a_type, dataclasses)\n            or (isinstance(f.type, str)\n                and _is_type(f.type, cls, dataclasses, dataclasses.InitVar,\n                             _is_initvar))):\n            f._field_type = _FIELD_INITVAR\n\n    # Validations for individual fields.  This is delayed until now,\n    # instead of in the Field() constructor, since only here do we\n    # know the field name, which allows for better error reporting.\n\n    # Special restrictions for ClassVar and InitVar.\n    if f._field_type in (_FIELD_CLASSVAR, _FIELD_INITVAR):\n        if f.default_factory is not MISSING:\n            raise TypeError(f\'field {f.name} cannot have a \'\n                            \'default factory\')\n        # Should I check for other field settings? default_factory\n        # seems the most serious to check for.  Maybe add others.  For\n        # example, how about init=False (or really,\n        # init=<not-the-default-init-value>)?  It makes no sense for\n        # ClassVar and InitVar to specify init=<anything>.\n\n    # For real fields, disallow mutable defaults for known types.\n    if f._field_type is _FIELD and isinstance(f.default, (list, dict, set)):\n        raise ValueError(f\'mutable default {type(f.default)} for field \'\n                         f\'{f.name} is not allowed: use default_factory\')\n\n    return f\n\n\ndef _set_new_attribute(cls, name, value):\n    # Never overwrites an existing attribute.  Returns True if the\n    # attribute already exists.\n    if name in cls.__dict__:\n        return True\n    setattr(cls, name, value)\n    return False\n\n\n# Decide if/how we\'re going to create a hash function.  Key is\n# (unsafe_hash, eq, frozen, does-hash-exist).  Value is the action to\n# take.  The common case is to do nothing, so instead of providing a\n# function that is a no-op, use None to signify that.\n\ndef _hash_set_none(cls, fields, globals):\n    return None\n\ndef _hash_add(cls, fields, globals):\n    flds = [f for f in fields if (f.compare if f.hash is None else f.hash)]\n    return _hash_fn(flds, globals)\n\ndef _hash_exception(cls, fields, globals):\n    # Raise an exception.\n    raise TypeError(f\'Cannot overwrite attribute __hash__ \'\n                    f\'in class {cls.__name__}\')\n\n#\n#                +-------------------------------------- unsafe_hash?\n#                |      +------------------------------- eq?\n#                |      |      +------------------------ frozen?\n#                |      |      |      +----------------  has-explicit-hash?\n#                |      |      |      |\n#                |      |      |      |        +-------  action\n#                |      |      |      |        |\n#                v      v      v      v        v\n_hash_action = {(False, False, False, False): None,\n                (False, False, False, True ): None,\n                (False, False, True,  False): None,\n                (False, False, True,  True ): None,\n                (False, True,  False, False): _hash_set_none,\n                (False, True,  False, True ): None,\n                (False, True,  True,  False): _hash_add,\n                (False, True,  True,  True ): None,\n                (True,  False, False, False): _hash_add,\n                (True,  False, False, True ): _hash_exception,\n                (True,  False, True,  False): _hash_add,\n                (True,  False, True,  True ): _hash_exception,\n                (True,  True,  False, False): _hash_add,\n                (True,  True,  False, True ): _hash_exception,\n                (True,  True,  True,  False): _hash_add,\n                (True,  True,  True,  True ): _hash_exception,\n                }\n# See https://bugs.python.org/issue32929#msg312829 for an if-statement\n# version of this table.\n\n\ndef _process_class(cls, init, repr, eq, order, unsafe_hash, frozen):\n    # Now that dicts retain insertion order, there\'s no reason to use\n    # an ordered dict.  I am leveraging that ordering here, because\n    # derived class fields overwrite base class fields, but the order\n    # is defined by the base class, which is found first.\n    fields = {}\n\n    if cls.__module__ in sys.modules:\n        globals = sys.modules[cls.__module__].__dict__\n    else:\n        # Theoretically this can happen if someone writes\n        # a custom string to cls.__module__.  In which case\n        # such dataclass won\'t be fully introspectable\n        # (w.r.t. typing.get_type_hints) but will still function\n        # correctly.\n        globals = {}\n\n    setattr(cls, _PARAMS, _DataclassParams(init, repr, eq, order,\n                                           unsafe_hash, frozen))\n\n    # Find our base classes in reverse MRO order, and exclude\n    # ourselves.  In reversed order so that more derived classes\n    # override earlier field definitions in base classes.  As long as\n    # we\'re iterating over them, see if any are frozen.\n    any_frozen_base = False\n    has_dataclass_bases = False\n    for b in cls.__mro__[-1:0:-1]:\n        # Only process classes that have been processed by our\n        # decorator.  That is, they have a _FIELDS attribute.\n        base_fields = getattr(b, _FIELDS, None)\n        if base_fields is not None:\n            has_dataclass_bases = True\n            for f in base_fields.values():\n                fields[f.name] = f\n            if getattr(b, _PARAMS).frozen:\n                any_frozen_base = True\n\n    # Annotations that are defined in this class (not in base\n    # classes).  If __annotations__ isn\'t present, then this class\n    # adds no new annotations.  We use this to compute fields that are\n    # added by this class.\n    #\n    # Fields are found from cls_annotations, which is guaranteed to be\n    # ordered.  Default values are from class attributes, if a field\n    # has a default.  If the default value is a Field(), then it\n    # contains additional info beyond (and possibly including) the\n    # actual default value.  Pseudo-fields ClassVars and InitVars are\n    # included, despite the fact that they\'re not real fields.  That\'s\n    # dealt with later.\n    cls_annotations = cls.__dict__.get(\'__annotations__\', {})\n\n    # Now find fields in our class.  While doing so, validate some\n    # things, and set the default values (as class attributes) where\n    # we can.\n    cls_fields = [_get_field(cls, name, type)\n                  for name, type in cls_annotations.items()]\n    for f in cls_fields:\n        fields[f.name] = f\n\n        # If the class attribute (which is the default value for this\n        # field) exists and is of type \'Field\', replace it with the\n        # real default.  This is so that normal class introspection\n        # sees a real default value, not a Field.\n        if isinstance(getattr(cls, f.name, None), Field):\n            if f.default is MISSING:\n                # If there\'s no default, delete the class attribute.\n                # This happens if we specify field(repr=False), for\n                # example (that is, we specified a field object, but\n                # no default value).  Also if we\'re using a default\n                # factory.  The class attribute should not be set at\n                # all in the post-processed class.\n                delattr(cls, f.name)\n            else:\n                setattr(cls, f.name, f.default)\n\n    # Do we have any Field members that don\'t also have annotations?\n    for name, value in cls.__dict__.items():\n        if isinstance(value, Field) and not name in cls_annotations:\n            raise TypeError(f\'{name!r} is a field but has no type annotation\')\n\n    # Check rules that apply if we are derived from any dataclasses.\n    if has_dataclass_bases:\n        # Raise an exception if any of our bases are frozen, but we\'re not.\n        if any_frozen_base and not frozen:\n            raise TypeError(\'cannot inherit non-frozen dataclass from a \'\n                            \'frozen one\')\n\n        # Raise an exception if we\'re frozen, but none of our bases are.\n        if not any_frozen_base and frozen:\n            raise TypeError(\'cannot inherit frozen dataclass from a \'\n                            \'non-frozen one\')\n\n    # Remember all of the fields on our class (including bases).  This\n    # also marks this class as being a dataclass.\n    setattr(cls, _FIELDS, fields)\n\n    # Was this class defined with an explicit __hash__?  Note that if\n    # __eq__ is defined in this class, then python will automatically\n    # set __hash__ to None.  This is a heuristic, as it\'s possible\n    # that such a __hash__ == None was not auto-generated, but it\n    # close enough.\n    class_hash = cls.__dict__.get(\'__hash__\', MISSING)\n    has_explicit_hash = not (class_hash is MISSING or\n                             (class_hash is None and \'__eq__\' in cls.__dict__))\n\n    # If we\'re generating ordering methods, we must be generating the\n    # eq methods.\n    if order and not eq:\n        raise ValueError(\'eq must be true if order is true\')\n\n    if init:\n        # Does this class have a post-init function?\n        has_post_init = hasattr(cls, _POST_INIT_NAME)\n\n        # Include InitVars and regular fields (so, not ClassVars).\n        flds = [f for f in fields.values()\n                if f._field_type in (_FIELD, _FIELD_INITVAR)]\n        _set_new_attribute(cls, \'__init__\',\n                           _init_fn(flds,\n                                    frozen,\n                                    has_post_init,\n                                    # The name to use for the "self"\n                                    # param in __init__.  Use "self"\n                                    # if possible.\n                                    \'__dataclass_self__\' if \'self\' in fields\n                                            else \'self\',\n                                    globals,\n                          ))\n\n    # Get the fields as a list, and include only real fields.  This is\n    # used in all of the following methods.\n    field_list = [f for f in fields.values() if f._field_type is _FIELD]\n\n    if repr:\n        flds = [f for f in field_list if f.repr]\n        _set_new_attribute(cls, \'__repr__\', _repr_fn(flds, globals))\n\n    if eq:\n        # Create _eq__ method.  There\'s no need for a __ne__ method,\n        # since python will call __eq__ and negate it.\n        flds = [f for f in field_list if f.compare]\n        self_tuple = _tuple_str(\'self\', flds)\n        other_tuple = _tuple_str(\'other\', flds)\n        _set_new_attribute(cls, \'__eq__\',\n                           _cmp_fn(\'__eq__\', \'==\',\n                                   self_tuple, other_tuple,\n                                   globals=globals))\n\n    if order:\n        # Create and set the ordering methods.\n        flds = [f for f in field_list if f.compare]\n        self_tuple = _tuple_str(\'self\', flds)\n        other_tuple = _tuple_str(\'other\', flds)\n        for name, op in [(\'__lt__\', \'<\'),\n                         (\'__le__\', \'<=\'),\n                         (\'__gt__\', \'>\'),\n                         (\'__ge__\', \'>=\'),\n                         ]:\n            if _set_new_attribute(cls, name,\n                                  _cmp_fn(name, op, self_tuple, other_tuple,\n                                          globals=globals)):\n                raise TypeError(f\'Cannot overwrite attribute {name} \'\n                                f\'in class {cls.__name__}. Consider using \'\n                                \'functools.total_ordering\')\n\n    if frozen:\n        for fn in _frozen_get_del_attr(cls, field_list, globals):\n            if _set_new_attribute(cls, fn.__name__, fn):\n                raise TypeError(f\'Cannot overwrite attribute {fn.__name__} \'\n                                f\'in class {cls.__name__}\')\n\n    # Decide if/how we\'re going to create a hash function.\n    hash_action = _hash_action[bool(unsafe_hash),\n                               bool(eq),\n                               bool(frozen),\n                               has_explicit_hash]\n    if hash_action:\n        # No need to call _set_new_attribute here, since by the time\n        # we\'re here the overwriting is unconditional.\n        cls.__hash__ = hash_action(cls, field_list, globals)\n\n    if not getattr(cls, \'__doc__\'):\n        # Create a class doc-string.\n        cls.__doc__ = (cls.__name__ +\n                       str(inspect.signature(cls)).replace(\' -> None\', \'\'))\n\n    return cls\n\n\ndef dataclass(cls=None, /, *, init=True, repr=True, eq=True, order=False,\n              unsafe_hash=False, frozen=False):\n    """Returns the same class as was passed in, with dunder methods\n    added based on the fields defined in the class.\n\n    Examines PEP 526 __annotations__ to determine fields.\n\n    If init is true, an __init__() method is added to the class. If\n    repr is true, a __repr__() method is added. If order is true, rich\n    comparison dunder methods are added. If unsafe_hash is true, a\n    __hash__() method function is added. If frozen is true, fields may\n    not be assigned to after instance creation.\n    """\n\n    def wrap(cls):\n        return _process_class(cls, init, repr, eq, order, unsafe_hash, frozen)\n\n    # See if we\'re being called as @dataclass or @dataclass().\n    if cls is None:\n        # We\'re called with parens.\n        return wrap\n\n    # We\'re called as @dataclass without parens.\n    return wrap(cls)\n\n\ndef fields(class_or_instance):\n    """Return a tuple describing the fields of this dataclass.\n\n    Accepts a dataclass or an instance of one. Tuple elements are of\n    type Field.\n    """\n\n    # Might it be worth caching this, per class?\n    try:\n        fields = getattr(class_or_instance, _FIELDS)\n    except AttributeError:\n        raise TypeError(\'must be called with a dataclass type or instance\')\n\n    # Exclude pseudo-fields.  Note that fields is sorted by insertion\n    # order, so the order of the tuple is as the fields were defined.\n    return tuple(f for f in fields.values() if f._field_type is _FIELD)\n\n\ndef _is_dataclass_instance(obj):\n    """Returns True if obj is an instance of a dataclass."""\n    return hasattr(type(obj), _FIELDS)\n\n\ndef is_dataclass(obj):\n    """Returns True if obj is a dataclass or an instance of a\n    dataclass."""\n    cls = obj if isinstance(obj, type) else type(obj)\n    return hasattr(cls, _FIELDS)\n\n\ndef asdict(obj, *, dict_factory=dict):\n    """Return the fields of a dataclass instance as a new dictionary mapping\n    field names to field values.\n\n    Example usage:\n\n      @dataclass\n      class C:\n          x: int\n          y: int\n\n      c = C(1, 2)\n      assert asdict(c) == {\'x\': 1, \'y\': 2}\n\n    If given, \'dict_factory\' will be used instead of built-in dict.\n    The function applies recursively to field values that are\n    dataclass instances. This will also look into built-in containers:\n    tuples, lists, and dicts.\n    """\n    if not _is_dataclass_instance(obj):\n        raise TypeError("asdict() should be called on dataclass instances")\n    return _asdict_inner(obj, dict_factory)\n\n\ndef _asdict_inner(obj, dict_factory):\n    if _is_dataclass_instance(obj):\n        result = []\n        for f in fields(obj):\n            value = _asdict_inner(getattr(obj, f.name), dict_factory)\n            result.append((f.name, value))\n        return dict_factory(result)\n    elif isinstance(obj, tuple) and hasattr(obj, \'_fields\'):\n        # obj is a namedtuple.  Recurse into it, but the returned\n        # object is another namedtuple of the same type.  This is\n        # similar to how other list- or tuple-derived classes are\n        # treated (see below), but we just need to create them\n        # differently because a namedtuple\'s __init__ needs to be\n        # called differently (see bpo-34363).\n\n        # I\'m not using namedtuple\'s _asdict()\n        # method, because:\n        # - it does not recurse in to the namedtuple fields and\n        #   convert them to dicts (using dict_factory).\n        # - I don\'t actually want to return a dict here.  The main\n        #   use case here is json.dumps, and it handles converting\n        #   namedtuples to lists.  Admittedly we\'re losing some\n        #   information here when we produce a json list instead of a\n        #   dict.  Note that if we returned dicts here instead of\n        #   namedtuples, we could no longer call asdict() on a data\n        #   structure where a namedtuple was used as a dict key.\n\n        return type(obj)(*[_asdict_inner(v, dict_factory) for v in obj])\n    elif isinstance(obj, (list, tuple)):\n        # Assume we can create an object of this type by passing in a\n        # generator (which is not true for namedtuples, handled\n        # above).\n        return type(obj)(_asdict_inner(v, dict_factory) for v in obj)\n    elif isinstance(obj, dict):\n        return type(obj)((_asdict_inner(k, dict_factory),\n                          _asdict_inner(v, dict_factory))\n                         for k, v in obj.items())\n    else:\n        return copy.deepcopy(obj)\n\n\ndef astuple(obj, *, tuple_factory=tuple):\n    """Return the fields of a dataclass instance as a new tuple of field values.\n\n    Example usage::\n\n      @dataclass\n      class C:\n          x: int\n          y: int\n\n    c = C(1, 2)\n    assert astuple(c) == (1, 2)\n\n    If given, \'tuple_factory\' will be used instead of built-in tuple.\n    The function applies recursively to field values that are\n    dataclass instances. This will also look into built-in containers:\n    tuples, lists, and dicts.\n    """\n\n    if not _is_dataclass_instance(obj):\n        raise TypeError("astuple() should be called on dataclass instances")\n    return _astuple_inner(obj, tuple_factory)\n\n\ndef _astuple_inner(obj, tuple_factory):\n    if _is_dataclass_instance(obj):\n        result = []\n        for f in fields(obj):\n            value = _astuple_inner(getattr(obj, f.name), tuple_factory)\n            result.append(value)\n        return tuple_factory(result)\n    elif isinstance(obj, tuple) and hasattr(obj, \'_fields\'):\n        # obj is a namedtuple.  Recurse into it, but the returned\n        # object is another namedtuple of the same type.  This is\n        # similar to how other list- or tuple-derived classes are\n        # treated (see below), but we just need to create them\n        # differently because a namedtuple\'s __init__ needs to be\n        # called differently (see bpo-34363).\n        return type(obj)(*[_astuple_inner(v, tuple_factory) for v in obj])\n    elif isinstance(obj, (list, tuple)):\n        # Assume we can create an object of this type by passing in a\n        # generator (which is not true for namedtuples, handled\n        # above).\n        return type(obj)(_astuple_inner(v, tuple_factory) for v in obj)\n    elif isinstance(obj, dict):\n        return type(obj)((_astuple_inner(k, tuple_factory), _astuple_inner(v, tuple_factory))\n                          for k, v in obj.items())\n    else:\n        return copy.deepcopy(obj)\n\n\ndef make_dataclass(cls_name, fields, *, bases=(), namespace=None, init=True,\n                   repr=True, eq=True, order=False, unsafe_hash=False,\n                   frozen=False):\n    """Return a new dynamically created dataclass.\n\n    The dataclass name will be \'cls_name\'.  \'fields\' is an iterable\n    of either (name), (name, type) or (name, type, Field) objects. If type is\n    omitted, use the string \'typing.Any\'.  Field objects are created by\n    the equivalent of calling \'field(name, type [, Field-info])\'.\n\n      C = make_dataclass(\'C\', [\'x\', (\'y\', int), (\'z\', int, field(init=False))], bases=(Base,))\n\n    is equivalent to:\n\n      @dataclass\n      class C(Base):\n          x: \'typing.Any\'\n          y: int\n          z: int = field(init=False)\n\n    For the bases and namespace parameters, see the builtin type() function.\n\n    The parameters init, repr, eq, order, unsafe_hash, and frozen are passed to\n    dataclass().\n    """\n\n    if namespace is None:\n        namespace = {}\n    else:\n        # Copy namespace since we\'re going to mutate it.\n        namespace = namespace.copy()\n\n    # While we\'re looking through the field names, validate that they\n    # are identifiers, are not keywords, and not duplicates.\n    seen = set()\n    anns = {}\n    for item in fields:\n        if isinstance(item, str):\n            name = item\n            tp = \'typing.Any\'\n        elif len(item) == 2:\n            name, tp, = item\n        elif len(item) == 3:\n            name, tp, spec = item\n            namespace[name] = spec\n        else:\n            raise TypeError(f\'Invalid field: {item!r}\')\n\n        if not isinstance(name, str) or not name.isidentifier():\n            raise TypeError(f\'Field names must be valid identifiers: {name!r}\')\n        if keyword.iskeyword(name):\n            raise TypeError(f\'Field names must not be keywords: {name!r}\')\n        if name in seen:\n            raise TypeError(f\'Field name duplicated: {name!r}\')\n\n        seen.add(name)\n        anns[name] = tp\n\n    namespace[\'__annotations__\'] = anns\n    # We use `types.new_class()` instead of simply `type()` to allow dynamic creation\n    # of generic dataclassses.\n    cls = types.new_class(cls_name, bases, {}, lambda ns: ns.update(namespace))\n    return dataclass(cls, init=init, repr=repr, eq=eq, order=order,\n                     unsafe_hash=unsafe_hash, frozen=frozen)\n\n\ndef replace(*args, **changes):\n    """Return a new object replacing specified fields with new values.\n\n    This is especially useful for frozen classes.  Example usage:\n\n      @dataclass(frozen=True)\n      class C:\n          x: int\n          y: int\n\n      c = C(1, 2)\n      c1 = replace(c, x=3)\n      assert c1.x == 3 and c1.y == 2\n      """\n    if len(args) > 1:\n        raise TypeError(f\'replace() takes 1 positional argument but {len(args)} were given\')\n    if args:\n        obj, = args\n    elif \'obj\' in changes:\n        obj = changes.pop(\'obj\')\n        import warnings\n        warnings.warn("Passing \'obj\' as keyword argument is deprecated",\n                      DeprecationWarning, stacklevel=2)\n    else:\n        raise TypeError("replace() missing 1 required positional argument: \'obj\'")\n\n    # We\'re going to mutate \'changes\', but that\'s okay because it\'s a\n    # new dict, even if called with \'replace(obj, **my_changes)\'.\n\n    if not _is_dataclass_instance(obj):\n        raise TypeError("replace() should be called on dataclass instances")\n\n    # It\'s an error to have init=False fields in \'changes\'.\n    # If a field is not in \'changes\', read its value from the provided obj.\n\n    for f in getattr(obj, _FIELDS).values():\n        # Only consider normal fields or InitVars.\n        if f._field_type is _FIELD_CLASSVAR:\n            continue\n\n        if not f.init:\n            # Error if this field is specified in changes.\n            if f.name in changes:\n                raise ValueError(f\'field {f.name} is declared with \'\n                                 \'init=False, it cannot be specified with \'\n                                 \'replace()\')\n            continue\n\n        if f.name not in changes:\n            if f._field_type is _FIELD_INITVAR and f.default is MISSING:\n                raise ValueError(f"InitVar {f.name!r} "\n                                 \'must be specified with replace()\')\n            changes[f.name] = getattr(obj, f.name)\n\n    # Create the new object, which calls __init__() and\n    # __post_init__() (if defined), using all of the init fields we\'ve\n    # added and/or left in \'changes\'.  If there are values supplied in\n    # changes that aren\'t fields, this will correctly raise a\n    # TypeError.\n    return obj.__class__(**changes)\nreplace.__text_signature__ = \'(obj, /, **kwargs)\'\n')
    from typing import List, Tuple, Union, Optional
    from funcparserlib.lexer import make_tokenizer, TokenSpec, Token
    from funcparserlib.parser import tok, Parser, many, forward_decl, finished, maybe
    from funcparserlib.util import pretty_tree
    from dataclasses import dataclass
    import sys
    
    def tokenize(s: str) -> List[Token]:
         specs = [
             TokenSpec("float", r"[+\-]?\d+\.\d*([Ee][+\-]?\d+)*"),
             TokenSpec("float", r"\d+"),
             TokenSpec("relation",r"\.\w+"),
             TokenSpec("target", r":\w+"),
             TokenSpec("inverse_relation", r"'\w+"),
             TokenSpec("idea",r"\w+"),
             TokenSpec("whitespace", r"\s+"),
             TokenSpec("or", r"\|"),
             TokenSpec("and", r"&"),
             TokenSpec("addqty", r"\+="),
             TokenSpec("minusqty", r"\-="),
             TokenSpec("timesqty", r"\*="),
             TokenSpec("divideqty", r"/="),
             TokenSpec("modqty", r"%="),
             TokenSpec("expqty", r'\^='),
         ]
         tokenizer = make_tokenizer(specs)
         return [t for t in tokenizer(s) if t.type != "whitespace"]
    
    @dataclass
    class RelationalExpr:
        idea: Optional[str]
        relation: Optional[str]
        target: Optional[None]
    
    @dataclass
    class MathExpr:
         op: str
         val: str
         
    @dataclass
    class RelationalMathExpr:
         relExpr: RelationalExpr
         mathExpr: MathExpr
         
    @dataclass
    class DisjunctionExpr:
         left: RelationalExpr
         right: 'QueryExpr'
    
    @dataclass
    class ConjunctionExpr:
         left: RelationalExpr
         right: 'QueryExpr'
    
    QueryExpr = Union[RelationalExpr, DisjunctionExpr, ConjunctionExpr, RelationalMathExpr]
    
    def parse(tokens: List[Token]) -> QueryExpr:
        idea = tok('idea')
        relation = tok('relation') >> (lambda a: a[1:])
        target = tok('target') >> (lambda a: a[1:])
        inverse_relation = tok('inverse_relation') >> (lambda a: a[1:])
    
        expr = forward_decl()
        relationalExpr = \
            idea + relation + target >> (lambda args: RelationalExpr(*args)) | \
            idea + relation >> (lambda args: RelationalExpr(idea=args[0], relation=args[1], target=None)) | \
            idea + target >> (lambda args: RelationalExpr(idea=args[0], relation=None,target=args[1])) | \
            idea + inverse_relation + target >> (lambda args: RelationalExpr(idea=args[2], relation=args[1], target=args[0])) | \
            idea + inverse_relation >> (lambda args: RelationalExpr(idea=None, relation=args[1], target=args[0])) | \
            relation + target >> (lambda args: RelationalExpr(idea=None, relation=args[0], target=args[1])) | \
            idea >> (lambda i: RelationalExpr(idea=i, relation=None, target=None)) | \
            relation >> (lambda r: RelationalExpr(idea=None, relation=r,  target=None)) | \
            target >> (lambda t: RelationalExpr(idea=None, relation=None, target=t))
        mathOp = tok('addqty') | tok('minusqty') | tok('timesqty') | tok('divideqty') | tok('modqty') | tok('expqty')    
        mathExpr = mathOp + tok('float') >> (lambda args: MathExpr(op=args[0], val=args[1]))
        relationalMathExpr = relationalExpr + mathExpr >> (lambda args: RelationalMathExpr(relExpr=args[0], mathExpr=args[1]))
        term = relationalMathExpr | relationalExpr 
        disjunctionExpr = term + tok('or') + expr >> (lambda args: DisjunctionExpr(left=args[0], right=args[2]))
        conjunctionExpr = term + tok('and') + expr >> (lambda args: ConjunctionExpr(left=args[0], right=args[2]))
        expr.define(disjunctionExpr | conjunctionExpr | term)
    
        document = expr + -finished
        return document.parse(tokens)
    
    def intersperse(value, seq):
        res = [value] * (2 * len(seq) - 1)
        res[::2] = seq
        return res
    
    def compile_sql(q: QueryExpr, argc: int=1) -> Tuple[str,List[str]]:
        arg_stack = []
        sql = 'SELECT * FROM meme.meme WHERE '
        filters = []
        if isinstance(q, RelationalExpr):
            if q.idea is not None:
                filters.append(f'aid=${argc}')
                argc += 1
                arg_stack.append(q.idea)
            if q.relation is not None:
                filters.append(f'rid=${argc}')
                argc += 1
                arg_stack.append(q.relation)
            if q.target is not None:
                filters.append(f'bid=${argc}')
                argc += 1
                arg_stack.append(q.target)
        elif isinstance(q, RelationalMathExpr):
             (basesql, baseparams) = compile_sql(q.relExpr, argc)
             math_projection = compile_math_sql(q.mathExpr)
             sql = basesql.replace('*', f'aid,rid,bid,{math_projection} AS qnt', 1)
             arg_stack = baseparams
        elif isinstance(q, DisjunctionExpr):
             (left_sql, left_params) = compile_sql(q.left, argc)
             (right_sql, right_params) = compile_sql(q.right, argc + len(left_params))
             sql = f'{left_sql} UNION {right_sql}'
             arg_stack.extend(left_params + right_params)
        elif isinstance(q, ConjunctionExpr):
             (left_sql, left_params) = compile_sql(q.left, argc)
             (right_sql, right_params) = compile_sql(q.right, argc + len(left_params))
             filters.append(f'EXISTS ({left_sql}) AND EXISTS ({right_sql})')
             arg_stack.extend(left_params + right_params)
        else:
           raise ValueError(f'unknown parse value: {q}')
        sql += ' '.join(intersperse("AND", filters))
        return (sql, arg_stack)
    
    def compile_math_sql(expr: MathExpr) -> str:
         if expr.op == '+=':
              return f'qnt + {expr.val}'
         elif expr.op == '-=':
              return f'qnt - {expr.val}'
         elif expr.op == '*=':
              return f'qnt * {expr.val}'
         elif expr.op == '/=':
              return f'qnt / {expr.val}'
         elif expr.op == '%=':
              return f'qnt % {expr.val}'
         elif expr.op == '^=':
              return f'qnt ^ {expr.val}'
         else:
              raise ValueError(f'invalid operator: {expr.op}')
        
    def execute_memelang():
         (sql, params) = compile_sql(parse(tokenize(memelang_in)))
         plpy.info(sql)
         plan = plpy.prepare(sql, ['text'] * len(params))
         result = plpy.execute(plan, params)
         #for r in result:
         #     plpy.info(r)
         return [r for r in result]
    
    
return execute_memelang()
$$ LANGUAGE plpython3u;